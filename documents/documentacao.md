# Documenta√ß√£o Modelo Preditivo - Inteli

## Modelo preditivo para identifica√ß√£o de anomalias no consumo de g√°s natural
### Galv√£o & Associados: Gases e Dados 

## üë®‚Äçüéì Integrantes: 
- <a href="https://www.linkedin.com/in/caio-alcantara-santos/">Caio de Alcantara Santos</a>
- <a href="https://www.linkedin.com/in/cec%C3%ADlia-galv%C3%A3o/">Cec√≠lia Beatriz Melo Galv√£o</a>
- <a href="https://www.linkedin.com/in/eduardo-f-libutti-salles-599299263/">Eduardo Faris Libutti Salles</a> 
- <a href="https://www.linkedin.com/in/kethlen-martins-040332221/">Kethlen Martins da Silva</a> 
- <a href="https://www.linkedin.com/in/lucas-cozzolino-tort-783273270/">Lucas Cozzolino Tort</a> 
- <a href="https://www.linkedin.com/in/mariella-kamezawa/">Mariella Sayumi Mercado Kamezawa</a> 
- <a href="https://www.linkedin.com/in/natalycunha/">Nataly de Souza Cunha</a>
- <a href="https://www.linkedin.com/in/pabloazevedo/">Pablo de Azevedo</a>

## üë©‚Äçüè´ Professores:
### Orientador(a) 
- <a href="https://www.linkedin.com/in/profclaudioandre/">Claudio Fernando Andr√©</a>
### Instrutores
- <a href="https://www.linkedin.com/in/diogo-martins-gon%C3%A7alves-de-morais-96404732/">Diogo Martins Gon√ßalves de Morais</a>
- <a href="https://www.linkedin.com/in/francisco-escobar/">Francisco de Souza Escobar</a> 
- <a href="https://www.linkedin.com/in/henrique-mohallem-paiva-6854b460/">Henrique Mohallem Paiva</a>
- <a href="https://www.linkedin.com/in/michele-bazana-de-souza-69b77763/">Michele Bazana de Souza</a> 
- <a href="https://www.linkedin.com/in/rafael-will-m-de-araujo-20809b18b/">Rafael Will Macedo de Araujo</a> 

## Sum√°rio
[1. Introdu√ß√£o](#c1)

[2. Objetivos e Justificativa](#c2)

[3. Metodologia](#c3)

[4. Desenvolvimento e Resultados](#c4)

[5. Conclus√µes e Recomenda√ß√µes](#c5)

[6. Refer√™ncias](#c6)

[Anexos](#attachments)


## <a name="c1"></a>1. Introdu√ß√£o
&nbsp;&nbsp;&nbsp;&nbsp;A *Compass* √© uma empresa de g√°s e energia que surgiu em mar√ßo de 2020, a partir da aquisi√ß√£o da *Comg√°s* pelo *Grupo Cosan* em 2012. Desse modo, a *Compass* surgiu como um modelo de neg√≥cio vencedor que possibilitou ampliar o n√∫mero de clientes e expandir a rede de gasodutos de distribui√ß√£o, oferecendo op√ß√µes para um mercado de g√°s e energia cada vez mais livre no Brasil, colaborando para o crescente desenvolvimento da economia do pa√≠s. Assim, atualmente a *Compass* atua em dois segmentos espec√≠ficos: Distribui√ß√£o e Marketing & Servi√ßos, sendo o primeiro focado na distribui√ß√£o pela *Comg√°s* e mais outras 11 empresas gerenciadas pela *Commit*, e o segundo concentrado no oferecimento de alternativas de suprimento de g√°s natural, atrav√©s da gest√£o da *Edge*, empresa criada pela *Compass*. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Por meio da *Comg√°s*, a maior distribuidora de g√°s do Brasil, a *Compass* √© operacionalizada por uma rede de mais de 19 mil km, atendendo 2,1 milh√µes de clientes nos setores residencial, comercial, industrial, veicular, entre outros, segundo informa√ß√µes do pr√≥prio site da [*Compass*](https://www.compassbr.com/sobre-a-compass/quem-somos/)</a>. Al√©m disso, em 2020, foi distribu√≠do 4,5 bilh√µes de m¬≥ de g√°s, o que representa 32% do volume de g√°s canalizado no pa√≠s, consolidando-a com a maior participa√ß√£o de mercado entre as distribuidoras nacionais, conforme dados do *Minist√©rio de Minas e Energia*. Dessa forma, presente em 94 munic√≠pios do Estado de S√£o Paulo, incluindo a capital e sua regi√£o metropolitana, a *Comg√°s* foi reconhecida pela *American Gas Association (AGA)* como refer√™ncia global em seguran√ßa no setor de distribui√ß√£o de g√°s por 13 anos consecutivos, contribuindo, assim, para a concretiza√ß√£o da *Compass* no mercado brasileiro. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Dessa maneira, tendo em vista o contexto de constante crescimento da *Compass*, a empresa atualmente enfrenta um grande desafio de desenvolvimento: o surgimento de fraudes e anomalias no seu neg√≥cio. Nesse sentido, esses pontos de preocupa√ß√£o aparecem principalmente no formato de vazamento de medidores e distor√ß√£o nas informa√ß√µes retornadas nos equipamentos de manuten√ß√£o. √Ä vista disso, √© responsabilidade do grupo desenvolver uma solu√ß√£o, caracterizada por algoritmos de aprendizado de m√°quina, capaz de ser aplicado para monitorar os dados de consumo de g√°s em tempo real, identificando padr√µes anormais com alta precis√£o. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Logo, o projeto ser√° desenvolvido com o objetivo de escalar modelos preditivos de dados de consumo de g√°s, beneficiando as equipes de Produtos, Tecnologia e Opera√ß√µes da *Compass* e aprimorando a qualidade dos servi√ßos prestados aos clientes, sendo eles distribuidoras de g√°s, condom√≠nios e usu√°rios finais. Portanto, atrav√©s desses modelos preditivos, buscamos implementar uma gest√£o de riscos mais eficiente, entregando uma plataforma de intelig√™ncia de dados que, ao cruzar o consumo monitorado dos clientes da *Compass*, gerar√° informa√ß√µes valiosas para melhorar opera√ß√µes, seguran√ßa e vendas, al√©m de detectar anomalias de consumo que possam indicar desvios ou fraudes.<br>


## <a name="c2"></a>2. Objetivos e Justificativa

&nbsp;&nbsp;&nbsp;&nbsp;Esta se√ß√£o trata da breve contextualiza√ß√£o da empresa parceira, falando sobre seus principais objetivos. Al√©m disso, ser√° realizada uma an√°lise acerca da solu√ß√£o que ser√° desenvolvida, tratando da proposta da solu√ß√£o, como ela ser√° utilizada, crit√©rios de sucesso e justificativa para seu uso.

### 2.1 Objetivos

&nbsp;&nbsp;&nbsp;&nbsp;Assim como abordado na introdu√ß√£o deste documento, a Compass √© uma empresa criada em 2020 que surgiu a partir da aquisi√ß√£o da Comgas pelo Grupo Cosan. Desse modo, a Compass se coloca como uma *holding* com um modelo de neg√≥cios que fez capaz a amplia√ß√£o da sua rede de clientes, expandido a rede de gasodutos de distribui√ß√£o no Brasil. Atualmente, a Compass direciona o seu trabalho a dois segmentos: o de distribui√ß√£o de g√°s e o de marketing & servi√ßos, onde o primeiro √© focado na distribui√ß√£o pela Comgas e demais empresas gerenciadas pela Commit, e o segundo tem como objetivo oferecer alternativas de suprimento de g√°s natural (biometano, por exemplo). <br>
&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, ficam listados como principais objetivos da Compass a vontade de se colocar como a empresa respons√°vel pelo maior fluxo de distribui√ß√£o de g√°s encanado na Am√©rica Latina, bem como o desejo de realizar uma transforma√ß√£o em tal mercado, fazendo com que cada vez mais parcelas significativas da popula√ß√£o utilizem fontes de energia mais renov√°veis como o biometano.

### 2.2 Proposta de solu√ß√£o

&nbsp;&nbsp;&nbsp;&nbsp;Retornamos, ent√£o, a um dos principais desafios que a Compass tem enfrentado durante o seu desenvolvimento: o surgimento de anomalias e fraudes no consumo de g√°s. Dessa forma, foi colocada como proposta de solu√ß√£o a elabora√ß√£o de um modelo preditivo que utilizar√° dados de consumo fornecidos pela empresa a fim de treinar tal modelo para identificar inconsist√™ncias no consumo dos clientes. Essas anomalias variam desde a vazamentos de g√°s at√© a diferen√ßas bruscas no consumo desses clientes. Assim, identificar esses problemas pode ajudar a empresa a encontrar poss√≠veis clientes fraudulentos, que podem estar utilizando, por exemplo, de mecanismos para burlar o medidor instalado nas resid√™ncias a fim de pagar menos pelo g√°s consumido. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Por fim, este modelo preditivo para fraudes e anomalias atende aos objetivos da Compass por meio da facilita√ß√£o no processo de identifica√ß√£o de fraudes, possivelmente reduzindo o √≠ndice de clientes que praticam a√ß√µes fraudulentas e a perda monet√°ria que decorre de tais a√ß√µes. Ademais, o modelo preditivo a ser desenvolvido pode ser uma forte ferramenta para fornecer √† Compass uma intelig√™ncia sobre o consumo de seus clientes, possibilitando a oferta de servi√ßos cada vez mais personalizados, algo que atende ao objetivo de ampliar a base de clientes e a rede de distribui√ß√£o de g√°s encanado no Brasil. 

### 2.3 Justificativa

&nbsp;&nbsp;&nbsp;&nbsp;Modelos preditivos s√£o dotados de grande potencial quando o assunto √© an√°lise de dados, e, dentre os principais identificados na solu√ß√£o proposta, podemos apontar a identifica√ß√£o de fraudes e anomalias com um **grande** volume de dados, sendo este um trabalho que √© invi√°vel de ser realizado de maneira manual. Dessa forma, o modelo preditivo descrito na proposta da solu√ß√£o pode resolver os problemas j√° citados por meio da realiza√ß√£o da an√°lise de uma grande quantidade de dados de consumo e entregando √† empresa quais clientes est√£o com consumo anormal e podem estar cometendo fraude. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, identificamos como benef√≠cios desta solu√ß√£o a possibilidade de, ao final e com base nos resultados, ofertar servi√ßos personalizados para os clientes da Compass. Ademais, como dito anteriormente, a identifica√ß√£o das anomalias e fraudes podem acarretar na diminui√ß√£o das perdas monet√°rias que ocorrem por conta dessas situa√ß√µes. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Finalizando, destacamos tamb√©m o principal diferencial desta solu√ß√£o em rela√ß√£o √†s demais que podem estar presentes no mercado como sendo a possibilidade de realizar uma detec√ß√£o em tempo real das anomalias, sem a necessidade de esperar semanas para que um profissional v√° at√© o local realizar a leitura do medidor.  

## <a name="c3"></a>3. Metodologia

&nbsp;&nbsp;&nbsp;&nbsp;Para o atual projeto, a metodologia aplicada √© a CRISP-DM (*Cross Industry Standard Process for Data Mining*). Tal metodologia tem como principal objetivo o desenvolvimento de modelos a partir da an√°lise de informa√ß√µes e dados de um neg√≥cio a fim de prever futuras falhas e solu√ß√µes (Roberto, 2023). Esta metodologia √© dividida em 6 etapas, onde cada uma possui suas peculiaridades. Al√©m disso, √© importante ressaltar que o processo da metodologia CRISP-DM √© iterativo, ou seja, quando terminada cada etapa, posteriormente √© sempre poss√≠vel voltar para um passo anterior, a fim de aprimorar o trabalho que foi realizado anteriormente. As seis etapas da metodologia s√£o:
* **Entendimento do neg√≥cio**: Trata-se da primeira etapa da metodologia e uma das mais essenciais para o projeto, uma vez que √© neste momento em que se definem os objetivos do projeto que est√° sendo realizado e as necessidades da empresa. Por conta disso, caso esse entendimento seja feito de maneira errada, o projeto inteiro pode ser comprometido. 
* **Entendimento dos dados**: Esta etapa precede a explora√ß√£o dos dados em si, e √© marcada por questionamentos acerca dos dados que ser√£o recebidos para o projeto. A partir destes questionamentos √© que √© feita a coleta dos dados, por exemplo (algo que, por decis√µes de escopo, n√£o ocorreu no presente projeto).
* **Prepara√ß√£o dos dados**: J√° com os dados em m√£os, √© nesta etapa onde ocorre o tratamento e pr√©-processamento dos dados, ou seja, tratamento de valores *outliers* e nulos, enriquecimento dos dados e at√© a escolha das *features* mais relevantes para o modelo. Esta etapa normalmente √© a mais demorada do processo e tamb√©m a que mais √© revisitada, uma vez que os dados precisam estar em processo de tratamento constante durante o decorrer do projeto.
* **Modelagem**: Esta √©, normalmente, a etapa mais aguardada do processo, apesar de ser geralmente a mais r√°pida. Neste momento, √© definido o tipo de modelagem/modelo que ser√° utilizado e quais atributos ser√£o vari√°veis na constru√ß√£o de tal modelo. Al√©m disso, √© nesta etapa onde, de fato, um modelo √© performado e se obt√©m resultados referentes, sendo um exemplo a acur√°cia de um modelo preditivo. 
* **Avalia√ß√£o**: Possuindo os resultados da modelagem, obtidos na etapa anterior, este √© o momento onde se deve avaliar se o resultado do modelo, como a acur√°cia, corresponde com as expectativas existentes ao come√ßar o projeto. Caso o resultado n√£o seja satisfat√≥rio e exista espa√ßo para melhorias, os esfor√ßos da equipe devem ser apontados para realizar tais melhorias. A partir desta etapa, √© comum que equipes retornem para a etapa de prepara√ß√£o dos dados a fim de melhorar a qualidade dos dados utilizados na modelagem, buscando obter resultados melhores.
* **Implementa√ß√£o ou *deploy***: Sendo a √∫ltima etapa do processo, o *deploy* √© o momento onde o modelo deve ser colocado em produ√ß√£o e deve trazer impacto real para o neg√≥cio/empresa. Esta √© uma etapa que pode variar grandemente de projeto para projeto, mas um exemplo de *deploy* consiste em disponibilizar o modelo em um servi√ßo de *cloud* e disponibiliz√°-lo para membros da empresa. 

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, destaca-se que o principal benef√≠cio que √© trazido a partir da aplica√ß√£o de uma metodologia CRISP-DM √© a maneira como a cria√ß√£o do modelo preditivo se encaixa com o entendimento do neg√≥cio da empresa parceira. Com tal metodologia, pode-se analisar a fundo como funciona o modelo de neg√≥cio da Compass e como o problema trazido impacta a empresa. A partir disso, √© poss√≠vel criar um modelo preditivo que combata mais diretamente o problema, sendo mais eficaz para resolv√™-lo. Al√©m disso, a metodologia CRISP-DM se mostra muito eficaz em projetos de an√°lise de dados por conta do seu car√°ter iterativo, em que podemos sempre voltar para etapas anteriores, algo que se demonstra muito √∫til em casos onde, ap√≥s se avaliar determinado modelo e perceber que ele n√£o √© satisfat√≥rio, √© poss√≠vel retornar para a etapa de prepara√ß√£o dos dados a fim de refinar o *dataset* e obter resultados melhores.  

## <a name="c4"></a>4. Desenvolvimento e Resultados

&nbsp;&nbsp;&nbsp;&nbsp;Esta se√ß√£o detalha o desenvolvimento e os resultados do projeto, desde a compreens√£o do problema a ser resolvido e a an√°lise dos dados que ser√£o utilizados no modelo preditivo, at√© a prepara√ß√£o dos dados e a etapa de modelagem.

### 4.1. Compreens√£o do Problema
&nbsp;&nbsp;&nbsp;&nbsp;Para compreender o problema que iremos solucionar, √© necess√°rio passar por uma s√©rie de etapas, como estudar o contexto da ind√∫stria, realizar uma an√°lise da empresa parceira e da solu√ß√£o proposta que estamos desenvolvendo, identificar os poss√≠veis riscos do projeto, definir as personas e mapear a jornada do usu√°rio. O objetivo de todo esse processo √© realizar um projeto mais consistente e fiel √†s necessidades dos clientes.



#### 4.1.1. Contexto da ind√∫stria 
&nbsp;&nbsp;&nbsp;&nbsp;A *Compass* √© uma empresa de g√°s e energia fundada em 2020 que, desde sua funda√ß√£o, tem se empenhado em promover o uso consciente e eficiente da energia em diversas regi√µes do Brasil. Com uma presen√ßa significativa no mercado, a Compass integra o setor de g√°s natural e disputa espa√ßo com outras empresas que tamb√©m atuam nesse mesmo ramo, como *Petrobras*, *Naturgy* e *Eneva*. Assim, al√©m de fornecer servi√ßos essenciais, a *Compass* est√° comprometida com a responsabilidade social e ambiental, buscando expandir suas opera√ß√µes e investir em tecnologias que aumentem a efici√™ncia e a sustentabilidade de seus servi√ßos. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, a *Compass* √© uma empresa do setor privado com um modelo de neg√≥cios dividido em dois principais segmentos: Distribui√ß√£o e Marketing & Servi√ßos. No segmento de distribui√ß√£o, a *Compass* opera por meio de duas empresas: a *Comg√°s*, maior distribuidora de g√°s natural do Brasil, e a *Commit*, uma parceria estrat√©gica com o conglomerado japon√™s *Mitsui*. Os ativos da *Commit* est√£o distribu√≠dos em dois *clusters* principais: o Nordeste, representado pela *Norg√°s*, e o Centro-Sul, onde a *Sulg√°s*, adquirida do Estado do Rio Grande do Sul em 2022, e a *Necta*, controlada diretamente pela *Commit*, desempenham pap√©is importantes. A *Commit* tamb√©m possui participa√ß√µes minorit√°rias em outras distribuidoras e trabalha em conjunto com seus parceiros para implementar as melhores pr√°ticas de gest√£o. <br>

&nbsp;&nbsp;&nbsp;&nbsp;No segmento de Marketing & Servi√ßos, a *Compass* tem como objetivo fornecer alternativas de suprimento de g√°s natural, garantindo seguran√ßa, flexibilidade e promovendo a descarboniza√ß√£o para seus clientes, tanto os conectados √† rede de distribui√ß√£o quanto os que operam fora dela (*off-grid*), atrav√©s do modal rodovi√°rio (*GNL B2B*). A partir de 2023, esse segmento passou a operar sob a marca *"Edge"*, que concentra ativos estrat√©gicos como o *Terminal de Regasifica√ß√£o de S√£o Paulo (TRSP)* localizado em Santos, al√©m de ativos e contratos de *Biometano*, como a planta de purifica√ß√£o em parceria com a *Orizon*, e o *GNL B2B*. Todos esses ativos s√£o geridos de forma integrada pela *Edge*, visando maximizar efici√™ncia e sinergia. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, atuando no segundo setor, a *Compass* trabalha em um mercado bastante competitivo, tendo que se encarregar constantemente do surgimento de inova√ß√µes tecnol√≥gicas e estrat√©gias de neg√≥cios eficientes. Portanto, √© atrav√©s do desenvolvimento de um modelo preditivo que a *Compass*, juntamente com o seu time de Tecnologia, Produtos e Opera√ß√µes, al√©m do aux√≠lio da *IOLIT*, empresa de controle de dados, conseguir√° desenvolver o seu empreendimento, enfrentando futuros impasses, considerando a import√¢ncia dessas predi√ß√µes para a an√°lise de anomalias e fraudes, em prol do crescimento exponencial do neg√≥cio. <br>

#### 4.1.2. An√°lise SWOT 

&nbsp;&nbsp;&nbsp;&nbsp;Segundo Mian _et al._ (2020), a an√°lise SWOT (Strengths, Weaknesses, Opportunities, Threats) ou an√°lise FOFA (For√ßas, Oportunidades, Fraquezas, Amea√ßas) √© uma ferramenta anal√≠tica, colaborativa e vers√°til, amplamente utilizada para uma compreens√£o aprofundada da situa√ß√£o atual de uma empresa, facilitando o processo de lidar com os desafios para alcan√ßar um objetivo estrat√©gico.

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, a realiza√ß√£o dessa an√°lise √© fundamental para garantir uma gest√£o eficaz do projeto da Compass G√°s e Energia S.A., permitindo um direcionamento estrat√©gico mais preciso. A an√°lise SWOT examina tanto o ambiente interno, identificando as for√ßas e fraquezas da empresa, quanto o ambiente externo, avaliando as oportunidades e amea√ßas do mercado. 

<div align="center">
   
   <sub>Figura 1 - An√°lise Swot </sub>

   <img src="../assets/analise_swot.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;De acordo com a an√°lise SWOT da empresa apresentado acima, temos:

### For√ßas 

- Estrat√©gias de sustentabilidade: A Compass definiu uma estrat√©gia de sustentabilidade focada na descarboniza√ß√£o e na constru√ß√£o de uma matriz energ√©tica mais segura, acess√≠vel e sustent√°vel.  A empresa reconhece o g√°s natural como um componente estrat√©gico para substituir combust√≠veis mais poluentes, com metas para 2030. Esses objetivos incluem alcan√ßar a neutralidade de carbono, liderar a distribui√ß√£o de g√°s renov√°vel no Brasil e impulsionar o uso do g√°s na matriz de transportes, demonstrando um compromisso com a sustentabilidade.

-  Varia√ß√£o regional e operacional: A Compass est√° presente em v√°rias regi√µes do Brasil atrav√©s de distribuidoras como a Comg√°s, Sulg√°s e CeG√°s, al√©m de atuar em diferentes √°reas, incluindo Distribui√ß√£o e Marketing & Servi√ßos de g√°s. Essa diversifica√ß√£o regional e operacional permite √† empresa atender uma ampla gama de clientes e se adaptar melhor √†s varia√ß√µes do mercado, o que ajuda a diminuir riscos √† empresa.

- Forte reputa√ß√£o no Setor de G√°s Natural: A Compass √© uma empresa que controla a Comg√°s, a maior distribuidora de g√°s natural encanado do Brasil. Al√©m disso, a Compass tamb√©m possui a participa√ß√£o de mais de 11 distribuidoras de g√°s, que s√£o gerenciadas pela Commit, uma subsidi√°ria da Compass que conta com a parceria da Mitsui, um dos maiores conglomerados do Jap√£o. Isso amplia a atua√ß√£o da empresa em diferentes regi√µes do Brasil, fortalecendo sua posi√ß√£o no mercado de g√°s natural e aumentando sua capacidade de distribui√ß√£o e fornecimento.



### Fraquezas 


- Complexidade operacional e log√≠stica: A Compass controla mais de 11 distribuidoras de g√°s, o que fortalece a reputa√ß√£o da empresa no mercado. No entanto, essa ampla rede tamb√©m apresenta grandes desafios de gest√£o. Coordenar e controlar distribuidoras em diferentes regi√µes do Brasil, cada uma com suas pr√≥prias regulamenta√ß√µes e infraestruturas, √© uma tarefa complexa. Se essas opera√ß√µes n√£o forem coordenadas de forma eficaz, a complexidade pode comprometer a efici√™ncia do servi√ßo.

- Baixa divulga√ß√£o da marca: Embora a Compass controle grandes distribuidoras de g√°s, como a Comg√°s, Sulg√°s e Necta, a pr√≥pria marca Compass tem pouca visibilidade e reconhecimento entre o p√∫blico em geral. Muitas pessoas est√£o familiarizadas com as distribuidoras regionais, mas n√£o associam essas opera√ß√µes √† Compass. Isso indica uma fraqueza em termos de marketing e divulga√ß√£o da marca corporativa. A falta de uma presen√ßa forte em redes sociais e de materiais informativos detalhados sobre a empresa na m√≠dia pode limitar a percep√ß√£o p√∫blica da Compass como uma empresa importante no setor de energia, embora n√£o seja.


- Altos custos operacionais: A manuten√ß√£o, a log√≠stica e a conformidade regulat√≥ria nas opera√ß√µes de distribui√ß√£o de g√°s geram custos elevados, que se tornam ainda mais desafiadores quando h√° v√°rias distribuidoras para gerenciar. Com a responsabilidade de preservar e atualizar as infraestruturas em diferentes regi√µes, a Compass enfrenta despesas significativas. Esses custos elevados representam uma fraqueza para a empresa, pois podem impactar sua margem de lucro e limitar sua capacidade de investir em outras √°reas estrat√©gicas.

### Oportunidades 

- Combust√≠vel para termel√©trica: O g√°s natural √© atualmente o principal combust√≠vel utilizado na expans√£o da gera√ß√£o termel√©trica no Brasil, devido √† sua efici√™ncia e menor impacto ambiental em compara√ß√£o com outras fontes f√≥sseis. Isso representa uma grande oportunidade para a Compass, que pode aproveitar essa tend√™ncia crescente ao fornecer g√°s natural para novas usinas termel√©tricas, aumentando assim sua participa√ß√£o no mercado e fortalecendo sua posi√ß√£o no setor energ√©tico.

- Aumento da demanda de baixo carbono: Diante da crescente press√£o global para reduzir emiss√µes de carbono e combater as mudan√ßas clim√°ticas, a Compass est√° bem posicionada para aproveitar essa tend√™ncia. Com um plano de sustentabilidade que inclui metas para 2030, como alcan√ßar a neutralidade de carbono e liderar a distribui√ß√£o de g√°s renov√°vel no Brasil, a empresa estar√° preparada para atender a crescente demanda por solu√ß√µes de baixo carbono. Dessa maneira, esse posicionamento estrat√©gico n√£o s√≥ fortalece os pontos fortes da Compass, como tamb√©m a coloca em vantagem competitiva em um mercado cada vez mais voltado para a sustentabilidade.

- Crescimento da Tecnologia: O avan√ßo da tecnologia, como automa√ß√£o, intelig√™ncia artificial entre outros, apresenta uma oportunidade para a Compass no setor de g√°s natural. Ao adotar tecnologias avan√ßadas, como sistemas inteligentes de monitoramento de redes e an√°lises de dados, a empresa pode otimizar a efici√™ncia operacional, reduzir custos, e melhorar a seguran√ßa de suas opera√ß√µes. Al√©m disso, o uso de novas tecnologias pode facilitar a detec√ß√£o de anomalias e fraudes, aprimorar a manuten√ß√£o preditiva e melhorar a gest√£o da rede de distribui√ß√£o de g√°s, minimizando riscos e aumentando a capacidade da infraestrutura.


### Amea√ßas 

- Concorr√™ncia forte: Apesar de sua forte reputa√ß√£o, a Compass enfrenta concorrentes no mercado de g√°s, como Petrobras, Naturgy e G√°s Natural A√ßu (GNA). Empresas como Petrobras e Naturgy dominam grande parte do mercado devido √†s suas vastas infraestruturas e capacidades de produ√ß√£o e distribui√ß√£o de g√°s natural. Enquanto isso, a G√°s Natural A√ßu (GNA) est√° expandindo suas opera√ß√µes e introduzindo inova√ß√µes, aumentando a competitividade no setor.

- Depend√™ncia do mercado brasileiro: A Compass realiza suas opera√ß√µes exclusivamente no Brasil, o que a torna vulner√°vel √†s condi√ß√µes econ√¥micas e pol√≠ticas do pa√≠s. Dessa forma, qualquer instabilidade no cen√°rio nacional pode impactar negativamente as opera√ß√µes da empresa, dificultando a continuidade dos neg√≥cios e a execu√ß√£o de suas estrat√©gias.

- Mudan√ßas clim√°ticas: O aumento da severidade de eventos clim√°ticos, como deslizamentos causados por chuvas intensas, ciclones tropicais e inunda√ß√µes, representa uma amea√ßa √†s opera√ß√µes da Compass. Esses fen√¥menos podem causar danos √† infraestrutura da empresa, como gasodutos e instala√ß√µes de distribui√ß√£o, interrompendo o fornecimento de g√°s e elevando consideravelmente os custos de manuten√ß√£o, reparo e implementa√ß√£o de medidas de preven√ß√£o de riscos. 

&nbsp;&nbsp;&nbsp;&nbsp;Assim, ap√≥s a realiza√ß√£o da an√°lise SWOT da Compass G√°s e Energia SA, √© percept√≠vel uma vis√£o mais ampla da situa√ß√£o da empresa. Foram identificados pontos que precisam ser melhorados, bem como os pontos fortes que colocam a marca em uma posi√ß√£o de vantagem. Al√©m disso, h√° as oportunidades que podem ser exploradas e os riscos que a empresa pode enfrentar diante das circunst√¢ncias atuais. Desse modo, todos esses aspectos s√£o fundamentais para o desenvolvimento e o sucesso do projeto.


#### 4.1.3. Planejamento Geral da Solu√ß√£o

&nbsp;&nbsp;&nbsp;&nbsp;Para desenvolver o planejamento geral da solu√ß√£o, √© essencial levantar alguns pontos importantes para o sucesso do projeto, como os dados dispon√≠veis, a solu√ß√£o proposta, sua aplica√ß√£o pr√°tica e os benef√≠cios esperados. Desta forma, podemos nos aprofundar a respeito dos t√≥picos necess√°rios ao decorrer do texto.

&nbsp;&nbsp;&nbsp;&nbsp;Primeiramente, em rela√ß√£o aos dados fornecidos √† equipe, podemos destacar que o projeto trabalhar√° com dois tipos de planilhas espec√≠ficas, disponibilizadas oficialmente pelo time de tecnologia da Compass: a planilha de informa√ß√µes cadastrais dos clientes e as planilhas mensais de consumo desse p√∫blico.

&nbsp;&nbsp;&nbsp;&nbsp;Os dados trabalhados nas planilhas de informa√ß√µes cadastrais s√£o os seguintes:

- **clientCode (C√≥digo do Cliente):** Identificador √∫nico do cliente, vinculado diretamente ao CPF ou CNPJ do cliente.
- **clientIndex (√çndice do Cliente):** Identificador √∫nico para cada instala√ß√£o (medidor) realizada vinculada ao cliente.
- **cep (CEP da Instala√ß√£o):** CEP da instala√ß√£o.
- **bairro (Bairro da Instala√ß√£o):** Bairro da instala√ß√£o.
- **cidade (Cidade da Instala√ß√£o):** Cidade da instala√ß√£o.
- **categoria (Categoria do Cliente):** Categoria do cliente na qual ele se enquadra.
- **contratacao (Data da Contrata√ß√£o):** Data da contrata√ß√£o do servi√ßo.
- **situacao (Situa√ß√£o Atual do Cliente):** Situa√ß√£o do cliente, se est√° consumindo g√°s ou desativado.
- **perfil_consumo (Servi√ßos Contratados):** Tipo/Perfil de consumo do cliente, quais situa√ß√µes utiliza o servi√ßo de g√°s.
- **condCode (C√≥digo do Condom√≠nio):** Identificador √∫nico do condom√≠nio.
- **condIndex (√çndice do Condom√≠nio):** Identificador √∫nico para cada instala√ß√£o realizada na √°rea comum do condom√≠nio.

&nbsp;&nbsp;&nbsp;&nbsp;Os dados que ser√£o fornecidos na planilha de consumo do cliente s√£o os seguintes:

- **clientCode (C√≥digo do Cliente):** Identificador √∫nico do cliente, vinculado diretamente ao CPF ou CNPJ do cliente.
- **clientIndex (√çndice do Cliente):** Identificador √∫nico para cada instala√ß√£o (medidor) realizada vinculada ao cliente.
- **datetime (Data de Recep√ß√£o):** Data de recep√ß√£o da informa√ß√£o pelo ecossistema LoRa.
- **meterIndex (Leitura Atual):** Valor da leitura atual do medidor.
- **initialIndex (Valor Inicial do Equipamento):** Valor inicial do equipamento quando foi realizada a instala√ß√£o do sensor.
- **pulseCount (Valor de Pulso):** Valor de pulsos lidos referente ao consumo.
- **gain (Valor de Ganho):** Fator de multiplica√ß√£o de pulsos para metros c√∫bicos.
- **rssi (Intensidade de Sinal):** Intensidade de sinal do sensor ao gateway da publica√ß√£o.
- **gatewayGeoLocation.alt (Altitude ou Altura do Gateway):** Valor de altura/altitude da instala√ß√£o do gateway.
- **gatewayGeoLocation.lat (Latitude do Gateway):** Valor de latitude da instala√ß√£o do gateway.
- **gatewayGeoLocation.long (Longitude do Gateway):** Valor de longitude da instala√ß√£o do gateway.
- **model (Modelo do Equipamento):** Modelo do equipamento.
- **meterSN (N√∫mero do Medidor):** N√∫mero do medidor de cada instala√ß√£o.
- **inputType (Tipo de Leitura):** Tipo de leitura do medidor.

&nbsp;&nbsp;&nbsp;&nbsp;Com os dados que ser√£o trabalhados ao longo do desenvolvimento do projeto exemplificados, podemos compreender, portanto, a solu√ß√£o proposta. Neste contexto, a Compass est√° enfrentando desafios relacionados a anomalias e fraudes no consumo de g√°s, das quais se destacam vazamentos de g√°s e poss√≠veis manipula√ß√µes de medidores f√≠sicos. Assim, visando extinguir essas problem√°ticas, a empresa parceira prop√¥s que o grupo criasse um modelo preditivo que identifique essas inconsist√™ncias.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, o modelo elaborado pela equipe ser√° encarregado de auxiliar a detectar fraudes e reduzir as perdas financeiras associadas a esses incidentes para a Compass. Em meio a esse processo, a empresa poder√° compreender melhor o comportamento do consumo de g√°s por parte dos seus clientes. Por fim, espera-se que, com a elabora√ß√£o do modelo preditivo, a empresa possa oferecer servi√ßos mais personalizados e eficazes, contribuindo para a expans√£o de sua base de clientes e da rede de distribui√ß√£o de g√°s encanado no Brasil.

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s desenvolvida, a solu√ß√£o proposta ser√° implementada como uma forma de gest√£o de dados focada em detectar anomalias no consumo de g√°s natural e identificar as poss√≠veis fraudes levantadas. Por meio do nosso modelo preditivo, poderemos monitorar dados que s√£o recebidos de 12 em 12 horas, analisando padr√µes de consumo a partir de dados recebidos pelos sensores instalados nos medidores dos clientes.

&nbsp;&nbsp;&nbsp;&nbsp;Falando um pouco sobre o modelo de neg√≥cios da empresa e os usu√°rios que ser√£o analisados pela solu√ß√£o desenvolvida pela equipe, podemos mencionar, Grandes distribuidores de g√°s natural, configurando um modelo _Business to Business_ (B2B), condom√≠nios residenciais com foco em individualiza√ß√£o de consumo, tamb√©m configurando um modelo _Business to Business_ (B2B), e, por fim, consumidores finais, caracterizando um modelo _Business to Consumer_ (B2C).

&nbsp;&nbsp;&nbsp;&nbsp;Podemos concluir que a solu√ß√£o proposta oferece diversos benef√≠cios para a empresa Compass. Em termos de detec√ß√£o de fraudes, nosso modelo preditivo permitir√° a identifica√ß√£o de fraudes no consumo, evitando perdas financeiras e poss√≠veis danos aos equipamentos f√≠sicos da empresa. 

&nbsp;&nbsp;&nbsp;&nbsp;Em um segundo plano, pode-se mencionar a melhoria na seguran√ßa, garantida pela detec√ß√£o de anomalias em tempo real, possibilitando uma resposta r√°pida para corrigir problemas, como vazamentos de g√°s ou falhas nos equipamentos, assegurando a prote√ß√£o dos usu√°rios finais. 

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, em rela√ß√£o √† efici√™ncia operacional, a an√°lise de dados permitir√° a otimiza√ß√£o dos processos, reduzindo o tempo de resposta a problemas e, consequentemente, melhorando a qualidade do servi√ßo prestado.

#### 4.1.4. Value Proposition Canvas

&nbsp;&nbsp;&nbsp;&nbsp;Segundo a autora Amanda Gushiken (2023), o Canvas Proposta de Valor √© uma ferramenta estrat√©gica que permite aos empreendedores desenhar, testar, visualizar e aprimorar produtos e servi√ßos, baseando-se na compreens√£o das necessidades, desejos e desafios dos clientes. A Proposta de Valor √© uma mensagem clara que explica o que uma empresa oferece, como ela se diferencia dos concorrentes e como ajuda a criar propostas de valor que ressoam com o p√∫blico-alvo. <br>
&nbsp;&nbsp;&nbsp;&nbsp;A estrutura do Canvas Proposta de Valor √© composta por duas partes: um c√≠rculo, que representa o perfil do cliente, e um quadrado, que simboliza o mapa de valor do produto. Essas partes s√£o divididas em tr√™s √°reas: <br>
- **Tarefas**: No perfil do cliente, s√£o identificadas as tarefas realizadas ao consumir um servi√ßo. O produto deve auxiliar na execu√ß√£o dessas tarefas para criar valor.
- **Dores**: S√£o levantados os problemas que o cliente enfrenta. O produto deve reduzir ou eliminar esses problemas para criar valor.
- **Ganhos**: O cliente busca benef√≠cios em suas experi√™ncias. O produto deve proporcionar esses benef√≠cios para criar valor.

<div align="center">
   
   <sub>Figura 2 - Proposta de Valor Canvas Sprint 1 </sub>

   <img src="..\assets\proposta-de-valor-canvas.jpg"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Como mostrado na imagem acima, est√£o definidos os t√≥picos relacionados ao perfil do cliente e √† proposta de valor: <br>
- **Tarefas**: A Compass atua na distribui√ß√£o de g√°s, marketing e servi√ßos, controlando todos esses servi√ßos para suas filiais.
- **Dores**: A Compass enfrenta dificuldades para identificar e controlar anomalias presentes nos processos.
- **Ganhos**: A Compass busca identificar de forma mais f√°cil e proativa as anomalias relacionadas ao g√°s.
- **Produtos e servi√ßos**: Ser√° fornecido um servi√ßo para a an√°lise de dados, visando a descoberta de poss√≠veis anomalias, como desvio de g√°s, vazamentos, medi√ß√µes negativas (medi√ß√£o abaixo da quantidade real utilizada), picos (medi√ß√£o acima da quantidade real utilizada) e aus√™ncia de dados (falta de registro da quantidade de g√°s utilizada).
- **Criadores de ganhos**: Facilitamos o acompanhamento das medi√ß√µes de anomalias pela Compass, tornando os dados mais claros e acess√≠veis para um monitoramento mais eficaz.
- **Aliviadores de dores**: Ser√£o fornecidos os recursos necess√°rios para que a Compass identifique anomalias na distribui√ß√£o de g√°s.

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, o Canvas Proposta de Valor permite √†s empresas como a Compass mapear e entender as necessidades e desafios de seus clientes. Com base nisso, a Compass pode desenvolver um servi√ßo focado na an√°lise de dados para identificar anomalias na distribui√ß√£o de g√°s, facilitando o monitoramento e controle dos processos. Ao abordar as dores e criar ganhos para seus clientes, a empresa alinha sua oferta de produtos e servi√ßos √†s expectativas e necessidades do mercado.


#### 4.1.5. Cinco For√ßas de Porter

&nbsp;&nbsp;&nbsp;&nbsp;Segundo Michael Porter (1986), no contexto de elabora√ß√£o de a√ß√µes estrat√©gicas dentro de uma corpora√ß√£o, torna-se essencial compreender como o setor em que ela atua no mercado interfere seu desempenho. Nesse sentido, as chamadas Cinco For√ßas de Porter s√£o um *framework* para a investiga√ß√£o de fatores-chave que exercem influ√™ncia no sucesso de um neg√≥cio, sendo eles: rivalidade entre concorrentes; poder de barganha dos fornecedores e de compradores; amea√ßa de novos entrantes, de produtos ou servi√ßos substitutos. A seguir, segue uma explica√ß√£o em t√≥picos do que cada amea√ßa significa.

- Rivalidade entre Concorrentes: descreve a concorr√™ncia entre empresas posicionadas em um mesmo setor, disputando por aten√ß√£o do p√∫blico atrav√©s de pre√ßos mais atrativos, diferencial de marca/produto/servi√ßo, melhor atendimento ao cliente, entre v√°rios fatores. 
- Poder de Barganha dos Fornecedores: explica como o poder dos fornecedores de aumentar os pre√ßos ou reduzir a qualidade pode impactar os custos e a lucratividade.
- Poder de Barganha dos Compradores: descreve como consumidores exigentes podem for√ßar as empresas a reduzir pre√ßos ou aumentar a qualidade.
- Amea√ßa de Novos Entrantes: representa as barreiras de entrada que protegem as empresas j√° posicionadas em um setor de novas concorr√™ncias, e como isso pode afetar a estrat√©gia em geral.
- Amea√ßa de Produtos ou Servi√ßos Substitutos: analisa como a disponibilidade de alternativas de produtos e servi√ßos pode limitar os pre√ßos e impactar a demanda.

<div align="center">

   <sub>Figura 3 - Modelo de Cinco For√ßas de Porter - Sprint 1 </sub>

   <img src="../assets/cincoForcas.png" width="70%"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>

</div>

 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://abre.ai/cincoforcas).

**Rivalidade entre concorrentes existentes**

&nbsp;&nbsp;&nbsp;&nbsp;A _Compass_ atua na distribui√ß√£o de g√°s natural, um setor n√£o conhecido pela competitividade em rela√ß√£o ao n√∫mero de participantes, mas formado por empresas bem estabelecidas em suas regi√µes, como a Naturgy, Bahiag√°s e a Casmig, e com isso podem disputar atrav√©s da qualidade dos insumos, efici√™ncia operacional e inova√ß√£o tecnol√≥gica. Entretanto, a Compass destaca-se como detentora das maior parte das marcas de g√°s natural no Sudeste, Sul e Nordeste ‚Äî suas regi√µes de atua√ß√£o ‚Äî, ou seja, representa uma das principais fornecedoras e investidoras do ramo, garantindo uma presen√ßa bem distribu√≠da em v√°rias localidades do Brasil, o que a evidencia como uma forte concorrente no mercado.

**Poder de barganha de fornecedores**

&nbsp;&nbsp;&nbsp;&nbsp;Sabendo-se que a Compass √© detentora das principais marcas de distribui√ß√£o de g√°s natural de suas regi√µes de atua√ß√£o, essa posi√ß√£o consolidada no mercado faz com que ela n√£o seja t√£o amea√ßada pelo poder de barganha dos fornecedores, afinal, possui um maior controle sobre sua cadeia de suprimentos e pode negociar termos favor√°veis com os fornecedores. Al√©m disso, a presen√ßa forte em diversas localidades estrat√©gicas, como no Sudeste, Sul e Nordeste, oferece √† empresa uma vantagem competitiva, pois pode diversificar suas fontes de abastecimento e, assim, reduzir sua depend√™ncia de um √∫nico fornecedor.

**Poder de barganha de compradores**

&nbsp;&nbsp;&nbsp;&nbsp;O poder de barganha dos compradores no setor de distribui√ß√£o de g√°s natural √© relativo √† categoria residencial do cliente, podendo-se citar dois exemplos de utiliza√ß√£o:<br>
&nbsp;&nbsp;&nbsp;&nbsp;No caso de clientes que residem em condom√≠nios ou pr√©dios que j√° possuem alguma marca da Compass como contratada para a distribui√ß√£o de g√°s ‚Äî cen√°rio especialmente favor√°vel em regi√µes onde as marcas da Compass s√£o predominantes ‚Äî, √© improv√°vel que os moradores tenham autonomia para escolher livremente a distribuidora para sua casa ou apartamento, afinal, a respectiva troca de infraestrutura acaba por ser invi√°vel ou n√£o permitida pela administra√ß√£o destes tipos residenciais, portanto, o poder de barganha dos compradores nesse cen√°rio acaba por ser reduzido.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Por outro lado, em rela√ß√£o ao uso dom√©stico e, em alguns casos, industrial, o cliente possui autonomia para escolher entre distribuidoras de g√°s natural, bem como pode decidir se ir√° utilizar o g√°s natural ou outra fonte alternativa. Neste caso, o poder de barganha dos compradores √© relativamente elevado, o que lhes d√° liberdade para pressionar distribuidoras de g√°s em rela√ß√£o a pre√ßo, qualidade do servi√ßo, infraestrutura, entre outros fatores, consistindo em um ponto de aten√ß√£o para as rela√ß√µes entre clientes e Compass, e para o seu posicionamento no mercado.<br>
&nbsp;&nbsp;&nbsp;&nbsp;Por fim, percebe-se que a amea√ßa do poder de barganha de compradores √© relativo a fatores como localidade e tipo de cliente, tornando-se clara a import√¢ncia de analisar as diferentes facetas dessa influ√™ncia por parte dos clientes finais, em prol de aprimorar o posicionamento da Compass no setor de distribui√ß√£o de g√°s natural.

**Amea√ßa entre novos entrantes**

&nbsp;&nbsp;&nbsp;&nbsp;A amea√ßa de novos entrantes no mercado de distribui√ß√£o de g√°s natural tamb√©m √© reduzida devido a algumas barreiras de entrada, como a infraestrutura inicial necess√°ria ‚Äî como a constru√ß√£o e manuten√ß√£o de redes de gasodutos ‚Äî, exige um grande investimento e tempo maior para implementa√ß√£o. Al√©m disso, o setor √© conhecido pela sua forte regulamenta√ß√£o, o que pode ser um obst√°culo para novos entrantes com menos experi√™ncia ou recursos para atender aos padr√µes de conformidade. Nesse sentido, ainda que se tenha a amea√ßa de novos competidores, considerando que a Compass √© uma das lideran√ßas do mercado, ela ainda tem a capacidade de investir em inova√ß√£o, melhoria cont√≠nua de seus servi√ßos e companhias menores, refor√ßando sua posi√ß√£o de lideran√ßa no mercado.

**Amea√ßa de produtos substitutos**

&nbsp;&nbsp;&nbsp;&nbsp;O g√°s natural √© uma fonte de energia limpa e eficiente, frequentemente utilizada em aplica√ß√µes residenciais, de transporte, comerciais e industriais (Reis, 2018). Existem alternativas energ√©ticas, como eletricidade, biocombust√≠veis e energia solar, cada uma delas possuindo seus benef√≠cios e limita√ß√µes em termos de custo, infraestrutura e aplicabilidade. √â poss√≠vel que, conforme circunst√¢ncias de localidades e suas distribuidoras dispon√≠veis, precifica√ß√£o, finalidade e outros fatores, possa se optar por fontes de energia diferentes do g√°s natural. Com isso, entende-se que a amea√ßa de produtos substitutos √© relativamente moderada.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, evidencia-se a import√¢ncia de se implementar do modelo de Cinco For√ßas de Porter em rela√ß√£o √† *Compass* com o objetivo de compreender as influ√™ncias que o setor de distribui√ß√£o de g√°s natural exerce na atua√ß√£o e desempenho da empresa. O entendimento desses poss√≠veis pontos de amea√ßa setoriais ‚Äî como as fontes de energias alternativas ao g√°s natural e o poder de barganha de fornecedores ‚Äî √© imprescind√≠vel para a delimita√ß√£o do presente projeto, que visa elaborar um modelo de predi√ß√£o de anomalias no consumo e preven√ß√£o de fraudes, afinal, para isso, deve-se sempre considerar os fatores de risco, tanto internos como externos, a fim de uma solu√ß√£o vi√°vel e pr√°tica para as necessidades da empresa parceira.

#### 4.1.6. Matriz de Riscos

&nbsp;&nbsp;&nbsp;&nbsp;Segundo os autores Tomaz; Gagliasso; Malavota (2020), uma Matriz de Riscos √© uma poderosa ferramenta de gest√£o respons√°vel por fornecer aux√≠lio para que grupos possam identificar riscos e amea√ßas que possam impactar negativamente um determinado projeto. Uma Matriz de Riscos √© elaborada partindo da probabilidade de algo negativo acontecer durante o projeto e relacionando isso com o impacto que esse algo teria sobre o projeto. Ainda de acordo com os autores, um fator muito importante de uma Matriz de Riscos √© a sua iteratividade. De maneira simplificada, um grupo trabalhando em um projeto deve sempre estar retornando √† Matriz de Riscos a fim de entender se os riscos previamente identificados ainda carregam a mesma probabilidade/impacto de acontecerem. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Al√©m da Matriz de Riscos, times tamb√©m podem se beneficiar da elabora√ß√£o de uma Matriz de Oportunidades, que tem como objetivo identificar as principais ocorr√™ncias das quais o grupo tocando o projeto pode tirar maior proveito. Assim como a Matriz de Riscos, na de oportunidades o foco est√° em aproveitar as a√ß√µes que requerem menor esfor√ßo, possuem maior probabilidade de ocorrerem e impactem mais o projeto.   

<div align="center">
   
   <sub>Figura 4 - Matriz de Riscos e Oportunidades Sprint 1 </sub>

   <img src="../assets/matriz_de_riscos.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Em rela√ß√£o aos riscos identificados para este projeto, temos:
- **N√£o-anonimiza√ß√£o dos dados fornecidos**: Caso a empresa forne√ßa dados que n√£o estejam anonimizados de acordo com a LGPD (<a href="https://www.planalto.gov.br/ccivil_03/_ato2015-2018/2018/lei/l13709.htm">Lei Geral de Prote√ß√£o dos Dados</a>), o projeto pode sofrer com impactos legais e san√ß√µes da ANPD (<a href="https://www.gov.br/anpd/pt-br">Autoridade Nacional de Prote√ß√£o dos Dados</a>). O plano de a√ß√£o para caso tal risco venha a se concretizar inclui a pr√≥pria equipe realizando a anonimiza√ß√£o dos dados por meio de t√©cnicas como mascaremento. 
- **Inconsist√™ncia nos dados fornecidos**: Com inconsist√™ncia, apontamos que os dados fornecidos podem necessitar de uma quantidade de tratamento e pr√©-processamento, que constitui o processo de limpar e remover *outliers* dos dados, maior que a esperada, causando atrasos no projeto. √â entend√≠vel que, num projeto de ci√™ncia de dados, boa parte do tempo √© despendido adequando a fonte de dados. Entretanto, uma fonte de dados muito inconsistente pode fazer com que tenhamos que passar muito tempo limpando esses dados (removendo valores nulos, tratando _outliers_, etc). Como plano de a√ß√£o para este risco, definimos que √© necess√°rio estudar os dados a fim de entender a qualidade deles e, caso necess√°rio, tra√ßar rotas mais ideais para se realizar a limpeza, diminuindo o tempo necess√°rio para adequar os dados para rodar um modelo preditivo. 
- **Baixo volume de dados**: No mundo de ci√™ncia de dados e cria√ß√£o de modelos preditivos, quase sempre √© regra que ‚Äúquanto mais dados, melhor‚Äù. Isso ocorre porque modelos desse tipo precisam, antes de realizar qualquer predi√ß√£o, ser treinados. Dessa forma, uma baixa quantidade de dados pode inferir diretamente na qualidade de treinamento dos modelos, o que resultaria em uma baixa acur√°cia. No presente projeto, identificamos que possu√≠mos apenas 5 meses de dados, uma quantidade considerada relativamente baixa. Dessa forma, um impacto que pode decorrer disso √© que n√£o √© poss√≠vel observar efeitos de sazonalidade nos dados. Um poss√≠vel plano de a√ß√£o para este risco inclui realizar um tratamento mais aprofundado dos dados, a fim de fazer com que eles sejam mais √∫teis para o treinamento dos modelos. Al√©m disso, podem ser utilizadas t√©cnicas para se gerar novos dados a partir de dados existentes e tamb√©m realizar o enriquecimento dos dados com fontes extras, como API's para obter a temperatura de determinado local. 
- **Comunica√ß√£o ineficiente com _stakeholders_**: Como o projeto que est√° sendo desenvolvido √© para uma empresa parceira, √© de extrema import√¢ncia se atentar aos requisitos que essa empresa coloca sobre o projeto e o que ela espera da solu√ß√£o que estamos desenvolvendo. Dessa forma, uma comunica√ß√£o ineficiente com esta empresa pode levar a mal-entendidos quanto aos requisitos dos projetos. Para amenizar este risco, o grupo pretende estabelecer uma comunica√ß√£o objetiva com os _stakeholders_, sempre deixando claro o que entendemos do projeto e o que podemos e pretendemos entregar.

&nbsp;&nbsp;&nbsp;&nbsp;Em rela√ß√£o √†s oportunidades, identificamos:
- **Automatiza√ß√£o do monitoramento**: O processo de executar o modelo e obter poss√≠veis clientes que estejam fraudando o consumo de g√°s pode ser automatizado a fim de ser executado, por exemplo, 1 vez por semana, a fim de sempre manter as informa√ß√µes atualizadas. Ao passo em que isto n√£o est√° incluido no escopo do projeto, √© uma oportunidade que pode ser explorada futuramente e pode ser realizada a partir do desenvolvimento de uma *pipeline* de an√°lise de dados, que consegue receber um *input* de novos dados e classificar aqueles que, por exemplo, possuam um consumo anormal.
- **Cria√ß√£o de novos dados por meio de combina√ß√µes**: Em projetos de an√°lise de dados, uma oportunidade comum de se aproveitar √© a cria√ß√£o de novos dados a partir da combina√ß√£o de dados j√° existentes. Por exemplo, a partir da altura e peso de um cliente, podemos inferir o seu IMC. Neste projeto, podemos utilizar os dados dispon√≠veis para gerar novas colunas com dados que possam servir para aumentar a acur√°cia do modelo. Al√©m disso, foi poss√≠vel identificar a oportunidade de se utilizar uma API externa que disponibiliza dados hist√≥ricos de temperatura para diversas cidades. A partir disso, podemos enriquecer os dados com valores hist√≥ricos de temperatura, que podem ter rela√ß√£o direta com o consumo de g√°s da regi√£o.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, percebe-se como uma ferramenta como a Matriz de Riscos e Oportunidades pode ser extremamente √∫til, nos ajudando a identificar, durante todo o decorrer do projeto, os riscos que podem afetar o nosso desenvolvimento. Por fim, a elabora√ß√£o dessas matrizes serve para que o que o grupo fique atento a essas amea√ßas e consiga as combater caso ocorram. 

#### 4.1.7. Personas

&nbsp;&nbsp;&nbsp;&nbsp;Segundo Page Laubheimer, especialista s√™nior em Experi√™ncia do Usu√°rio no Nielsen Norman Group, personas s√£o representa√ß√µes fict√≠cias - mas que guardam caracter√≠sticas identific√°veis e reais - do p√∫blico alvo de determinado produto. Nesse contexto, o design de produtos e servi√ßos com foco na experi√™ncia do usu√°rio se mostra mais assertivo, pois identifica mais claramente quais s√£o as dores que o p√∫blico sofre e quais s√£o os seus desejos, permitindo o desenvolvimento de solu√ß√µes que fazem sentido para seu usu√°rio final.

&nbsp;&nbsp;&nbsp;&nbsp;√â poss√≠vel criar 3 tipos diferentes de persona, a depender do objetivo da solu√ß√£o, dos aspectos que se quer conhecer sobre o p√∫blico alvo e do m√©todo utilizado para o desenvolvimento da representa√ß√£o: proto personas, personas qualitativas e personas estat√≠sticas.

- A proto persona √© uma representa√ß√£o feita pelo time de desenvolvimento a partir de ideias pr√©-concebidas sobre seu p√∫blico alvo e sem a interfer√™ncia de dados externos √†queles j√° conhecidos;
- A persona qualitativa √© um retrato do usu√°rio feito por meio de entrevistas com 5 a 30 pessoas e que busca identificar aspectos subjetivos e pessoais do p√∫blico, como suas dores, seus desejos e quais aspectos da solu√ß√£o mais est√£o em conformidade com o que o consumidor espera;
- A persona estat√≠stica √© feita por meio de processos de entrevistas com pelo menos 100 pessoas e tem por objetivo encontrar interesses em comum dentre um grande n√∫mero de poss√≠veis usu√°rios finais.

&nbsp;&nbsp;&nbsp;&nbsp;O processo de cria√ß√£o e desenvolvimento de persona escolhido pela equipe foi o de proto persona, tendo em vista o tempo in√°bil para coleta de respostas da equipe de dados das empresas parceiras *Compass* e *IOLIT* e constru√ß√£o de personas qualitativas por meio de entrevistas e respostas com um pequeno grupo de pessoas do p√∫blico alvo - profissionais da equipe de dados que utilizar√£o o modelo preditivo em seu dia a dia na *startup IOLIT*.

&nbsp;&nbsp;&nbsp;&nbsp;Para embasamento e maior fidedignidade da primeira proto persona, o Thiago, foi utilizado o √∫ltimo relat√≥rio (edi√ß√£o de 2023) da pesquisa *State of Data*, realizada anualmente pela comunidade *Data Hackers* e que coleta dados sobre os profissionais e o ambiente profissional de dados no Brasil. Al√©m disso, foi utilizado o site de apresenta√ß√£o da *startup* parceira, de forma a encontrar quais aspectos eram mais valorizados e que s√£o a base para o fit cultural da equipe de profissionais da mesma. Al√©m disso, para o embasamento de segunda persona, a Thalyta, foi utilizada a edi√ß√£o de 2021-2022 do Panorama de Mercado de Product Management, produzido pela PM3, do Grupo Alura. O referido relat√≥rio foi uma importante refer√™ncia para a interpreta√ß√£o de dados demogr√°ficos e de comportamento relacionados ao perfil desejado, que busca representar um cargo de gest√£o de neg√≥cios da Compass.

&nbsp;&nbsp;&nbsp;&nbsp;Informa√ß√µes como sexo, idade m√©dia, n√≠vel de cargo, forma√ß√£o m√©dia e localidade foram utilizadas para cria√ß√£o dos perfis demogr√°ficos, bem como para embasar a biografia, os interesses e motiva√ß√µes das proto personas, as quais pode ser analisada nas imagens abaixo.

<div align="center">
   
   <sub>Figura 5 - Persona 1 </sub>

   <img src="../assets/persona.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;A primeira persona cont√©m as seguintes informa√ß√µes:

- Perfil: possui nome, g√™nero, idade, cidade de moradia e posi√ß√£o ou cargo desempenhado na empresa em que trabalha.
   - Nome: Thiago Ferreira da Silva;
   - G√™nero: Masculino;
   - Idade: 27 anos;
   - Cidade: S√£o Paulo - SP;
   - Posi√ß√£o: CIentista de dados;
   <br>

- Biografia: conta a origem e caracter√≠sticas pessoais e culturais da persona.
   - Thiago nasceu no Rio de Janeiro e come√ßou a se interessar por matem√°tica durante o ensino Ensino Fundamental, quando ajudou seu av√¥ a cuidar das finan√ßas de seu bar, enquanto seu hobby favorito era jogar videogame. Decidiu fazer sua gradua√ß√£o em Ci√™ncia da Computa√ß√£o na UFRJ e se especializou em Intelig√™ncia Artificial. Hoje, atua na empresa Compass e √© um dos chefes de inova√ß√£o da √°rea de dados e na forma√ß√£o da startup IOLIT, na qual emprega seus conhecimentos de neg√≥cios e matem√°tica para inovar os processos da empresa e na experi√™ncia de clientes.
   <br>

- Frustra√ß√µes: dores que o p√∫blico alvo carrega diariamente e que se relacionam com a proposta de valor do produto desenvolvido pela equipe.
   - Precisa fazer an√°lises de dados demoradas;
   - Seu fluxo de trabalho possui poucas automa√ß√µes;
   - N√£o sabe como encontrar anomalias entre os dados.
   <br>

- Motiva√ß√µes: aspectos da personalidade e valores que s√£o importantes para a persona e que podem ser identificadas pela semelhan√ßa de valores das empresas parceiras.
   - Interessa-se pela inova√ß√£o e por projetos de impacto cultural e deseja ser um gestor s√™nior que identifique oportunidades de melhoria econ√¥mica e social de sua empresa por meio da tecnologia de dados e intelig√™ncia artificial.
   <br>
- Interesses: gostos e prefer√™ncias expostos pelo p√∫blico alvo e que se relacionam com a solu√ß√£o proposta.
   - Intelig√™ncia artificial;
   - Estat√≠stica;
   - Neg√≥cios.

<div align="center">
   
   <sub>Figura 6 - Persona 2 </sub>

   <img src="../assets/thalyta.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;A segunda persona cont√©m as seguintes informa√ß√µes:

- Perfil:
   - Nome: Thalyta Meideiros Souza;
   - G√™nero: Feminino;
   - Idade: 34 anos;
   - Cidade: S√£o Paulo - SP;
   - Posi√ß√£o: Gerente de Produto;
   <br>

- Biografia:
   - Nascida e criada em S√£o Paulo, Thalyta sempre se destacou pela sua curiosidade e criatividade diante de desafios, al√©m de seu interesse por Inform√°tica, o que a levou a cursar Engenharia de Produto na Universidade Presbiteriana Mackenzie e, posteriormente, fazer um MBA em Tecnologia e Inova√ß√£o. Ao adentrar √†  Compass e conhecer a iniciativa da IOLIT, Thalyta enxergou a possibilidade de conhecer mais sobre o mercado de distribui√ß√£o g√°s e auxiliar na moderniza√ß√£o do gerenciamento do consumo. Mais do que mergulhar em novos projetos, Thalyta ama praticar nata√ß√£o nos fins de semana.
   <br>

- Frustra√ß√µes:
   - Possui dificuldade em determinar um timing adequado para o oferecimento personalizado de propaganda;
   - Possui dificuldade em interpretar o comportamento de consumo de clientes da Compass atrav√©s dos m√©todos manuais; 
   - Sente necessidade de implementar tecnologia nos processos de gest√£o de consumo.
   <br>

- Motiva√ß√µes:
   - Acredita no poder da integra√ß√£o de tecnologia inovadora para o aprimoramento de processos internos, de forma a diminuir custos e aumentar a receita empresarial. Planeja no futuro ser s√≥cia da Compass.
   <br>
- Interesses: gostos e prefer√™ncias expostos pelo p√∫blico alvo e que se relacionam com a solu√ß√£o proposta.
   - Neg√≥cios;
   - Vendas;
   - Tecnologia.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, as personas do Thiago e da Thalyta devem servir como guias estrat√©gicos para a equipe de desenvolvimento. Utilizando esses perfis, o time poder√° direcionar o modelo preditivo de maneira mais precisa, garantindo que a solu√ß√£o final atenda n√£o apenas √†s expectativas t√©cnicas, mas tamb√©m √†s necessidades emocionais e funcionais do p√∫blico representado. Isso aumentar√° a probabilidade de entregar um produto que realmente agrega valor e se aproxime das motiva√ß√µes e prefer√™ncias reais dos usu√°rios finais.

##### 4.1.7.1 User Stories

&nbsp;&nbsp;&nbsp;&nbsp;As *User Stories*, segundo *Max Rehkopf, Product Marketing Manager na Atlassian*, s√£o explica√ß√µes simples e sucintas de funcionalidades de um produto feitas com base na perspectiva e experi√™ncia do usu√°rio final. Nesse sentido, essa ferramenta √© utilizada pela equipe de desenvolvimento porque traz benef√≠cios e facilita√ß√µes, como:
- Entendimento de como as funcionalidades da solu√ß√£o entregam valor para o cliente final;
- Prioriza√ß√£o de tarefas tendo em vista as que mais impactam o usu√°rio;
- Melhor entendimento de o que est√° sendo desenvolvido e o porqu√™ de estar sendo desenvolvido.

&nbsp;&nbsp;&nbsp;&nbsp;A estrutura de uma *User Story* traz a descri√ß√£o da funcionalidade, o motivo pelo qual ela √© importante para o usu√°rio final e qual valor ela entrega ao mesmo, e usualmente √© escrita em linguagem n√£o-t√©cnica na 1¬™ pessoa do singular.

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, as hist√≥rias de usu√°rio tamb√©m trazem consigo os crit√©rios de aceite, par√¢metros utilizados para validar como e em quais condi√ß√µes uma *User Story* √© atendida correta e completamente.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, as *User Stories* desenvolvidas para atender √†s necessidades da persona de Thiago Ferreira, apresentada anteriormente na se√ß√£o 4.1.7, bem como seus respectivos crit√©rios de aceite, podem ser vistos nas tabelas abaixo:

<div align="center">

<sub>Tabela 1 - User Stories </sub>

</div>

Identifica√ß√£o | 01
--- | ---
Persona | Thiago Ferreira da Silva
User Story | "Como cientista de dados da startup IOLIT, gostaria de utilizar solu√ß√µes tecnol√≥gicas que estejam na plataforma Google Collaboratory, para facilitar a utiliza√ß√£o de bibliotecas de an√°lise de dados enquanto compartilho o material com minha equipe em tempo real."
Crit√©rio de aceite 1 | CR1: Thiago acessa a internet logado com sua conta Google e entra na interface da ferramenta Google Collaboratory.
Crit√©rio de aceite 2 | CR2: Ele acessa o notebook no qual o modelo preditivo foi desenvolvido e pode compartilh√°-lo com sua equipe.

Identifica√ß√£o | 02
--- | ---
Persona | Thiago Ferreira da Silva
User Story | "Como cientista de dados da startup IOLIT, gostaria de utilizar solu√ß√µes tecnol√≥gicas nas quais n√£o preciso instalar depend√™ncias e bibliotecas no meu computador, para que meus equipamentos mantenham um bom desempenho e o processo de setup n√£o seja t√£o demorado."
Crit√©rio de aceite 1 | CR1: Thiago acessa o notebook no qual o modelo preditivo foi desenvolvido.
Crit√©rio de aceite 2 | CR2: Ele aciona a primeira c√©lula de c√≥digo do notebook, na qual s√£o importadas as bibliotecas utilizadas pelo modelo preditivo.
Crit√©rio de aceite 3 | CR3: Ao terminar de acionar a primeira c√©lula, Thiago pode utilizar as bibliotecas sem instal√°-las em seu computador.

Identifica√ß√£o | 03
--- | ---
Persona | Thiago Ferreira da Silva
User Story | "Como cientista de dados da startup IOLIT, gostaria de utilizar automa√ß√µes e intelig√™ncia artificial no meu processo de trabalho, para que as an√°lises de dados que eu j√° fa√ßo sejam executadas com maior rapidez e para que eu descubra insights valiosos referentes aos padr√µes de consumo de g√°s dos clientes."
Crit√©rio de aceite 1 | CR1: Thiago faz o upload das bases de dados que gostaria que fossem analisadas pelo modelo preditivo no notebook.
Crit√©rio de aceite 2 | CR2: Ao acionar as c√©lulas de c√≥digo referentes ao modelo preditivo, ele recebe os insights associados aos dados de consumo de g√°s das bases de dados analisadas.
Crit√©rio de aceite 3 | CR3: Ao acionar as c√©lulas de c√≥digo referentes √† visualiza√ß√£o gr√°fica, Thiago recebe gr√°ficos relevantes contendo as an√°lises j√° recebidas anteriormente.

Identifica√ß√£o | 04
--- | ---
Persona | Thalyta Meideiros Souza
User Story |"Como gerente de produto da Compass, quero ter acesso a an√°lises simplificadas que me permitam visualizar insights gerados pelo modelo preditivo, para que eu possa compreender as tend√™ncias e tomar decis√µes informadas, mesmo sem profundo conhecimento em an√°lise de dados."
Crit√©rio de aceite 1 | CR1: Ao acessar o sistema, Thalyta visualiza uma an√°lise que apresenta gr√°ficos e indicadores chave que resumem as tend√™ncias e padr√µes identificados nos dados, permitindo uma compreens√£o r√°pida.
Crt√©rio de aceite 2 | CR2:  Ao interpretar os dados apresentados, Thalyta consegue compartilhar a an√°lise com sua equipe e alinhar as estrat√©gias de gerenciamento do produto de acordo com os insights gerados.
Crit√©rio de aceite 3 | CR3: Ao acessar a an√°lise de dados, Thalyta visualiza as m√©dias e os desvios padr√£o, apresentadas em gr√°ficos e tabelas de f√°cil entendimento.



Identifica√ß√£o | 05
--- | ---
Persona | Thalyta Meideiros Souza
User Story | "Como gerente de produto da Compass, quero que a utiliza√ß√£o do notebook para importar os dados fosse simples e f√°cil, e que os dados utilizados nas an√°lises estejam devidamente limpos e tratados, para assegurar que as estrat√©gias de gerenciamento de produto sejam baseadas em informa√ß√µes precisas e confi√°veis."
Crit√©rio de aceite 1 | CR1: Ao acessar as an√°lises, Thalyta visualiza dados que foram limpos e tratados, sem valores ausentes significativos.
Crit√©rio de aceite 2 | CR2: Ao precisar fazer o upload da base de dados, Thalyta recebe um tutorial de como realizar a importa√ß√£o e a execu√ß√£o dos dados no notebook.





&nbsp;&nbsp;&nbsp;&nbsp;Assim, √© esperado que as User Stories sejam utilizadas pela equipe de desenvolvimento durante a constru√ß√£o da solu√ß√£o final, de forma a n√£o s√≥ melhorar a produtividade e a prioriza√ß√£o de tarefas conforme a metodologia de trabalho √°gil, mas tamb√©m para que o produto final atenda as necessidades principais de seus usu√°rios e entregue valor em todas as suas funcionalidades.

#### 4.1.8. Jornadas do Usu√°rio

&nbsp;&nbsp;&nbsp;&nbsp;O mapeamento da jornada do usu√°rio, segundo *Gerry McGovern*, √© essencial para identificar os pontos de contato mais importantes onde o conte√∫do pode ser mais eficaz. Dessa maneira, ao mapear cada etapa do usu√°rio, desde o primeiro contato at√© a conclus√£o da intera√ß√£o com o produto, torna-se poss√≠vel entender suas expectativas em cada fase, prever poss√≠veis dificuldades e desenvolver estrat√©gias eficazes para resolv√™-las. Consequentemente, esse processo √© fundamental para criar uma experi√™ncia mais convincente, garantindo que o produto atenda √†s necessidades dos usu√°rios.

&nbsp;&nbsp;&nbsp;&nbsp;Com isso em mente, desenvolvemos um template de mapeamento da jornada do usu√°rio para nossas personas. Esse template detalha as experi√™ncias durante as diferentes fases de utiliza√ß√£o do nosso produto: importa√ß√£o dos dados, visualiza√ß√£o e an√°lise dos dados, e conclus√£o sobre os dados.

&nbsp;&nbsp;&nbsp;&nbsp;Para melhor compreender o processo vivenciado por nossos usu√°rios, nosso mapa de jornada inclui os seguintes elementos:

- **Atividade:** S√£o as a√ß√µes que o usu√°rio realiza em cada fase.
- **Necessidades:** S√£o as principais necessidades do usu√°rio durante as atividades.
- **Emo√ß√µes:** S√£o os sentimentos do usu√°rio ao longo da jornada.
- **Oportunidades:** S√£o as solu√ß√µes para os problemas identificados que podem melhorar a experi√™ncia do usu√°rio.

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s entender a estrutura do nosso template de mapeamento da jornada do usu√°rio, aplicamos esse modelo √† persona Thiago, um cientista de dados da _Compass_ e da _startup_ IOLIT, que recebeu a tarefa de identificar e prever poss√≠veis fraudes e anomalias no consumo de g√°s natural utilizando um modelo preditivo. Ele espera poder analisar os dados de forma r√°pida e eficiente, visualizando informa√ß√µes claras e precisas. Dessa maneira, detalhamos como Thiago interage com nosso produto ao longo das diferentes fases do processo, explorando suas a√ß√µes, necessidades, emo√ß√µes e as oportunidades de melhoria identificadas, atendendo as User Stories 2 e 3.

<div align="center">
   
   <sub>Figura 7 - Persona 1 </sub>

   <img src="../assets/persona.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

<div align="center">
   
   <sub>Figura 8 - Jornada do Usu√°rio 1 </sub>

   <img src="../assets/jornada do usu√°rio.jpg">
  
  <sup>Fonte: Material produzido pelos autores (2024)</sup>
  
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://abrir.link/agLrh)

&nbsp;&nbsp;&nbsp;&nbsp;A jornada do usu√°rio cont√©m as seguintes informa√ß√µes:

- *Importa√ß√£o dos dados:*

  - **Atividades**: Thiago abre o notebook do modelo preditivo e realiza o upload de um arquivo .csv de uma base de dados.

  - **Necessidades**: Durante a fase de importa√ß√£o dos dados, Thiago sente a necessidade de conseguir realizar a visualiza√ß√£o dos dados; manter a seguran√ßa e a integridade dos dados; saber utilizar o notebook; e receber insights de f√°cil compreens√£o.

  - **Emo√ß√µes**: Thiago sente medo de perder os dados e sente inseguran√ßa de n√£o saber utilizar o notebook.

  - **Oportunidades**: Para ajudar Thiago, a equipe oferece uma instru√ß√£o de como realizar a importa√ß√£o de dados no notebook.

- *Visualiza√ß√£o e an√°lise dos dados:*

   - **Atividades**: Thiago clica em executar os c√≥digos; visualiza as tabelas e os gr√°ficos dos dados importados, a partir da programa√ß√£o do modelo; cria hip√≥teses sobre as poss√≠veis anomalias  no consumo de g√°s; analisa os insights para valisar ou refutar as hip√≥teses levantadas.

   - **Necessidades**: Durante a fase de visualiza√ß√£o e an√°lise, Thiago sente a necessidade de receber dados confi√°veis e √∫teis para uma an√°lise mais r√°pida e eficiente; obter gr√°ficos claros que facilitem a interpreta√ß√£o precisa dos dados; e alcan√ßar seu objetivo de identificar anomalias e fraudes no consumo de g√°s natural rapidamente.

   - **Emo√ß√µes**: Thiago fica concentrado e pensativo ao realizar a an√°lise e, logo em seguida, fica contente por conseguir visualizar os gr√°ficos e as tabelas gerados.

   - **Oportunidades**: Como solu√ß√µes para atender suas necessidades, poderiam ser fornecidos guias para interpretar os modelos de gr√°ficos apresentados, e de disponibilizar um documento que certifique que os dados est√£o limpos, garantindo sua confiabilidade para as an√°lises.

- *Conclus√£o sobre os dados:*

   - **Atividades**: Thiago valida ou refuta as hip√≥teses levantadas ap√≥s uma an√°lise mais detalhada dos dados; identifica anomalias no consumo de g√°s; e registra a conclus√£o de sua an√°lise em um documento.

   - **Necessidades**:Durante a fase de conclus√£o sobre os dados, Thiago sente a necessidade de compartilhar os resultados com a equipe e de receber feedback sobre sua an√°lise.

   - **Emo√ß√µes**: Thiago fica feliz por conseguir interpretar o modelo e identificar as anomalias.

   - **Oportunidades**: Para melhorar a organiza√ß√£o e clareza dos registros, desenvolver um modelo/template para documentar as conclus√µes da an√°lise, garantindo que os resultados sejam apresentados de forma acess√≠vel e estruturada para toda a equipe.

&nbsp;&nbsp;&nbsp;A seguir, tem-se a jornada do usu√°rio baseada na persona Thalyta, a qual representa a perspectiva de uma personalidade de neg√≥cios no time da Compass/IOLIT que pretende utilizar o modelo preditivo para n√£o somente implementar a√ß√µes em rela√ß√£o a poss√≠veis anomalias de consumo, mas tamb√©m para elaborar estrat√©gias comerciais de marketing, de forma a tomar decis√µes baseadas em dados fact√≠veis. √â importante salientar que, diferentemente do Thiago, a Thalyta n√£o possuir√° contato com a execu√ß√£o em si do modelo, somente com as hip√≥teses e resultados alcan√ßados, sintetizados em uma documenta√ß√£o feita pelo Thiago. Tal framework foi preenchido com base nas User Stories 04 e 05. 

<div align="center">
   
   <sub>Figura 9 - Persona 2 </sub>

   <img src="../assets/thalyta.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

<div align="center">
   
   <sub>Figura 10 - Jornada de Usu√°rio da Persona 2, Thalyta </sub>

   <img src="../assets/jornadaUsuarioThalyta.jpg"> 

<sup>Fonte: Material produzido pelos autores (2024)</sup>

</div>

 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://bit.ly/4dJ22hB).<br>

 &nbsp;&nbsp;&nbsp;&nbsp;**Thalyta Meideiros Souza**

 - *Recebimento de insights*:

   - **Atividades**: Thalyta abre a documenta√ß√£o de an√°lises, hip√≥teses, tabelas e gr√°ficos sobre o consumo de g√°s, elaborada pelo Thiago;

   - **Necessidades**: gerenciar o comportamento de consumo dos clientes da Compass, identificando anormalidades de forma eficiente; obter insights de f√°cil compreens√£o sobre o comportamento de consumo dos clientes;

   - **Emo√ß√µes**: neste momento inicial, Thalyta sente peocupa√ß√£o pela falta de repert√≥rio em seu dia a dia comercial para a tomada de decis√µes relacionadas ao consumo de clientes da Compass. Ent√£o, com o recebimento dos insights do Thiago, sente curiosidade com o que poderia extrair de an√°lise.
   
 - *Visualiza√ß√£o e an√°lise dos insights*:

   - **Atividades**: ent√£o, Thalyta come√ßa a analisar as tabelas, gr√°ficos e hip√≥teses da documenta√ß√£o.

   - **Necessidades**: obter uma interpreta√ß√£o facilitada da documenta√ß√£o e dos modelos para decis√µes comerciais estrat√©gicas com base no consumo dos clientes;

   - **Emo√ß√µes**: quando consegue visualizar o repert√≥rio de informa√ß√µes, sente-se feliz e satisfeita pela utilidade do sistema como ferramenta de aux√≠lio e colabora√ß√£o entre a equipe.

 - *Conclus√£o sobre os dados*:

   - **Atividades**: a partir da visualiza√ß√£o dos dados, Thalyta registra conclus√µes mais diretas a partir do modelo e elabora novas hip√≥teses, caso necess√°rias. Ent√£o, cria estrat√©gias comerciais, de marketing ou outro campo de interesse, com a inten√ß√£o de validar essas hip√≥teses e/ou tomar decis√µes.

   - **Necessidades**: nessa etapa, Thalyta sente necessidade de extrair conclus√µes sobre o consumo de g√°s com base nos dados coletados, bem como aprimorar a estrat√©gia de comunica√ß√£o, de marca e o relacionamento com o cliente da Compass.

   - **Emo√ß√µes**: sente felicidade por conseguir interpretar o modelo e elaborar estrat√©gias com sucesso.

- **Oportunidades**: em todas as fases, para aumentar a efici√™ncia da an√°lise da Thalyta, a equipe pode disponibilizar uma documenta√ß√£o de orienta√ß√µes sobre a interpreta√ß√£o dos gr√°ficos e tabelas.

&nbsp;&nbsp;&nbsp;A partir da elabora√ß√£o da Jornada de Usu√°rio, foi poss√≠vel estimular a empatia com as duas personas idealizadas pela equipe, elaboradas para refletir dois tipos de perfis de usu√°rio da equipe de Tecnologia e Produto da Compass que poder√£o se beneficiar da ferramenta. Com isso, foi poss√≠vel alcan√ßar suposi√ß√µes essenciais em rela√ß√£o √†s tarefas, necessidades, emo√ß√µes e oportunidades em toda a trajet√≥ria de uso do modelo preditivo, obtendo-se insights valiosos sobre os poss√≠veis pontos de aten√ß√£o e melhoria para o seu desenvolvimento.

#### 4.1.9 Pol√≠tica de Privacidade

&nbsp;&nbsp;&nbsp;A pol√≠tica de privacidade √© um documento que busca descrever a forma que uma organiza√ß√£o coleta, utiliza, armazena e, principalmente, protege os dados pessoais de seus usu√°rios, em conformidade com a Lei Geral de Prote√ß√£o de Dados (LGPD) no Brasil. A LGPD √© respons√°vel por estabelecer diretrizes claras sobre o tratamento de dados pessoais, assegurando que os direitos dos titulares sejam devidamente respeitados. A pol√≠tica de privacidade garante um tratamento de dados de forma √©tica, segura e em conformidade com a legisla√ß√£o vigente desde 2020 no pa√≠s.

**Data da √∫ltima atualiza√ß√£o da pol√≠tica de privacidade:** Aug 2024.

&nbsp;&nbsp;&nbsp;**A COMPASS GAS E ENERGIA S.A**, pessoa jur√≠dica de direito privado, com sede na Avenida Brigadeiro Faria Lima, n¬∫ 4.100, 16¬∫ andar, Sala 24, Itaim Bibi, S√£o Paulo, SP, Brasil, CEP 04538-132, Telefone (11) 3897-9797, E-mail ri@compassbr.com, inscrita no CNPJ/MF sob o n¬∫ 21.389.501/0001-81, leva a sua privacidade a s√©rio e zela pela seguran√ßa e prote√ß√£o de dados de todos os seus clientes, parceiros, fornecedores e usu√°rios (‚ÄúUsu√°rios‚Äù ou ‚Äúvoc√™‚Äù) do site ‚Äúwww.compassbr.com/‚Äù e qualquer outro site, empresa, aplicativo operado pelo prestador de servi√ßo (aqui designados, simplesmente, ‚Äúempresa‚Äù).

&nbsp;&nbsp;&nbsp;Esta Pol√≠tica de Privacidade (‚ÄúPol√≠tica de Privacidade‚Äù) destina-se a inform√°-lo sobre o modo como n√≥s utilizamos e divulgamos informa√ß√µes coletadas em suas visitas √† nossa empresa e em mensagens que trocamos com voc√™ (‚ÄúComunica√ß√µes‚Äù).

&nbsp;&nbsp;&nbsp;**AO ACESSAR A EMPRESA, ENVIAR COMUNICA√á√ïES OU FORNECER QUALQUER TIPO DE DADO PESSOAL, VOC√ä DECLARA ESTAR CIENTE E DE ACORDO COM ESTA POL√çTICA DE PRIVACIDADE,** a qual descreve as finalidades e formas de tratamento de seus dados pessoais que voc√™ disponibilizar na empresa.

##### Defini√ß√µes

- **Dados Pessoais**: Qualquer informa√ß√£o que, direta ou indiretamente, identifique ou possa identificar uma pessoa natural, como por exemplo, nome, CPF, data de nascimento, endere√ßo IP, dentre outros.
- **Dados Pessoais Sens√≠veis**: Qualquer informa√ß√£o que revele, em rela√ß√£o a uma pessoa natural, origem racial ou √©tnica, convic√ß√£o religiosa, opini√£o pol√≠tica, filia√ß√£o a sindicato ou a organiza√ß√£o de car√°ter religioso, filos√≥fico ou pol√≠tico, dado referente √† sa√∫de ou √† vida sexual, dado gen√©tico ou biom√©trico.
- **Tratamento de Dados Pessoais**: Qualquer opera√ß√£o efetuada no √¢mbito dos Dados Pessoais, por meio de meios autom√°ticos ou n√£o, tal como a recolha, grava√ß√£o, organiza√ß√£o, estrutura√ß√£o, armazenamento, adapta√ß√£o ou altera√ß√£o, recupera√ß√£o, consulta, utiliza√ß√£o, divulga√ß√£o por transmiss√£o, dissemina√ß√£o ou, alternativamente, disponibiliza√ß√£o, harmoniza√ß√£o ou associa√ß√£o, restri√ß√£o, elimina√ß√£o ou destrui√ß√£o.
- **Leis de Prote√ß√£o de Dados**: Todas as disposi√ß√µes legais que regulem o Tratamento de Dados Pessoais, incluindo, por√©m sem se limitar, a Lei n¬∫ 13.709/18, Lei Geral de Prote√ß√£o de Dados Pessoais (‚ÄúLGPD‚Äù).

##### Uso de Dados Pessoais

Coletamos e usamos Dados Pessoais para:

- Gerenciar seu relacionamento conosco e melhor atend√™-lo quando voc√™ estiver adquirindo produtos e/ou servi√ßos na empresa, personalizando e melhorando sua experi√™ncia.
- Confirmar ou corrigir as informa√ß√µes que temos sobre voc√™.
- Enviar informa√ß√µes que acreditamos ser do seu interesse.
- Personalizar sua experi√™ncia de uso da empresa.
- Personalizar o envio de publicidades para voc√™, baseada em seu interesse em nossa empresa.
- Entrar em contato por um n√∫mero de telefone e/ou endere√ßo de e-mail fornecido.

Exemplos de uso dos dados incluem:

- **Detec√ß√£o de Anomalias no Consumo de G√°s**: Coleta de dados para identificar padr√µes an√¥malos no consumo de g√°s, que possam indicar problemas como vazamentos ou falhas nos equipamentos.
- **Identifica√ß√£o de Fraudes**: An√°lise dos dados de consumo para detectar padr√µes de comportamento que possam sugerir fraudes.
- **Aprimoramento de Gest√£o de Riscos**: Uso dos dados coletados para desenvolver modelos de predi√ß√£o que melhorem a gest√£o de riscos para os clientes e parceiros.

##### N√£o fornecimento de Dados Pessoais

&nbsp;&nbsp;&nbsp;Voc√™ n√£o √© obrigado a compartilhar os Dados Pessoais que solicitamos, no entanto:

- Se voc√™ optar por n√£o compartilh√°-los, em alguns casos, n√£o poderemos fornecer a voc√™ acesso completo √† empresa, alguns recursos especializados ou ser capaz de prestar a assist√™ncia necess√°ria.
- N√£o poderemos viabilizar a entrega do produto ou prestar o servi√ßo contratado por voc√™.

##### Dados coletados

- **Navega√ß√£o Geral na empresa**: O p√∫blico em geral poder√° navegar na empresa sem necessidade de qualquer cadastro e envio de Dados Pessoais.
- **Contato com a empresa**: N√≥s podemos coletar dados como nome, sobrenome, n√∫mero de telefone, cidade, estado e endere√ßo de e-mail.
- **Prefer√™ncias e Navega√ß√£o**: Informa√ß√µes sobre suas prefer√™ncias e interesses em rela√ß√£o aos produtos/servi√ßos, al√©m de dados sobre suas visitas e atividades na empresa.

##### Compartilhamento de Dados Pessoais com terceiros

&nbsp;&nbsp;&nbsp;N√≥s poderemos compartilhar seus Dados Pessoais:

- Com empresas parceiras que voc√™ selecionar ou optar em enviar os seus dados, d√∫vidas, perguntas, etc.
- Com provedores de servi√ßos ou parceiros para gerenciar ou suportar certos aspectos de nossas opera√ß√µes comerciais em nosso nome.
- Com terceiros, caso ocorra qualquer reorganiza√ß√£o, fus√£o, venda, joint venture, cess√£o, transmiss√£o ou transfer√™ncia de toda ou parte da nossa empresa.

##### Transfer√™ncias internacionais de Dados

&nbsp;&nbsp;&nbsp;Dados Pessoais e informa√ß√µes de outras naturezas coletadas por n√≥s podem ser transferidos ou acessados por entidades pertencentes ao grupo corporativo das empresas parceiras em todo o mundo de acordo com esta Pol√≠tica de Privacidade.

##### Forma de coleta autom√°tica de Dados Pessoais

&nbsp;&nbsp;&nbsp;Quando voc√™ visita a empresa, ela pode armazenar ou recuperar informa√ß√µes em seu navegador, seja na forma de cookies e de outras tecnologias semelhantes. 

Exemplos incluem:

- **Uso de cookies**: Permite a coleta de informa√ß√µes como tipo de navegador, tempo dispendido na empresa, as p√°ginas visitadas, etc.
- **Uso de pixel tags e outras tecnologias similares**: Utilizados para rastrear a√ß√µes de usu√°rios da empresa, medir o sucesso das nossas campanhas de marketing e coletar dados estat√≠sticos sobre o uso da empresa e taxas de resposta.

##### Per√≠odo de Reten√ß√£o dos Dados

&nbsp;&nbsp;&nbsp;Seus Dados Pessoais ser√£o armazenados pelo per√≠odo necess√°rio para cumprir com as finalidades descritas nesta Pol√≠tica de Privacidade, exceto se um per√≠odo de reten√ß√£o mais longo for exigido ou permitido por lei.

##### Finalidades de Uso dos Dados

Os Dados Pessoais coletados s√£o utilizados para:

- **Detec√ß√£o de Anomalias no Consumo de G√°s**
- **Identifica√ß√£o de Fraudes**
- **Aprimoramento de Gest√£o de Riscos**

##### Uso de Cookies

&nbsp;&nbsp;&nbsp;Este website usa cookies para melhorar a experi√™ncia do usu√°rio. Ao utilizar o nosso website, voc√™ estar√° concordando com nossa Pol√≠tica de Cookies. Para mais informa√ß√µes, consulte nossa Declara√ß√£o de Cookies.

##### Direitos do Usu√°rio

Voc√™ pode, a qualquer momento, requerer:

- Confirma√ß√£o de que seus Dados Pessoais est√£o sendo tratados.
- Acesso aos seus Dados Pessoais.
- Corre√ß√µes a dados incompletos, inexatos ou desatualizados.
- Anonimiza√ß√£o, bloqueio ou elimina√ß√£o de dados desnecess√°rios, excessivos ou tratados em desconformidade com o disposto em lei.
- Portabilidade de Dados Pessoais a outro prestador de servi√ßos, contanto que isso n√£o afete nossos segredos industriais e comerciais.
- Elimina√ß√£o de Dados Pessoais tratados com seu consentimento, na medida do permitido em lei.
- Informa√ß√µes sobre as entidades √†s quais seus Dados Pessoais tenham sido compartilhados.

### 4.2. Compreens√£o dos Dados

&nbsp;&nbsp;&nbsp;&nbsp;Esta se√ß√£o tem como objetivo compreender todo o processo de compreens√£o de dados do projeto. Este processo √© respons√°vel por ser o primeiro momento de real contato dos integrantes do grupo com os dados fornecidos pela empresa parceira. Dessa forma, √© um momento destinado a entender a estrutura e composi√ß√£o destes dados, realizando diversas consultas, aplicando m√©todos de estat√≠stica descritiva, identificando quais colunas s√£o num√©ricas e quais s√£o categ√≥ricas, realizando tamb√©m o pr√©-processamento, normaliza√ß√£o e limpeza dos dados fornecidos. A partir da explora√ß√£o e do pr√©-processamento dos dados, tamb√©m s√£o criadas hip√≥teses sobre a rela√ß√£o dos dados e o problema.

#### 4.2.1. Explora√ß√£o de dados

&nbsp;&nbsp;&nbsp;&nbsp;Segundo De Oliveira (2020), quando estamos realizando o processo de an√°lise de dados √© crucial que tenhamos conhecimento dos tipos de vari√°veis que estamos tratando, sendo, principalmente, a categoriza√ß√£o delas entre categ√≥ricas e vari√°veis algo fundamental no desenvolvimento. Desse modo, de acordo ainda com tal estudo, as vari√°veis num√©ricas s√£o aquelas que definem quantidade, podendo ser medidas ou contadas, al√©m de serem usadas para opera√ß√µes matem√°ticas, sendo divididas entre cont√≠nuas e discretas, e apresentadas, como por exemplo, no formato de dados que tragam idade, sal√°rio, altura, etc. Assim, as vari√°veis categ√≥ricas trazem logicamente o oposto, tendo em vista que s√£o elas que trazem dados qualitativos, sendo divididas entre nominais e ordinais, identificando grupos e classifica√ß√µes, como cores, g√™neros, tipos de produtos, entre outros. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Dessa maneira, considerando o nosso projeto atual, iremos fazer essa an√°lise tendo em vista os dois tipos de base de dados que temos atualmente: a base de dados de cadastro do cliente e a de consumo do cliente. Come√ßando pelo estudo das vari√°veis da base de dados de cadastro dos usu√°rios da *Compass*, como √© apresentado na tabela abaixo: <br>

<div align="center">

<sub>Tabela 2 - Tabela de Classifica√ß√£o das Vari√°veis da Base de Dados Cadastral </sub>

</div>

| **Vari√°vel**                       | **Tipo**     | **Descri√ß√£o**                                                                 | **Classifica√ß√£o**  |
|------------------------------------|--------------|-------------------------------------------------------------------------------|--------------------|
| `clientCode`                       | Categ√≥rica   | Identificador √∫nico do cliente, vinculado ao CPF ou CNPJ do cliente.          | Nominal            |
| `clientIndex`                      | Categ√≥rica   | Identificador √∫nico para cada instala√ß√£o (medidor) vinculada ao cliente.      | Nominal            |
| `cep`                              | Categ√≥rica   | CEP da instala√ß√£o.                                                            | Nominal            |
| `bairro`                           | Categ√≥rica   | Bairro da instala√ß√£o.                                                         | Nominal            |
| `cidade`                           | Categ√≥rica   | Cidade da instala√ß√£o.                                                         | Nominal            |
| `categoria`                        | Categ√≥rica   | Categoria do cliente na qual ele se enquadra.                                 | Nominal            |
| `contratacao`                      | Categ√≥rica   | Data da contrata√ß√£o do servi√ßo.                                               | Nominal            |
| `situacao`                         | Categ√≥rica   | Situa√ß√£o atual do cliente (ex.: consumindo g√°s ou desativado).                | Nominal            |
| `perfil_consumo`                   | Categ√≥rica   | Tipo/Perfil de consumo do cliente, indicando em quais situa√ß√µes utiliza o servi√ßo de g√°s. | Nominal            |
| `condCode`                         | Categ√≥rica   | Identificador √∫nico do condom√≠nio.                                            | Nominal            |
| `condIndex`                        | Categ√≥rica   | Identificador √∫nico para cada instala√ß√£o realizada na √°rea comum do condom√≠nio. | Nominal            |

<div align="center">

<sup>Fonte: Material produzido pelos autores (2024)</sup>

</div>

&nbsp;&nbsp;&nbsp;&nbsp;√â poss√≠vel identificar que todas as vari√°veis trabalhadas nessa base s√£o do tipo categ√≥ricas, al√©m de serem tamb√©m nominais, a julgar que essa base tem como objetivo trazer dados qualitativos do cliente, a fim de conhecer melhor o seu perfil, como consumidor. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Em seguida, temos a base de dados focada no consumo do cliente, trazendo tamb√©m colunas e informa√ß√µes trazidas na base de dados anterior: <br>

<div align="center">

<sub>Tabela 3 - Tabela de Classifica√ß√£o das Vari√°veis da Base de Dados de Consumo </sub>

</div>

| **Vari√°vel**                       | **Tipo**     | **Descri√ß√£o**                                                                 | **Classifica√ß√£o**  |
|------------------------------------|--------------|-------------------------------------------------------------------------------|--------------------|
| `clientCode`                       | Categ√≥rica   | Identificador √∫nico do cliente, vinculado ao CPF ou CNPJ do cliente.          | Nominal            |
| `clientIndex`                      | Categ√≥rica   | Identificador √∫nico para cada instala√ß√£o (medidor) vinculada ao cliente.      | Nominal            |
| `datetime`                         | Categ√≥rica   | Data de recep√ß√£o da informa√ß√£o pelo ecossistema LoRa.                         | Nominal            |
| `meterIndex`                       | Num√©rica     | Valor da leitura atual do medidor.                                            | Cont√≠nua           |
| `initialIndex`                     | Num√©rica     | Valor inicial do equipamento ao realizar a instala√ß√£o do sensor.              | Cont√≠nua           |
| `pulseCount`                       | Num√©rica     | Valor de pulsos lidos, referencial ao consumo.                                | Discreta           |
| `gain`                             | Num√©rica     | Fator de multiplica√ß√£o de pulsos para metros c√∫bicos.                         | Cont√≠nua           |
| `rssi`                             | Num√©rica     | Intensidade de sinal do sensor ao gateway da publica√ß√£o.                      | Cont√≠nua           |
| `gatewayGeoLocation.alt`           | Num√©rica     | Altitude ou altura da instala√ß√£o do gateway.                                  | Cont√≠nua           |
| `gatewayGeoLocation.lat`           | Num√©rica     | Latitude da instala√ß√£o do gateway.                                            | Cont√≠nua           |
| `gatewayGeoLocation.long`          | Num√©rica     | Longitude da instala√ß√£o do gateway.                                           | Cont√≠nua           |
| `model`                            | Categ√≥rica   | Modelo do equipamento.                                                        | Nominal            |
| `meterSN`                          | Categ√≥rica   | N√∫mero do medidor de cada instala√ß√£o.                                         | Nominal            |
| `inputType`                        | Categ√≥rica   | Tipo de leitura do medidor.                                                   | Nominal            |

<div align="center">

<sup>Fonte: Material produzido pelos autores (2024)</sup>

</div>

&nbsp;&nbsp;&nbsp;&nbsp;Feito o estudo da tabela, podemos observar uma varia√ß√£o entre vari√°veis num√©ricas e vari√°veis categ√≥ricas, tendo nessa base somente seis vari√°veis categ√≥ricas, todas nominais, trazendo agora n√£o somente informa√ß√µes sobre o cliente, mas sobre o desempenho dos equipamentos de medi√ß√µes. Todavia, quando vamos para as vari√°veis num√©ricas, temos oito vari√°veis num√©ricas pontuadas, sendo s√≥ uma delas considerada discreta, a julgar que √© a vari√°vel que diz respeito ao n√∫mero de pulsos do medidor (n√∫mero inteiro), sendo as demais vari√°veis cont√≠nuas, considerando que elas trazem valores dentro de um intervalo, como a vari√°vel de leitura do medidor. <br>

&nbsp;&nbsp;&nbsp;&nbsp;Logo, podemos concluir que a nossa base de dados traz uma grande varia√ß√£o de categorias de vari√°veis, sendo bem precisas nos tipos de informa√ß√µes que s√£o trazidas, dando a devida aten√ß√£o e trazendo equil√≠brio no que tange dados quantitativos e qualitativos. Portanto, feito esse estudo, a an√°lise dos dados pode ser muito melhor trabalhada, no que diz respeito √†s contas matem√°ticas para pesquisa de anomalias e estudos de perfil de clientes. <br>

&nbsp;&nbsp;&nbsp;Conforme o apresentado acima, para ilustrar o supramencionado, iremos realizar algumas an√°lises de gr√°ficos que se demonstram interessantes ao longo do levantamento de informa√ß√µes. Mais gr√°ficos e an√°lises podem ser encontradas no notebook do projeto.

##### 4.2.1.1. Distribui√ß√£o Geogr√°fica dos Gateways com Altitude

&nbsp;&nbsp;&nbsp;O gr√°fico intitulado "Distribui√ß√£o Geogr√°fica dos Gateways com Altitude" visualiza a rela√ß√£o entre latitude, longitude e altitude dos gateways instalados. As cores utilizadas no gr√°fico variam do amarelo ao roxo, onde o amarelo representa altitudes superiores a 900 metros, enquanto o roxo indica altitudes inferiores a 100 metros. Gateways s√£o dispositivos de rede que atuam como pontos de acesso entre diferentes redes de comunica√ß√£o, permitindo a transmiss√£o e recep√ß√£o de dados entre elas. No contexto de sistemas de monitoramento, como os utilizados para a detec√ß√£o de anomalias e vazamentos de g√°s, os gateways coletam dados de v√°rios sensores distribu√≠dos geograficamente, incluindo informa√ß√µes de altitude, e os retransmitem para um sistema central, onde esses dados s√£o analisados. 

&nbsp;&nbsp;&nbsp;A an√°lise do gr√°fico revela duas √°reas de maior concentra√ß√£o de dados. A primeira √°rea est√° localizada entre as coordenadas de longitude -51 a -52 e latitude -29 a -31, onde predominam altitudes mais baixas, embora alguns pontos indiquem altitudes mais elevadas. A segunda √°rea em destaque est√° situada entre as longitudes -48 a -46 e latitudes -24 a -22, onde tamb√©m se observa uma concentra√ß√£o relevante de gateways. 
Compreender este gr√°fico no desenvolvimento de um modelo preditivo faz com que possamos visualizar o impacto da altitude na performance dos gateways. Dessa forma, podemos ajustar o modelo preditivo, garantindo que ele leve em conta varia√ß√µes de altitude que possam afetar a detec√ß√£o de anomalias, como oscila√ß√µes na transmiss√£o de dados ou interfer√™ncias na comunica√ß√£o, especialmente em regi√µes de grande varia√ß√£o altim√©trica.

<div align="center">
   
   <sub>Figura 11 - Gr√°fico de Distribui√ß√£o Geogr√°fica dos Gateways com Altitude </sub>

   <img src="..\assets\grafico_gateways.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

##### 4.2.1.2. Distribui√ß√£o de Tipo de Leitura por Modelo

&nbsp;&nbsp;&nbsp;Al√©m da an√°lise da altitude dos gateways supramencionada, a distribui√ß√£o dos tipos de leitura por modelo tamb√©m desempenha um papel essencial na an√°lise preditiva. O gr√°fico de distribui√ß√£o dos tipos de leitura por modelo √© representado por um gr√°fico de barras que destaca as leituras realizadas pelos medidores IG1-K-L-v2 e Infinity V2.

&nbsp;&nbsp;&nbsp;No medidor IG1-K-L-v2, as leituras s√£o categorizadas como DI1, DI2, DI3, DI4, DI5, DI6, DI7 e DI8, enquanto o medidor Infinity V2 realiza leituras remotas. A unidade de medida utilizada nesta planilha √© 1e6, o que significa um milh√£o, 1.000.000. Essa nota√ß√£o pode ser encontrada em an√°lises onde as contagens ou valores s√£o muito grandes, facilitando a interpreta√ß√£o e compara√ß√£o dos dados. 

&nbsp;&nbsp;&nbsp;Os resultados mostram que a leitura Remota do modelo Infinity V2 se aproxima de 1.0 (ou seja, 1 milh√£o de leituras), seguida pela leitura DI1, que registra 0,6 (600 mil leituras), e pela leitura DI2, com 0,4 (400 mil leituras). 

&nbsp;&nbsp;&nbsp;Essa an√°lise para um modelo preditivo de anomalias e vazamento de g√°s permite identificar padr√µes de leitura mais frequentes, ajudando na calibra√ß√£o e no ajuste do modelo para detec√ß√£o mais precisa de anomalias. Quanto maior a precis√£o na an√°lise da distribui√ß√£o das leituras, mais eficiente ser√° o modelo preditivo em identificar potenciais vazamentos de g√°s e outras irregularidades.

<div align="center">
   
   <sub>Figura 12 - Gr√°fico de Distribui√ß√£o de Tipo de Leitura por Modelo </sub>

   <img src="..\assets\grafico_distribuicao_de_leitura.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

##### 4.2.1.3. Situa√ß√£o dos Clientes

&nbsp;&nbsp;&nbsp;Em continuidade √† an√°lise das leituras e considerando a distribui√ß√£o dos gateways, a situa√ß√£o dos clientes tamb√©m fornece dados para aprimorar a efic√°cia dos modelos preditivos. 

&nbsp;&nbsp;&nbsp;O gr√°fico de situa√ß√£o dos clientes √© fundamental para entender a caracteriza√ß√£o dos contratos e o comportamento dos clientes em rela√ß√£o ao consumo de g√°s. A an√°lise revela que 96,9% dos clientes est√£o ativamente consumindo g√°s, um dado que demonstra a ampla utiliza√ß√£o do servi√ßo. Al√©m disso, 1,3% dos clientes est√£o sob contrato, mas n√£o apresentam consumo de g√°s, o que totaliza 98,2% de clientes ativos em rela√ß√£o ao total.

&nbsp;&nbsp;&nbsp;Por fim, o n√∫mero de clientes desativados √© relativamente baixo, representando apenas 1,8% do total levantado. 

&nbsp;&nbsp;&nbsp;A import√¢ncia dessa an√°lise para o desenvolvimento de um modelo preditivo de anomalias e vazamento de g√°s √© o aux√≠lio a compreens√£o da distribui√ß√£o e a situa√ß√£o dos clientes, o modelo pode ser ajustado para identificar padr√µes de consumo an√¥malos. Para trazer um exemplo na pr√°tica, podemos pontuar a identifica√ß√£o de clientes que possuem contrato, mas n√£o apresentam consumo, pode indicar poss√≠veis falhas no sistema, desvios ou at√© mesmo vazamentos n√£o detectados. 

&nbsp;&nbsp;&nbsp;Al√©m disso, a baixa taxa de clientes desativados sugere uma estabilidade na base de clientes, o que √© relevante para a previs√£o e monitoramento cont√≠nuo de poss√≠veis anomalias no consumo de g√°s.

<div align="center">
   
   <sub>Figura 13 - Gr√°fico de Situa√ß√£o dos Clientes </sub>

   <img src="..\assets\grafico_situacao_dos_clientes.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;A an√°lise integrada dos dados geogr√°ficos dos gateways, dos tipos de leitura por modelo e da situa√ß√£o dos clientes √© essencial para o desenvolvimento de um modelo preditivo de alta precis√£o para detec√ß√£o de anomalias e vazamentos de g√°s. A correla√ß√£o entre altitude e performance dos gateways, os padr√µes de leitura e o comportamento dos clientes oferece uma vis√£o mais ampla que pode nos permitir adaptar o modelo para diferentes cen√°rios, garantindo uma resposta eficiente e proativa em situa√ß√µes de risco, como vazamentos ou falhas nos sistemas de monitoramento. 


#### 4.2.2. Pr√©-processamento dos dados

&nbsp;&nbsp;&nbsp;&nbsp;Entende-se por "pr√©-processamento de dados" o processo respons√°vel por converter dados brutos em dados que podem, de fato, ser utilizados em projetos de an√°lise de dados, sendo essa uma etapa indispens√°vel para a realiza√ß√£o da an√°lise e modelagem dos dados (Pinheiro, 2021). Dessa forma, o pr√©-processamento se faz important√≠ssimo, uma vez que utilizar dados n√£o-preparados faz com que os resultados do modelo preditivo fiquem muito aqu√©m do esperado. Portanto, realizar uma boa higieniza√ß√£o dos dados garante um processo de modelagem mais confi√°vel e robusto. <br> 

##### 4.2.2.1 Pr√©-processamento das tabelas de consumo

&nbsp;&nbsp;&nbsp;&nbsp; Para o presente projeto, iniciamos o pr√©-processamento dos dados juntando todos os arquivos de dados de meses diferentes em um s√≥ *dataframe*:

```python
df_combined = pd.concat([df_mes1, df_mes2, df_mes3, df_mes4, df_mes5], ignore_index=True)
```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s isso, identificamos quais colunas seriam descartadas na an√°lise. Ao realizar uma busca, percebemos que 4 colunas (gatewayGeoLocation.alt, gatewayGeoLocation.lat, gatewayGeoLocation.long e rssi) possu√≠am, em sua grande maioria, valores atribu√≠dos como None, ou seja, n√£o possu√≠am valor. Assim, removemos essas 4 colunas da tabela:

```python
df_combined = df_combined.drop(columns=['gatewayGeoLocation.alt', 'gatewayGeoLocation.lat', 'gatewayGeoLocation.long', 'rssi'])

```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s remover as colunas consideradas n√£o vantajosas para a an√°lise, realizamos uma encodifica√ß√£o do tipo *label* para a coluna *inputType*. A encodifica√ß√£o do tipo *label* converte os valores categ√≥ricos para um valor num√©rico. No nosso caso, convertemos o *inputType* DI1 para 1, DI2 para 2, e assim por diante. Ainda no t√≥pico de encodifica√ß√£o, tamb√©m realizamos uma *One-Hot Encoding* na coluna de *model*, criando duas novas colunas que representam, por meio de um valor booleano, se tal registro foi feito a partir de tal medidor. 

```python

## Label Encoding
encoder = LabelEncoder()

df_combined['inputType_encoded'] = df_combined['inputType'].map({'DI1': 1, 'DI2': 2, 'DI3': 3, 'DI4': 4, 'DI5': 5, 'DI6': 6, 'DI7': 7, 'DI8': 8, 'leituraRemota': 9})

unique_inputType_encoded = df_combined['inputType_encoded'].unique()
for input_type in unique_inputType_encoded:
    first_result = df_combined[df_combined['inputType_encoded'] == input_type].iloc[0]
    print(f"InputType: {first_result['inputType']}, Encoded: {input_type}")

## One-Hot Encoding
df_combined = pd.get_dummies(df_combined, columns=['model'])


```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s realizar essa encodifica√ß√£o, tamb√©m j√° removemos a coluna *inputType*, restando apenas a *inputType_encode*. Al√©m disso, tamb√©m foi removida a coluna *meterSN*, que guarda o valor serial do medidor da instala√ß√£o. Identificamos que esta coluna √© apenas identificadora e n√£o impacta na an√°lise, ent√£o a removemos. 

```python
df_combined = df_combined.drop(columns=['inputType', 'meterSN'])
```

&nbsp;&nbsp;&nbsp;&nbsp;O pr√≥ximo passo ap√≥s remover essas colunas foi procurar e tratar por valores que estavam faltando nas colunas restantes. Dessa forma, identificamos que, dentre todas as colunas, apenas a coluna *gain* possu√≠a valores faltando, e, mais especificamente, esse valor aparecia como nulo apenas nas linhas em que o modelo do medidor era o *Infinity V2*. Dessa forma, decidimos que, em toda linha que o modelo do medidor fosse desse tipo, o *gain* seria igual a 0:

```python
df_combined.loc[df_combined['model_Infinity V2'] == True, 'gain'] = 0

```

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, outras a√ß√µes que realizamos foram:
* Encodifica√ß√£o label para √≠ndice do cliente, trocando o identificador dos clientes de um *hash* para um valor inteiro mais f√°cil de ser identificado.
* Utiliza√ß√£o de um Standard Scaler para colocar todos os valores num√©ricos na mesma escala.
* Convers√£o da coluna *datetime* para Unix Timestamp
* Verifica√ß√£o por valores duplicados, que retornou que nenhuma linha na tabela estava duplicada.
* Em rela√ß√£o aos valores considerados *outliers*, ou seja, fora da m√©dia, decidimos por mant√™-los, uma vez que tais valores podem, por si s√≥, representarem uma anomalia no consumo de g√°s, e, portanto, podem ser √∫teis para a modelagem dos dados.

##### 4.2.2.2 Pr√©-processamento da tabela de informa√ß√µes cadastrais

&nbsp;&nbsp;&nbsp;&nbsp;Depois de terminar o pr√©-processamento inicial das tabelas de consumo, decidimos come√ßar o pr√©-processamento da tabela de informa√ß√µes cadastrais. Essa tabela cont√©m informa√ß√µes como c√≥digo do cliente, c√≥digo do condom√≠nio, cidade, bairro, CEP, e padr√£o de consumo.

&nbsp;&nbsp;&nbsp;&nbsp;Primeiramente, importamos a tabela para o *notebook* j√° utilizado para o pr√©-processamento das tabelas anteriores e retiramos as colunas que n√£o impactariam a an√°lise, sendo elas as colunas 'condCode', 'contratacao' e 'cep'.

```python
df_cadastro.drop(columns=['condCode', 'contratacao', 'cep'], inplace=True)
```
&nbsp;&nbsp;&nbsp;&nbsp;Depois, efetuamos uma encodifica√ß√£o do tipo *One-Hot-Encoding* na coluna 'situacao', separando-a em tr√™s colunas com valores booleanos para o tipo de situa√ß√£o atrelado a cada clientCode.

```python
df_cadastro = pd.get_dummies(df_cadastro, columns=['situacao'])
```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s o processo de *encode*, decidimos analisar a quantidade e os tipos de perfil de consumo dos clientes cadastrados.

```python
perfil_consumo_counts = df_cadastro['perfil_consumo'].value_counts()
```
&nbsp;&nbsp;&nbsp;&nbsp;Assim, descobrimos que o perfil de consumo que mais se repetia era o perfil "Coc√ß√£o + Aquecedor", tendo 1479 clientes atrelados a essa identifica√ß√£o. Al√©m disso, havia 5 clientes com o valor "-" ocupando a coluna de perfil de consumo. Dessa forma, decidimos substituir os valores "-" pela moda da coluna. Ap√≥s a substitui√ß√£o, tamb√©m foi aplicada a encodifica√ß√£o *One-Hot-Encoding*.

```python
df_cadastro['perfil_consumo'] = df_cadastro['perfil_consumo'].replace('-', 'Coc√ß√£o + Aquecedor')
df_cadastro = pd.get_dummies(df_cadastro, columns=['perfil_consumo'])
```
&nbsp;&nbsp;&nbsp;&nbsp;Depois, identificamos quais eram as categorias de cliente que mais apareciam na tabela de informa√ß√µes cadastrais para entender como a categoria de cliente poderia impactar o tipo de consumo. Nesse sentido, foi identificado que a maioria dos clientes moram em moradias individuais que se localizam em pr√©dios.

```python
categoria_counts = df_cadastro['categoria'].value_counts()
categoria_mais_frequente = categoria_counts.idxmax()
categoria_mais_frequente
categoria_counts
```
&nbsp;&nbsp;&nbsp;&nbsp;Para preparar as tabelas para o *merge* entre as duas, come√ßamos a verificar a rela√ß√£o entre os valores de *clientIndex* e *clientCode* de ambas as tabelas, procurando entender se havia linhas com o mesmo c√≥digo de cliente, mas √≠ndices diferentes. Ao final, encontramos quais eram as instala√ß√µes √∫nicas, ou seja, aquelas que possuiam um √∫nico *clientCode* atrelado a um √∫nico *clientIndex*.

```python
len(df_combined['clientCode'].unique())
df_combined.groupby('clientCode')['clientIndex'].nunique().gt(1).any()
unique_combinations = df_combined.groupby(['clientCode', 'clientIndex']).size()
quantidade_unicas = unique_combinations.count()
print(f"Quantidade de linhas com combina√ß√µes √∫nicas de clientCode e clientIndex: {quantidade_unicas}")
```

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, foi realizado o *left join* das duas tabelas, processo esse que trouxe as informa√ß√µes da tabela de informa√ß√µes cadastrais para a tabela de informa√ß√µes de consumo, mantendo a correspond√™ncia entre os atributos *clientCode* e *clientIndex*. Entretanto, vimos que havia valores que n√£o possu√≠am nenhuma correspond√™ncia, ou seja, existem clientes com dados de consumo, mas que n√£o foram cadastrados na tabela de informa√ß√µes cadastrais.

```python
df_merged = pd.merge(df_combined, df_cadastro, on=['clientCode', 'clientIndex'], how='left')
df_merged
```

&nbsp;&nbsp;&nbsp;&nbsp;Come√ßamos, ent√£o, o processo de tratamento dos dados sem correspond√™ncia ap√≥s o *merge* das tabelas. Para o tratamento, decidimos preencher as colunas com valores vazios com a moda de cada uma das features.

```python
df_merged['situacao_CONSUMINDO G√ÅS'] = df_merged['situacao_CONSUMINDO G√ÅS'].fillna(True)
df_merged['situacao_CONTRATADO'] = df_merged['situacao_CONTRATADO'].fillna(False)
df_merged['situacao_DESATIVADO'] = df_merged['situacao_DESATIVADO'].fillna(False)
```

```python
df_merged['perfil_consumo_Coc√ß√£o + Aquecedor'] = df_merged['perfil_consumo_Coc√ß√£o + Aquecedor'] = df_merged['perfil_consumo_Coc√ß√£o + Aquecedor'].fillna(True)
df_merged['perfil_consumo_Aquecedor'] = df_merged['perfil_consumo_Aquecedor'].fillna(False)
df_merged['perfil_consumo_Coc√ß√£o'] = df_merged['perfil_consumo_Coc√ß√£o'].fillna(False)
df_merged['perfil_consumo_Caldeira'] = df_merged['perfil_consumo_Caldeira'].fillna(False)
df_merged['perfil_consumo_Coc√ß√£o + Aquecedor + Piscina'] = df_merged['perfil_consumo_Coc√ß√£o + Aquecedor + Piscina'].fillna(False)
df_merged['perfil_consumo_Coc√ß√£o + Caldeira'] = df_merged['perfil_consumo_Coc√ß√£o + Caldeira'].fillna(False)
```

```python
df_merged['categoria'] = df_merged['categoria'].fillna(categoria_mais_frequente)
df_merged['bairro'] = df_merged['bairro'].fillna(df_merged['bairro'].mode().iloc[0])
df_merged['cidade'] = df_merged['cidade'].fillna(df_merged['cidade'].mode().iloc[0])
df_merged['condIndex'] = df_merged['condIndex'].fillna(df_merged['condIndex'].mode().iloc[0])
```
&nbsp;&nbsp;&nbsp;&nbsp;Tamb√©m foram identificadas as colunas de cidade que traziam o mesmo valor, mas escrito de forma diferente (com e sem acento). Assim, tamb√©m efetuamos a concatena√ß√£o e juntamos todos os valores em uma √∫nica coluna para cada cidade.

```python
import unidecode

# Remover acentos, converter para min√∫sculas, e garantir consist√™ncia
df_merged['cidade'] = df_merged['cidade'].str.lower()
df_merged['cidade'] = df_merged['cidade'].apply(lambda x: unidecode.unidecode(x))
df_merged['cidade'] = df_merged['cidade'].str.strip()
```

Por fim, foram efetuadas as encodifica√ß√µes *One-Hot-Encode* nas colunas de cidade e categoria e *Label encode* na coluna de bairro.

```python
df_merged = pd.get_dummies(df_merged, columns=['cidade'])
df_merged = pd.get_dummies(df_merged, columns=['categoria'])
df_merged['bairro_encoded'] = encoder.fit_transform(df_merged['bairro'])
```

##### 4.2.2.3 Mesclagem da tabela com dados de temperatura di√°ria

&nbsp;&nbsp;&nbsp;&nbsp;Para aux√≠lio nas an√°lises futuras, decidimos mesclar com a tabela de consumo as temperaturas di√°rias j√° relacionadas com suas datas. Nesse sentido, foi utilizada a API *Weatherbit* por meio da gera√ß√£o de chaves de API por dois membros do grupo e a cada chamada, a API retornava as temperaturas di√°rias de 15 em 15min. Ao fim do tratamento, decidimos calcular somente a temperatura m√©dia di√°ria.

```python
from fastapi import FastAPI
import requests
import uvicorn
import pandas as pd

import os
from dotenv import load_dotenv
load_dotenv()

app = FastAPI()

```
&nbsp;&nbsp;&nbsp;&nbsp;Primeiramente, foram importadas as bibliotecas necess√°rias para utilizar a API. Al√©m disso, a API escolhida funciona por meio de coordenadas geogr√°ficas, ent√£o foram criadas vari√°veis para cada cidade contendo as coordenadas de latitude e longitude.

```python
canoas = [-29.91288005916852, -51.183972626036535]
porto_alegre = [-30.123022146019036, -51.19501166428309]
novo_hamburgo = [-29.69698953260176, -51.13017124234447]
gravatai = [-29.922741757628337, -51.020177029221045]
sao_leopoldo = [-29.768932683065, -51.14485480703105]
seabra = [-12.417641332786426, -41.77453344526071]
```
&nbsp;&nbsp;&nbsp;&nbsp;Depois de armazenar as coordenadas, foi criada a fun√ß√£o de chamada da API, que utiliza as API keys geradas por dois membros da equipe. Al√©m disso, a chamada retornou os dados de temperatura registrados de 15 em 15 minutos, ent√£o a fun√ß√£o incluiu a transforma√ß√£o de valor para a m√©dia di√°ria. 

```python
@app.get("/canoas")
def call_api_canoas():
    months = ["02", "03", "04", "05", "06"]
    year = "2024"
    
    all_records = []
    
    for month in months:
        start_date = f"{year}-{month}-01"
        end_date = f"{year}-{month}-31" if month != "02" else "2024-02-29"  # Fevereiro pode ter 29 dias
        
        url = f"https://api.weatherbit.io/v2.0/history/subhourly?lat={canoas[0]}&lon={canoas[1]}&start_date={start_date}&end_date={end_date}&key={os.getenv('API_KEY')}"
        print(f"Fetching data for: {start_date} to {end_date}")
        response = requests.get(url)
        data = response.json()
        
        if response.status_code != 200:
            print(f"Failed to fetch data for {month}/{year}. Status Code: {response.status_code}")
            continue

        for entry in data['data']:
            temp = entry['temp']
            date = entry['timestamp_local'].replace("T", " ")[:10]
            all_records.append({"date": date, "temp": round(temp, 2)})
    
    df = pd.DataFrame(all_records)
    
    daily_avg_temps = df.groupby("date")["temp"].mean().round(2).reset_index()
    
    daily_avg_temps.to_csv("daily_avg_temps_canoas.csv", index=False)
    
    city_name = data.get('city_name', 'Unknown City')
    for _, row in daily_avg_temps.iterrows():
        date = row['date']
        avg_temp = row['temp']
        print(f"City: {city_name}, Average Temperature: {avg_temp:.1f}¬∞C, Date: {date}")

    return daily_avg_temps.to_dict(orient="records")
```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s repetir o processo com todas as cidades que possuem registros no conjunto de dados de consumo, os valores de temperatura di√°ria foram salvos em arquivos CSV e importados para o *notebook* onde o pr√©-processamento foi realizado.

```python
csv_files = {
    'canoas': '../daily_avg_temps_canoas.csv',
    'gravatai': '../daily_avg_temps_gravatai.csv',
    'novo_hamburgo': '../daily_avg_temps_novo_hamburgo.csv',
    'porto_alegre': '../daily_avg_temps_porto_alegre.csv',
    'sao_leopoldo': '../daily_avg_temps_sao_leopoldo.csv'
}

## cria uma lista com os dataframes de temperatura e depois os concatena
temp_data = []
for cidade, file_name in csv_files.items():
    temp_df = pd.read_csv(file_name)
    temp_df['cidade'] = cidade  
    temp_data.append(temp_df)

df_temperatura = pd.concat(temp_data, ignore_index=True)
```
&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, foi utilizado o m√©todo de chunks, ou seja, o carregamento de grupos de c√©lulas em vez de carregar todas as c√©lulas de uma √∫nica vez.

```python
chunk_size = 100000

results = []

def get_cidade(row):
    for cidade in csv_files.keys():
        if row[f'cidade_{cidade}']:
            return cidade
    return None

for start in range(0, len(df_merged), chunk_size):
    end = min(start + chunk_size, len(df_merged))
    chunk = df_merged.iloc[start:end].copy()

    chunk['cidade'] = chunk.apply(get_cidade, axis=1)

    chunk['data'] = chunk['datetime'].dt.date

    results.append(chunk)
```

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, os valores de temperatura di√°ria de cada cidade foram pareados com as linhas em que a cidade fosse relacionada com a temperatura, permitindo a an√°lise dos dados de consumo tendo tamb√©m a informa√ß√£o clim√°tica do dia em que o dado foi registrado.

##### 4.2.2.4 Normaliza√ß√£o da tabela e tratamento de valores *outliers*

&nbsp;&nbsp;&nbsp;&nbsp;Ao analisar a coluna, foram identificadas colunas com valores num√©ricos em escalas muito diferentes, e tal diferen√ßa entre os valores m√°ximos e m√≠nimos de cada uma das colunas pode ser prejudicial ao treinamento do modelo.

&nbsp;&nbsp;&nbsp;&nbsp;Tentando impedir os preju√≠zos dos dados num√©ricos em escalas diferentes, foi utilizado o *Standard Scaler*, uma t√©cnica de pr√©-processamento utilizada largamente no tratamento de escalas dos valores.

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
numeric_columns = ['meterIndex', 'initialIndex', 'pulseCount', 'model_IG1K-L-v2', 'model_Infinity V2', 'inputType_encoded', 'gain', 'timestamp']
df_combined[numeric_columns] = scaler.fit_transform(df_combined[numeric_columns])
df_combined
```
&nbsp;&nbsp;&nbsp;&nbsp;Depois, para as tentativas de identifica√ß√£o de *outliers*, utilizamos o *Isolation Forest*, um algoritmo que encontra anomalias em *datasets*. Ao implement√°-lo, quase 30 mil colunas foram classificadas como anomalias entre os dados da tabela.

```python
from sklearn.ensemble import IsolationForest

model = IsolationForest(contamination=0.01, random_state=42)
df_combined['anomaly'] = model.fit_predict(df_combined[numeric_columns])
```

&nbsp;&nbsp;&nbsp;&nbsp;Para fins de teste, tamb√©m foi utilizado o m√©todo *z-score*, uma ferramenta estat√≠stica que calcula quantos desvios-padr√£o um valor est√° de dist√¢ncia da m√©dia do conjunto de dados e tamb√©m √© utilizada para encontrar valores *outliers*. Ao utiliz√°-lo, quase 5 mil linhas foram classificadas como anomalias. Entretanto, os valores an√¥malos n√£o foram retirados do conjunto de dados, pois eles podem eventualmente ser classificados como as anomalias que o modelo preditivo deve encontrar. Dessa forma, n√£o foi realizado o *drop* desses valores.

&nbsp;&nbsp;&nbsp;&nbsp;O c√°lculo do z-score foi implementado na coluna *meterIndex* da seguinte forma: ao calcular a m√©dia de consumo a partir do valor mostrado pelo medidor, foi tamb√©m calculado o desvio-padr√£o acima e abaixo do valor da m√©dia. Al√©m disso, foi definido que os valores dentro do intervalo de at√© 2 desvios-padr√£o acima ou abaixo da m√©dia n√£o seriam considerados *outliers*.

```python
from scipy import stats

df_merged['z_score_meterIndex'] = np.abs(stats.zscore(df_merged['meterIndex']))

threshold = 3
df_merged['outlier'] = df_merged['z_score_meterIndex'] > threshold

outliers = df_merged[df_merged['outlier']]

print(f"Outliers identificados: {len(outliers)}")
```
&nbsp;&nbsp;&nbsp;&nbsp;Depois de encontrar os valores outliers que foram identificados pelo z-score, os mesmos foram armazenados em uma vari√°vel. A utiliza√ß√£o do z-score √© uma ferramenta com resultados promissores para as an√°lises futuras e busca de valores que sejam an√¥malos no conjunto de dados, pois utiliza a quantidade de pulsos e o valor do medidor para identifica√ß√£o dos *outliers*, exatamente as colunas que mais se relacionam com o consumo por parte dos clientes.

```python
unique_installations = outliers.groupby(['clientCode', 'clientIndex']).size()
num_unique_installations = unique_installations.count()
print(f"N√∫mero de instala√ß√µes √∫nicas outliers: {num_unique_installations}")
```

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, foi realizado o pr√©-processamento de dados utilizando todo o volume de dados disponibilizado pela empresa parceira. Dessa maneira, √© esperado que o tratamento pr√©vio do conjunto de dados aumente a consist√™ncia e qualidade dos dados para o in√≠cio do treinamento do modelo e para a forma√ß√£o das hip√≥teses da se√ß√£o 4.2.3.

#### 4.2.3. Hip√≥teses
&nbsp;&nbsp;&nbsp;&nbsp;Segundo o Dicion√°rio Escolar da L√≠ngua Portuguesa (2015), a palavra "hip√≥tese" √© definida como sendo "uma explica√ß√£o poss√≠vel, mas que ainda n√£o se provou [...]". Nesse contexto, ap√≥s termos realizado a primeira etapa de explora√ß√£o de dados e, logo em sequ√™ncia, o pr√©-processamento e higieniza√ß√£o de tais dados, surgiram algumas hip√≥teses acerca dos dados que possu√≠mos.

**Varia√ß√£o de consumo de acordo com m√™s do ano:** <br>
&nbsp;&nbsp;&nbsp;&nbsp;A primeira hip√≥tese que surgiu ao nos depararmos com dados de consumo de g√°s √© de que os clientes iriam consumir mais durante meses mais frios, uma vez que a queda de temperatura causa uma demanda maior pelo g√°s que √© utilizado para aquecer √°gua para atividades como o banho. No notebook do projeto, executamos um algoritmo que calcula as m√©dias de consumo de cada m√™s e plota um gr√°fico, obtendo o seguinte resultado:

<div align="center">
   
   <sub>Figura 14 - Gr√°fico de varia√ß√£o de consumo de acordo com o m√™s </sub>

   <img src="../assets/grafico_hipotese_1.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Ao analisar o gr√°fico, podemos perceber claramente que a hip√≥tese n√£o se mostra verdadeira. Na verdade, houve um pico de consumo na metade final do m√™s de Fevereiro e in√≠cio de Mar√ßo, por√©m, logo ap√≥s isso, temos uma queda brusca. Este fen√¥meno provavelmente ocorreu por conta das grandes enchentes que ocorreram no estado do Rio Grande do Sul nessa √©poca do ano, o que provavelmente levou diversas fam√≠lias que ficaram desabrigadas a deixar de consumir ou consumir g√°s em quantidades significativamente menores.

**Diferen√ßa entre n√∫meros de instala√ß√µes residenciais familiares e outras:** <br>
&nbsp;&nbsp;&nbsp;&nbsp;A segunda hip√≥tese que trazemos √© a de que a base de dados possuir√° um n√∫mero muito pequeno de instala√ß√µes de categoria residencial unifamiliar. Essa hip√≥tese se deu em uma conversa com a empresa parceira, que nos disse que o simples ato de fazer a instala√ß√£o do encanamento do g√°s para uma resid√™ncia comum √© muito cara e n√£o vale a pena, e, por conta disso, a maior parte das pessoas opta por utilizar outras op√ß√µes de g√°s, como o botij√£o de g√°s. No notebook do projeto, executamos um algoritmo que realiza a contagem de valores para as categorias e tamb√©m calcula a porcentagem de instala√ß√µes que pertencem √† categoria residencial familiar, obtendo um resultado onde percebemos que a hip√≥tese se mostra verdadeira, uma vez que, dos cerca de 3.000.000 de registros, apenas 1949 s√£o de resid√™ncias unifamiliares, sendo este um percentual de 0,07%.

**Diferen√ßa de consumo para perfis de consumo que possuam caldeiras:** <br>
&nbsp;&nbsp;&nbsp;&nbsp;Para a √∫ltima hip√≥tese, temos a ideia de que instala√ß√µes que possuam um perfil de consumo igual a "Caldeira" ou "Coc√ß√£o + Caldeira" ter√£o uma m√©dia de consumo maior do que os demais perfis, uma vez que estas instala√ß√µes provavelmente est√£o vinculadas a ind√∫strias ou a grandes resid√™ncias prediais. No notebook do projeto, executamos um algoritmo que faz o c√°lculo da m√©dia de consumo para as categorias mencionadas e ent√£o comparamos os resultados, obtendo que a hip√≥tese se mostra verdadeira, uma vez que a m√©dia de consumo das instala√ß√µes que possuem um perfil de consumo com caldeira √© de 0.202 e o de outros perfis √© de -0.00037. Isso pode nos confirmar que a quantidade de g√°s que as empresas consomem √© consideravelmente maior do que resid√™ncias. Vale ainda ressaltar que esses valores n√£o representam a medida real de metros c√∫bicos de g√°s que foram consumidos por conta da normaliza√ß√£o de valores num√©ricos que foi feita com o *Standard Scaler*, que coloca todos os valores na mesma escala. 

&nbsp;&nbsp;&nbsp;&nbsp;De acordo com tudo que foi explorado, percebemos que estas hip√≥teses s√£o de grande utilidade para o projeto que est√° sendo desenvolvido. O principal exemplo disto est√° na explora√ß√£o da primeira hip√≥tese, onde pode-se perceber que existe uma queda brusca no consumo geral de g√°s por conta das enchentes que afetaram o Rio Grande do Sul. Dessa forma, essas hip√≥teses podem nos ajudar a entender melhor o contexto nos quais os dados que possu√≠mos foram coletados e como devemos ajustar melhor o nosso modelo para isso (por exemplo, passamos a entender que a temperatura n√£o se mostrar√° como um fator t√£o influente no consumo de g√°s).

### 4.3. Prepara√ß√£o dos Dados e Modelagem
&nbsp;&nbsp;&nbsp;&nbsp;Nesta se√ß√£o, ser√° apresentado o processo de prepara√ß√£o dos dados para a constru√ß√£o do nosso modelo n√£o-supervisionado. O primeiro passo consiste na escolha das features e suas justificativas, seguido pelo desenvolvimento do primeiro modelo candidato para o problema. E a partir disso, realizar uma discuss√£o sobre os resultados do modelo. Em seguida, ser√° apresentada a justificativa para a defini√ß√£o do valor de K no modelo, finalizando com a elabora√ß√£o de um sistema de recomenda√ß√£o.

#### 4.3.1. Justificativa das features
&nbsp;&nbsp;&nbsp;&nbsp;A partir de uma an√°lise das colunas dispon√≠veis, foram selecionadas as features para o desenvolvimento da modelagem, levando em considera√ß√£o o impacto que esses dados podem ter na detec√ß√£o de anomalias no consumo de g√°s natural. Sendo assim, a seguir est√£o as justificativas das escolhas de cada uma dessas features.

&nbsp;&nbsp;&nbsp;&nbsp;A feature ‚Äútemp_scaled‚Äù foi selecionada, pois a temperatura ambiente influencia significativamente o consumo de g√°s. Em per√≠odos mais frios, por exemplo, √© comum que o consumo aumente devido ao uso intensificado de aquecedores e caldeiras. Assim, √© importante incluir essa feature para analisar poss√≠veis anomalias, sempre considerando a temperatura m√©dia do dia em uma determinada regi√£o. Dessa maneira, essa an√°lise pode ajudar a explicar mudan√ßas de consumo que sejam resultado de varia√ß√µes clim√°ticas e n√£o de comportamentos an√¥malos.

&nbsp;&nbsp;&nbsp;&nbsp;As features ‚Äúperfil_consumo_Aquecedor‚Äù, ‚Äúperfil_consumo_Caldeira‚Äù, ‚Äúperfil_consumo_Coc√ß√£o‚Äù, ‚Äúperfil_consumo_Coc√ß√£o + Aquecedor‚Äù, ‚Äúperfil_consumo_Coc√ß√£o + Aquecedor + Piscina‚Äù e ‚Äúperfil_consumo_Coc√ß√£o + Caldeira‚Äù foram inclu√≠das para representar o perfil de consumo do cliente, isto √©, em que tipo de equipamentos o cliente utiliza o g√°s natural. Esses perfis variam entre aquecedores, coc√ß√£o, caldeiras e piscinas. Em muitos casos, os clientes podem utilizar o g√°s em mais de uma dessas aplica√ß√µes, como ilustrado pela feature ‚Äúperfil_consumo_Coc√ß√£o + Aquecedor‚Äù, o que significa que o consumo pode ser maior que aquele que utiliza apenas um equipamento. 

&nbsp;&nbsp;&nbsp;&nbsp;A feature ‚Äúconsumo em m^3‚Äù √© uma nova coluna que foi criada a partir da multiplica√ß√£o das colunas ‚Äúgain‚Äù e ‚ÄúpulseCount‚Äù, resultando na quantidade de g√°s consumido pelo cliente em metros c√∫bicos. Al√©m disso, gerou-se uma segunda coluna chamada ‚Äúvaria√ß√£o_consumo‚Äù, que permite ver a varia√ß√£o do consumo em um determinado per√≠odo de tempo. Dessa forma, essas duas novas features, ‚Äúconsumo em m^3‚Äù e ‚Äúvaria√ß√£o_consumo‚Äù, mostram-se relevantes para o modelo, uma vez que nos d√° informa√ß√µes indispens√°veis para a an√°lise.  

&nbsp;&nbsp;&nbsp;&nbsp;A feature ‚Äútimestamp‚Äù representa a quantidade de segundos que se passaram desde 1¬∫ de janeiro de 1970 at√© a data definida. Com essa coluna √© poss√≠vel analisar padr√µes de consumo de g√°s em diferentes per√≠odos, como picos em determinadas horas, dias ou meses. Assim, √© poss√≠vel realizar uma an√°lise temporal mais detalhada, o que facilita a detec√ß√£o de anomalias relacionadas a intervalos espec√≠ficos de tempo.

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m das features escolhidas, tamb√©m h√° aquelas que n√£o foram selecionadas, e suas exclus√µes foram analisadas com base no impacto insignificante na detec√ß√£o de anomalias ou por redund√¢ncia em rela√ß√£o a outras vari√°veis. Por exemplo, as features relacionadas √† localiza√ß√£o, como "bairro_encoded", "cidade_canoas", "cidade_gravatai", entre outras, n√£o foram consideradas essenciais, pois a an√°lise j√° inclui a coluna de temperatura, que registra a m√©dia da temperatura do ambiente em um determinado dia e influencia diretamente o consumo de g√°s, tornando as vari√°veis de localiza√ß√£o redundantes para esse fim.

&nbsp;&nbsp;&nbsp;&nbsp;Portanto, as justificativas apresentadas evidenciam a import√¢ncia dessas features na modelagem, destacando como elas podem influenciar na detec√ß√£o de anomalias no consumo de g√°s natural. Assim, com essa sele√ß√£o, o processo de an√°lise de dados torna-se mais centrado e objetivo, garantindo menor tempo de processamento da an√°lise.

#### 4.3.2 Justificativa para a defini√ß√£o do K do modelo
&nbsp;&nbsp;&nbsp;&nbsp;Modelos de aprendizagem n√£o-supervisionada s√£o aqueles que devem ser utilizados em casos de an√°lise de dados onde tais dados n√£o est√£o rotulados. No caso atual, como n√£o existe nenhum tipo de r√≥tulo sobre anomalias/fraudes no consumo de g√°s na base de dados, o modelo a ser usado ser√° um n√£o-supervisionado. Como discutido acima, utilizaremos, inicialmente, o modelo K-Means, e a presente se√ß√£o tem como objetivo rodar um algoritmo que consiga identificar qual o melhor valor de K (quantidade de clusters que o modelo utilizar√°) para rodar este modelo (Thiago, 2023). <br><br>
&nbsp;&nbsp;&nbsp;&nbsp;O algoritmo criado roda o modelo Kmeans para diversos valores em um intervalo de 1 a 10 e analisa a in√©rcia em cada caso, ou seja, o quanto de varia√ß√£o existe entre um K e outro. O objetivo disto reside em salvar esses valores de in√©rcia, e, ent√£o, fazer um gr√°fico chamado de ‚Äúgr√°fico do cotovelo‚Äù, onde podemos analisar como a quantidade de clusters influencia o modelo. De acordo com a an√°lise do gr√°fico, definimos inicialmente um valor de K = 3, uma vez que este √© o ponto onde h√° a ‚Äúquebra‚Äù.

<div align="center">
   <sup>Figura 15 - Gr√°fico do cotovelo gerado para valores K de 1 a 10 </sup>

   <img src="../assets/grafico_cotovelo.png"><br>
   
   <sub>Fonte: Material produzido pelos autores (2024)</sub>
</div>
 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://bit.ly/4ed0egY).<br><br>

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, a identifica√ß√£o de um valor ideal para o K do modelo √© de grande import√¢ncia para a etapa de modelagem dos dados, uma vez que significa que o modelo preditivo ir√° separar e classificar os dados em uma quantidade correta/adequada de clusters.

#### 4.3.3. Modelagem do problema

&nbsp;&nbsp;&nbsp;&nbsp;No contexto de aprendizado de m√°quina, depois que um *dataset* passa pelos processos de tratamento e pr√©-processamento ou demais metodologias iniciais, deve-se implementar opera√ß√µes, l√≥gicas e algoritmos sobre a base de dados normalizada para evidenciar rela√ß√µes entre as colunas, tentando identificar e representar visualmente padr√µes em  entre os registros (G√©ron, 2019).<br><Br>
&nbsp;&nbsp;&nbsp;&nbsp;No presente projeto de aprendizado de m√°quina n√£o supervisionado, ou seja, descontido de r√≥tulos pr√©vios dos padr√µes desejados, ser√° utilizado o modelo de clusteriza√ß√£o ‚Äî categoriza√ß√£o ‚Äî *k-means*, o qual funciona dividindo um conjunto de dados em k grupos, sendo k um valor definido pelo usu√°rio ou pela equipe. O processo come√ßa com a sele√ß√£o de k centr√≥ides (pontos iniciais que representam cada grupo). Em seguida, cada ponto da base de dados √© atribu√≠do ao seu centr√≥ide mais pr√≥ximo atrav√©s de um c√°lculo de dist√¢ncia entre pontos no plano cartesiano. Com isso, os centr√≥ides s√£o recalculados como a m√©dia dos pontos atribu√≠dos a eles, e o processo se repete at√© que os centr√≥ides n√£o apresentarem mais mudan√ßas significativas (Filho, 2017). Com isso, busca-se a aplica√ß√£o do modelo para identificar e agrupar padr√µes de consumo de g√°s natural, para assim detectar dados extrapolantes ou suspeitos que possam indicar anomalias na utiliza√ß√£o.<br>

**Importa√ß√£o das bibliotecas e obten√ß√£o das features selecionadas**

&nbsp;&nbsp;&nbsp;&nbsp;Como primeiro passo para a modelagem do problema, utilizou-se os componentes *KMeans* e *silhouette_score* da biblioteca *Sklearn* para a composi√ß√£o de gr√°ficos com as features selecionadas para a modelagem. Com o objetivo de selecionar um repert√≥rio com as colunas mais importantes do *DataFrame* normalizado, de forma a acarretar uma an√°lise bem direcionada sobre o consumo de g√°s natural, de 51 colunas foram escolhidas 9 como mais relevantes para compor os gr√°ficos do modelo. Com isso, guardou-se as colunas selecionadas na seguinte lista:

```python
colunas_selecionadas = [
    'temp_scaled',
    'perfil_consumo_Aquecedor',
    'perfil_consumo_Caldeira',
    'perfil_consumo_Coc√ß√£o',
    'perfil_consumo_Coc√ß√£o + Aquecedor',
    'perfil_consumo_Coc√ß√£o + Aquecedor + Piscina',
    'perfil_consumo_Coc√ß√£o + Caldeira',
    'consumo em m^3',
    'varia√ß√£o_consumo',
]

```
&nbsp;&nbsp;&nbsp;&nbsp;Em seguida, para fins de efici√™ncia de testagem, a equipe optou por implementar um algoritmo de recorte aleat√≥rio dos dados do DataFrame, inicialmente reduzido para 12% do conjunto. Dessa forma, apenas as colunas selecionadas foram integrados √† redu√ß√£o do DataFrame "*df_merged*" , compondo, assim, o novo DataFrame "*df_sample*".

```python

df_sample = df_merged.sample(frac=0.12, random_state=42) # 12% dos dados
## Converte valores booleanos para int (True = 1, False = 0)
df_sample[colunas_selecionadas] = df_sample[colunas_selecionadas].astype(int)
df_sample[colunas_selecionadas].info()

```

**Programa√ß√£o do modelo K-Means**

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a realiza√ß√£o do c√°lculo de *k* (n√∫mero de *clusters*), alcan√ßando-se o valor 3, construiu-se o algoritmo do k-means exposto e comentado abaixo. Com a execu√ß√£o do c√≥digo, ser√° exposto o gr√°fico que categoriza os 3 clusters considerando valores de data (timestamp) e varia√ß√£o de consumo. O crit√©rio utilizado para a defini√ß√£o das anomalias consistiu nos 5% de pontos com maiores dist√¢ncias dos clusters encontrados.<br>


```python

# N√∫mero de k definido pela equipe
k = 3

# Aplicando o K-Means, com o n√∫mero de clusters 3
kmeans = KMeans(n_clusters=k, random_state=42)
kmeans.fit(df_sample[colunas_selecionadas]) #Adapta√ß√£o do k-means √†s colunas selecionadas do dataframe

# Atribuindo os clusters
df_sample['cluster'] = kmeans.labels_

# Calculando a dist√¢ncia aos centroides para cada ponto do gr√°fico
df_sample['distance_to_centroid'] = np.min(kmeans.transform(df_sample[colunas_selecionadas]), axis=1)

# Definindo um limiar para identificar anomalias, os 5% de pontos com maiores dist√¢ncias ser√£o consideradas como anomalias
threshold = np.percentile(df_sample['distance_to_centroid'], 95)

# Identificando anomalias de acordo com o c√°lculo acima
df_sample['anomaly'] = df_sample['distance_to_centroid'] > threshold

df_sample_positive = df_sample[df_sample['varia√ß√£o_consumo'] >= 0]

plt.figure(figsize=(10, 6))

for cluster in range(k):
    cluster_data = df_sample_positive[df_sample_positive['cluster'] == cluster]
    plt.scatter(cluster_data['timestamp'], 
                cluster_data['varia√ß√£o_consumo'], 
                label=f'Cluster {cluster}')

plt.title('Clusters de Varia√ß√£o de Consumo de G√°s (Apenas Varia√ß√£o de Consumo Positiva)')
plt.xlabel('Timestamp')
plt.ylabel('Consumo em m¬≥')
plt.legend()

plt.grid(True)
plt.show()

```
&nbsp;&nbsp;&nbsp;&nbsp;Nesse contexto, dentro do seguinte gr√°fico, os pontos de mesma cor s√£o de mesma categoria, e foram apenas considerados os pontos de varia√ß√£o positiva de consumo de g√°s natural, em metros c√∫bicos. Nesse sentido, em hip√≥tese, os pontos laranja representam em maior parte as linhas de consumo em que n√£o houve, ou houve muito pouca, varia√ß√£o de consumo. Os pontos em azul significam uma diferen√ßa de consumo mais significativa, ao longo do tempo. J√° os pontos em verde apresentam varia√ß√£o do consumo praticamente nula. No geral, uma parte reduzida dos dados que comp√µem o modelo evidencia varia√ß√£o; das 2.900.000 linhas, s√≥ 400.000 tem diferen√ßa e o restante n√£o apresenta varia√ß√£o.

<div align="center">
   <sup>Figura 16 - Gr√°fico de Clusters de Varia√ß√£o Positiva do Consumo de G√°s </sup>

   <img src="../assets/grafico_variacao.png">

   <sub>Fonte: Material produzido pelos autores (2024)</sub>

</div>
 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://bit.ly/3z6QfLb).<br><br>

 &nbsp;&nbsp;&nbsp;&nbsp;Tamb√©m foi elaborado o algoritmo abaixo que plota o gr√°fico para demonstra√ß√£o de clusters por consumo acumulado (e n√£o varia√ß√£o) de g√°s natural. Os pontos em laranja representam o menor consumo acumulado, seguidos dos pontos em azul que possuem consumo mais significativo, observando-se, por fim, os pontos em verde que evidenciam uma utiliza√ß√£o exacerbada de g√°s natural, o que pode ser levado como aspecto de aten√ß√£o para a detec√ß√£o de anomalias.

 ```python
 plt.figure(figsize=(10, 6))

## Loop para plotar os clusters
for cluster in range(k):
    cluster_data = df_sample_positive[df_sample_positive['cluster'] == cluster]
    plt.scatter(cluster_data['timestamp'], 
                cluster_data['consumo em m^3'], 
                label=f'Cluster {cluster}')

plt.title('Clusters de Consumo de G√°s Acumulado')
plt.xlabel('Timestamp')
plt.ylabel('Consumo acumulado em m¬≥ (pulsecount * gain)')
plt.legend()

plt.grid(True)
plt.show()
 ```

<div align="center">
   <sup>Figura 17 - Gr√°fico de Clusters de Consumo Acumulado de G√°s </sup>

   <img src="../assets/grafico_consumo_acum.png">

   <sub>Fonte: Material produzido pelos autores (2024)</sub>

</div>

 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://bit.ly/3XrTXHc).<br>

&nbsp;&nbsp;&nbsp;&nbsp;Com isso, implementou-se uma outra programa√ß√£o para contagem valores obtidos em cada um dos 3 clusters.

```python
cluster_counts = df_sample_positive['cluster'].value_counts()

plt.figure(figsize=(8, 5))
plt.bar(cluster_counts.index, cluster_counts.values, color='skyblue')

plt.title('Quantidade de Valores em Cada Cluster')
plt.xlabel('Clusters')
plt.ylabel('Quantidade de Valores')

## Adicionando os valores acima das barras
for i, v in enumerate(cluster_counts.values):
    plt.text(cluster_counts.index[i] - 0.1, v + 10, str(v), color='black', fontweight='bold')

plt.grid(axis='y')
plt.show()

```
<div align="center">

   <sup>Figura 18 - Gr√°fico de Contagem de Dados por Cluster </sup>

   <img src="../assets/grafico_qtd_clusters.png">

   <sub>Fonte: Material produzido pelos autores (2024)</sub>

</div>
 &nbsp;&nbsp;&nbsp;&nbsp;Confira a imagem exposta acima por meio do link: (https://bit.ly/4ep6ySx).<br><Br>

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, como m√©todo de valida√ß√£o da quantifica√ß√£o de dados encontrados em cada cluster, tem-se o seguinte algoritmo de c√°lculo do coeficiente de silhueta, que ajuda a avaliar o modelo e detectar valores adequados, ou n√£o, para o coeficiente k de clusters. A varia√ß√£o do coeficiente deve estar entre -1 e 1, sendo valores negativos ou mais pr√≥ximos de zero indicadores de um agrupamento inadequado de clusters, e valores pr√≥ximos de 1 representantes um agrupamento cada vez mais delimitado, o que √© desejado para o modelo preditivo (Scaldelai, 2022).


```python
# Remover poss√≠veis anomalias para calcular a silhueta apenas para os dados normais (sem outliers)
df_sample_no_anomalies = df_sample_positive[df_sample_positive['anomaly'] == False]

# Selecionar as colunas utilizadas para a cria√ß√£o dos clusters
X = df_sample_no_anomalies[colunas_selecionadas]

# Calcular o coeficiente da silhueta
silhouette_avg = silhouette_score(X, df_sample_no_anomalies['cluster'])

print(f'Coeficiente da Silhueta: {silhouette_avg}')
# Output: Coeficiente da Silhueta: 0.9790473924065777

```

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a execu√ß√£o do modelo, tem-se como output o coeficiente de silhueta no valor 0.9790473924065777 (97%), que significa que os pontos est√£o muito bem agrupados dentro dos seus clusters e bem separados dos pontos de outros clusters. Apesar de que, quanto maior este coeficiente, melhor, nem sempre isso se mostra como verdade. Um coeficiente de 97% indica que os dados est√£o agrupados de forma quase excelente, havendo o risco de se indicar que pode ter ocorrido um *overfitting* dos dados.

&nbsp;&nbsp;&nbsp;&nbsp;Conclui-se, ent√£o, que a equipe alcan√ßou √™xito na aplica√ß√£o da base de dados normalizada para a implementa√ß√£o do modelo preditivo K-Means, demonstrando pensamento cr√≠tico e compet√™ncia na aplica√ß√£o dos conhecimentos de programa√ß√£o adquiridos na Sprint para a escolha de algoritmos e a prepara√ß√£o dos dados. O alto coeficiente de silhueta de 0.979 indica uma poss√≠vel efic√°cia do modelo em identificar padr√µes e poss√≠veis anomalias no consumo de g√°s natural, entretanto, tamb√©m evidencia uma possibilidade de overfitting do modelo, algo que necessitar√° corre√ß√£o. Esses resultados servir√£o de base para discuss√µes sobre as √°reas de aprimoramento, al√©m de contribuir para a evolu√ß√£o do modelo no contexto de detec√ß√£o de anomalias.

#### 4.3.4. An√°lise do Modelo
&nbsp;&nbsp;&nbsp;&nbsp;O modelo k-means √© um algoritmo de aprendizado n√£o supervisionado usado para agrupar dados em clusters, feito dividindo um conjunto de dados em k grupos, onde  k √© um n√∫mero definido pelo usu√°rio (AHMED, 2020). Assim, o algoritmo inicialmente escolhe k centros (centr√≥ides) de forma aleat√≥ria e, em seguida, atribui cada ponto de dados ao centro mais pr√≥ximo, formando clusters. Depois, os centr√≥ides s√£o ajustados para serem o ponto m√©dio de todos os dados em seus respectivos clusters. Desse modo, esse processo de atribui√ß√£o e ajuste continua at√© que os centr√≥ides n√£o mudem mais significativamente, indicando que os clusters est√£o est√°veis, caracterizando em um algoritmo simples e eficiente, sendo amplamente utilizado em an√°lise explorat√≥ria de dados, detec√ß√£o de padr√µes e segmenta√ß√£o, principalmente em grandes volumes de dados.

&nbsp;&nbsp;&nbsp;&nbsp; Nesse sentido, o modelo aqui presente foi desenvolvido seguindo esse tipo de algoritmo, com o objetivo de identificar padr√µes e agrupar dados de forma n√£o supervisionada, sendo especialmente √∫til para detec√ß√£o de anomalias em grandes conjuntos de dados, como no caso trago pela Compass. No contexto espec√≠fico, a equipe definiu k=3, ou seja, tr√™s clusters, para agrupar as varia√ß√µes de consumo de g√°s. Ap√≥s a aplica√ß√£o do k-means nas 9 colunas selecionadas do DataFrame, os dados foram organizados em clusters com base na proximidade aos centr√≥ides, que s√£o ajustados iterativamente para minimizar a dist√¢ncia entre os dados e seus respectivos centros. A dist√¢ncia de cada ponto ao centr√≥ide mais pr√≥ximo foi calculada, e um limiar foi estabelecido para identificar anomalias, considerando os 5% dos pontos com maiores dist√¢ncias como desvios. Em seguida, o modelo foi ent√£o visualizado em um gr√°fico que destaca as varia√ß√µes de consumo positivas e seus respectivos clusters, facilitando a an√°lise das anomalias e dos comportamentos regulares de consumo, como apresentado abaixo.

<div align="center">
   
   <sub>Figura 19 - Gr√°fico do Modelo apresentando Varia√ß√£o do Consumo de G√°s </sub>

   <img src="../assets/grafico_modelo1.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, ao longo da an√°lise do modelo desenvolvido, alguns pontos positivos foram destacados, como:

- **Uso de Clusters com K-means**: O modelo K-means foi adaptado para trabalhar com 9 colunas (features) do dataframe, o que permite uma maior compreens√£o dos padr√µes de consumo de g√°s, indo al√©m das tr√™s vari√°veis tradicionais. Isso pode melhorar a segmenta√ß√£o dos clusters e a detec√ß√£o de anomalias, j√° que mais informa√ß√µes est√£o sendo utilizadas na an√°lise. 
- **C√°lculo da Dist√¢ncia aos Centr√≥ides**: A inclus√£o da dist√¢ncia aos centr√≥ides (distance_to_centroid) para cada ponto de dado √© uma abordagem √∫til para identificar anomalias. Isso ajuda a distinguir dados que est√£o afastados dos centros dos clusters, que podem ser considerados comportamentos fora do padr√£o (an√¥malos).
- **Limiar para Anomalias**: A defini√ß√£o de um limiar (os 5% com maior dist√¢ncia, no caso do modelo apresentado) para detectar anomalias torna o processo autom√°tico e facilita a identifica√ß√£o de poss√≠veis outliers no consumo de g√°s. Essa abordagem quantitativa √© vantajosa, pois apresenta de maneira clara a distor√ß√£o dos dados ao longo do modelo. 
- **Gr√°fico de Clusters por Varia√ß√£o Positiva no Consumo**: Os colunas escolhidas para apresentar o modelo, que mostra apenas a varia√ß√£o positiva no consumo de g√°s, √© bem informativo, pois foca em dados que podem ser mais relevantes para a an√°lise (aumento de consumo). Essa abordagem ajuda a simplificar a visualiza√ß√£o e a destacar padr√µes importantes.

&nbsp;&nbsp;&nbsp;&nbsp; √Ä vista disso, essas caracter√≠sticas foram destacadas por sua import√¢ncia na efici√™ncia global do modelo, uma vez que s√£o elementos essenciais para o seu funcionamento adequado. Ao considerar esses aspectos, garantimos que o modelo opere de maneira otimizada, assegurando a precis√£o e a robustez dos resultados obtidos.

&nbsp;&nbsp;&nbsp;&nbsp; Todavia, ao longo da cria√ß√£o do modelo, pontos de ressalva foram aparecendo, ajudando na compreens√£o do modelo como um todo, no que diz respeito tamb√©m os passos futuros:

- **Avaliar Diferentes Limiares**: Testar diferentes percentuais ou estrat√©gias para definir o limiar de anomalias √© fundamental para adaptar o modelo √† distribui√ß√£o dos dados. Isso garante uma detec√ß√£o mais precisa, evitando a sub ou superestima√ß√£o de anomalias. Ajustes adequados aumentam a efic√°cia na identifica√ß√£o de padr√µes an√¥malos, otimizando a performance do modelo. 
- **N√∫mero de Clusters Fixo**: O n√∫mero de clusters no K-means √© um par√¢metro que deve ser ajustado manualmente. Se o n√∫mero de clusters n√£o for escolhido corretamente, o modelo pode sub ou superestimar os grupos, impactando a detec√ß√£o de anomalias. Embora exista o M√©todo do Cotovelo para encontrar o k, um valor fixo de clusters n√£o √© interessante no que diz respeito o contexto de anomalias, considerando que ele √© baseado na suposi√ß√£o de que todos os clusters s√£o representativos de padr√µes normais, o que pode n√£o ser ideal no cen√°rio trabalhado.
- **Detec√ß√£o de Anomalias**: O K-means n√£o √© originalmente um modelo de detec√ß√£o de anomalias (BUENO, 2021). Ele pode ser usado para esse fim ao identificar pontos que est√£o distantes dos centr√≥ides dos clusters, mas isso pode n√£o ser t√£o eficiente quanto m√©todos mais espec√≠ficos, como Isolation Forest ou DBSCAN, que s√£o projetados para identificar outliers.
- **Utiliza√ß√£o de Apenas um Modelo**: Modelos diferentes podem ter desempenhos distintos para o mesmo conjunto de dados. O K-means, apesar de ser eficaz em muitas situa√ß√µes, pode n√£o capturar todas as anomalias, especialmente em dados complexos. Testar mais de um modelo permite avaliar qual oferece as melhores previs√µes e identifica os padr√µes de maneira mais precisa. √â fundamental comparar os resultados de cada modelo em termos de precis√£o, recall, e taxa de falsos positivos. A an√°lise deve incluir essas m√©tricas para justificar por que um modelo pode ser preferido ao outro. 
- **Dificuldade da Visualiza√ß√£o das Anomalias do Modelo**: O gr√°fico inicialmente escolhido n√£o representa adequadamente a disposi√ß√£o dos clusters, pois alguns deles se sobrep√µem, dificultando a visualiza√ß√£o e a identifica√ß√£o das anomalias. Para melhorar a clareza, foi apresentado uma nova visualiza√ß√£o que inclui uma coluna adicional, o "timestamp". Essa nova visualiza√ß√£o agrupa as amostras dos clusters de maneira mais eficaz, facilitando a an√°lise e identifica√ß√£o das anomalias, como visto abaixo:

<div align="center">
   
   <sub>Figura 20 - Gr√°fico do Modelo apresentando Varia√ß√£o do Consumo de G√°s (Ap√≥s Melhoria) </sub>

   <img src="../assets/grafico_modelo2.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp; Nesse sentido, a an√°lise desses pontos negativos foi fundamental, pois trouxe clareza sobre √°reas de melhoria, contribuindo para o desenvolvimento de um modelo visualmente mais eficaz. Al√©m disso, essa avalia√ß√£o esclareceu os pr√≥ximos passos a serem tomados, especialmente no que diz respeito √† modelagem e √† sele√ß√£o de features, orientando o grupo para futuras decis√µes mais embasadas.

&nbsp;&nbsp;&nbsp;&nbsp; Logo, podemos ver que analisar um modelo preditivo de anomalias √© essencial para futuras implementa√ß√µes e melhorias, pois proporciona uma compreens√£o detalhada do desempenho do modelo e das suas limita√ß√µes. Assim, essa an√°lise permite identificar padr√µes de erro e √°reas onde o modelo pode n√£o estar capturando anomalias de maneira eficaz, possibilitando, com base nesses *insights*, ajustar algoritmos, aprimorar a sele√ß√£o de features e refinar os par√¢metros do modelo, garantindo uma detec√ß√£o mais precisa e robusta de anomalias. 

#### 4.3.5 An√°lise dos resultados do modelo

&nbsp;&nbsp;&nbsp;&nbsp;Ao utilizar um modelo de aprendizado n√£o supervisionado, conforme o qual foi utilizado e apresentado nas se√ß√µes 4.3.1 e 4.3.2, √© importante entender como ocorre a avalia√ß√£o dos resultados ap√≥s as predi√ß√µes do modelo. Por se tratar de um modelo que n√£o utiliza dados rotulados, o *K-means*, modelo de *machine learning* escolhido, analisa os dados e procura agrup√°-los em *clusters* com base nas caracter√≠sticas que os mesmos possuem em comum. Apesar de ser uma √≥tima ferramenta para dados n√£o categorizados, as m√©tricas para entendimento dos resultados s√£o diferentes das m√©tricas mais utilizadas para modelos supervisionadas, como precis√£o, *recall* e acur√°cia, pois n√£o √© poss√≠vel fazer compara√ß√µes entre os dados com r√≥tulos reais e as categorias preditas pelo modelo.

&nbsp;&nbsp;&nbsp;&nbsp;A partir disso, foram escolhidas 3 m√©tricas para analisar qu√£o bem o modelo n√£o supervisionado consegue distinguir os dados entre normais ou an√¥malos, sendo elas: o teste de Komogorov-Smirnov, o Silhouette Score e a compara√ß√£o de features entre as anomalias detectadas.

##### 4.3.5.1 Teste de Komogorov-Smirnov

&nbsp;&nbsp;&nbsp;&nbsp;O teste de Komogorov-Smirnov, tamb√©m conhecido como teste KS, √© um algoritmo que realiza testes estat√≠sticos e compara diferentes distribui√ß√µes tentando entender qu√£o diferentes as mesmas s√£o, o que possibilita entender qu√£o bem os dados foram separados. Como o teste KS √© um teste n√£o param√©trico - ou seja, funciona para distribui√ß√µes estat√≠sticas al√©m das normais e binomiais -, ele √© um forte candidato √† m√©trica para o modelo. Al√©m disso, o teste KS √© composto por duas vari√°veis: o KS, que quanto mais se aproxima de 1, √© melhor; e o p-value, que quanto mais se aproxima de 0, √© melhor.

&nbsp;&nbsp;&nbsp;&nbsp;O teste foi executado comparando as features selecionadas para o modelo nos 3 diferentes clusters utilizados pelo K-means, mas para que o entendimento de cada vari√°vel/valor de teste fosse mais f√°cil, foram encontrados as 10 compara√ß√µes com maior valor de KS, as 10 compara√ß√µes com menor valor de p-value e as melhores compara√ß√µes levando em considera√ß√£o tanto o valor de KS quanto o valor de p-value.


&nbsp;&nbsp;&nbsp;&nbsp;Abaixo, √© poss√≠vel ver as compara√ß√µes com maior valor de KS.

<div align="center">
   
   <sub>Figura 21 - Gr√°fico das 10 melhores compara√ß√µes em rela√ß√£o ao KS</sub>

   <img src="../assets/KS.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>


&nbsp;&nbsp;&nbsp;&nbsp;Depois, √© poss√≠vel observar as compara√ß√µes com menor p-value.

<div align="center">
   
   <sub>Figura 22 - Gr√°fico das 10 melhores compara√ß√µes em rela√ß√£o ao p-value</sub>

   <img src="../assets/p-values.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>


&nbsp;&nbsp;&nbsp;&nbsp;Por fim, as melhores compara√ß√µes levando em considera√ß√£o tanto os valores de KS quanto os valores de p-value.

<div align="center">
   
   <sub>Figura 23 - Gr√°fico das melhores compara√ß√µes do teste geral</sub>

   <img src="../assets/testeKS.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>


&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, teste de Komogorov-Smirnov mostrou que o modelo K-means detectou apenas 11% das compara√ß√µes com KS maior que 0.8 e p-value menor que 5%, o que demonstra a necessidade de treinar o modelo novamente para que o n√∫mero de anomalias que dentre os 3 clusters possam ser identificadas com mais facilidade e tamb√©m detectadas pelo teste KS.

##### 4.3.5.2 Silhouette Score

&nbsp;&nbsp;&nbsp;&nbsp;O Silhouette Score, ou coeficiente da silhueta, √© um c√°lculo que possibilita identificar qu√£o bem clusterizados est√£o os dados por meio do modelo. Esse coeficiente funciona por meio do c√°lculo de 2 vari√°veis: a coes√£o e a separa√ß√£o.
- A separa√ß√£o dos dados diz respeito ao c√°lculo da dist√¢ncia m√©dia entre um ponto qualquer e todos os pontos do cluster mais pr√≥ximo ao qual ele n√£o pertence;
- A coes√£o dos dados diz respeito ao c√°lculo da dist√¢ncia m√©dia entre um ponto qualquer e todos os pontos do cluster ao qual ele pr√≥prio pertence.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, o coeficiente da silhueta varia de -1 a 1. Quanto mais se aproxima de 1, mais os dados est√£o clusterizados e bem agrupados, e quanto mais se aproxima de -1, mais os dados est√£o dispersos.


&nbsp;&nbsp;&nbsp;&nbsp;Ao calcular o silhouette score m√©dio de todos os dados do dataset, o resultado obtido foi 0.96, o que indica um bom resultado para o coeficiente da silhueta. Entretanto, este score pode significar tamb√©m o overfitting do modelo, o que indica a necessidade de refazer o c√°lculo com o mesmo dataset e outros modelos de clusteriza√ß√£o.

##### 4.3.5.3 An√°lise e compara√ß√£o de features entre os dados an√¥malos


&nbsp;&nbsp;&nbsp;&nbsp;A √∫ltima etapa de an√°lise dos resultados do modelo K-means se baseia em uma investiga√ß√£o mais aprofundada sobre como as predi√ß√µes afetam o conjunto de dados de consumo. Para isso, os dados classificados como anomalias foram armazenados em um dataframe √∫nico e separado dos dados gerais, e a partir do dataframe de anomalias, foram feitas 3 compara√ß√µes e an√°lises: as anomalias de varia√ß√£o de consumo por cluster, a quantidade de anomalias por categoria de resid√™ncia e a rela√ß√£o entre a dist√¢ncia ao centr√≥ide e a varia√ß√£o de consumo.

&nbsp;&nbsp;&nbsp;&nbsp;Primeiro, foi analisado como as anomalias em rela√ß√£o √† feature de varia√ß√£o de consumo estavam distribu√≠das entre os clusters. Descobriu-se que a maioria das anomalias de consumo negativo, as quais podem significar poss√≠veis fraudes ou erros de registro, estavam mais presentes no cluster [].

<div align="center">
   
   <sub>Figura 24 - Gr√°fico das anomalias: varia√ß√£o de consumo x clusters</sub>

   <img src="../assets/variacao_cluster.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Ademais, foi investigado como as anomalias se relacionavam entre a varia√ß√£o de consumo e a dist√¢ncia ao centr√≥ide, o que possibilitou a visualiza√ß√£o de anomalias muito distantes dos centr√≥ides de seus clusters como tamb√©m anomalias com dist√¢ncia insignificante ou muito pequena, indicando que o trabalho de identifica√ß√£o e classifica√ß√£o de dados an√¥malos pelo K-means deve ser revisto.

<div align="center">
   
   <sub>Figura 25 - Gr√°fico das anomalias: varia√ß√£o de consumo x dist√¢ncia ao centr√≥ide</sub>

   <img src="../assets/variacao_distancia.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, foi observada a quantidade de anomalias pelas categorias de resid√™ncia identificadas no primeiro dataframe de informa√ß√µes cadastrais. Dessa maneira, foi poss√≠vel identificar que a maior quantidade de anomalias est√£o na categoria Pr√©dio Existente Individual. Entretanto, √© necess√°rio analisar a possibilidade da distribui√ß√£o de clientes entre as diferentes categorias ser muito concentrada em uma categoria espec√≠fica para que se afirme a maior propens√£o de anomalias em um tipo de instala√ß√£o.

<div align="center">
   
   <sub>Figura 26 - Gr√°fico das anomalias: distribui√ß√£o entre categorias de resid√™ncia</sub>

   <img src="../assets/anomalia_categoria.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Portanto, as m√©tricas utilizadas foram meios de observar qu√£o bem o modelo n√£o supervisionado K-means efetuou a clusteriza√ß√£o dos dados e a identifica√ß√£o das poss√≠veis anomalias e s√£o um ponto de partida para a compara√ß√£o de futuros modelos n√£o supervisionados, possibilitando a reutiliza√ß√£o dos mesmos conceitos e c√°lculos para encontrar um modelo mais ideal para os objetivos da *Compass*.


  
#### 4.3.6 Sistemas de recomenda√ß√£o

&nbsp;&nbsp;&nbsp;&nbsp;Os sistemas de recomenda√ß√£o s√£o ferramentas essenciais que utilizam algoritmos especializados e t√©cnicas de aprendizado de m√°quina para processar dados e fornecer sugest√µes personalizadas aos usu√°rios. Esses sistemas conseguem identificar padr√µes e prever as prefer√™ncias de um usu√°rio com base em intera√ß√µes anteriores, mesmo antes de ele expressar diretamente suas opini√µes. A aplica√ß√£o desses algoritmos permite que empresas ofere√ßam uma experi√™ncia mais personalizada, aumentando a satisfa√ß√£o e o engajamento do cliente.

&nbsp;&nbsp;&nbsp;&nbsp;H√° diferentes tipos de sistemas de recomenda√ß√£o, cada um com abordagens distintas para atender a essa demanda. A filtragem colaborativa, por exemplo, baseia-se na an√°lise do comportamento coletivo de grupos de usu√°rios com prefer√™ncias semelhantes para fazer recomenda√ß√µes. Esse m√©todo √© amplamente utilizado em plataformas de streaming e e-commerce, ajudando a sugerir produtos ou conte√∫dos com base nas escolhas de outros usu√°rios com gostos parecidos. A filtragem baseada em conte√∫do, por outro lado, foca nas caracter√≠sticas dos itens que o usu√°rio j√° demonstrou interesse, como filmes, livros ou produtos, para sugerir outros semelhantes, criando um perfil de prefer√™ncias individuais.

&nbsp;&nbsp;&nbsp;&nbsp;Outro modelo popular √© o sistema h√≠brido, que combina as t√©cnicas de filtragem colaborativa e baseada em conte√∫do para fornecer recomenda√ß√µes ainda mais precisas e abrangentes. Plataformas como a Netflix utilizam essa abordagem para melhorar a precis√£o das sugest√µes, cruzando informa√ß√µes sobre os h√°bitos de consumo dos usu√°rios e as caracter√≠sticas dos itens consumidos. Al√©m disso, existem sistemas de recomenda√ß√£o baseados em demografia, que categorizam os usu√°rios com base em atributos como idade e localiza√ß√£o, e sistemas baseados em utilidade, que levam em considera√ß√£o a funcionalidade e relev√¢ncia do produto para o usu√°rio.

&nbsp;&nbsp;&nbsp;&nbsp;Esses sistemas desempenham um papel fundamental em v√°rias ind√∫strias, desde o varejo at√© o entretenimento, ajudando a maximizar convers√µes, aumentar o valor m√©dio de pedidos e otimizar campanhas de marketing. Ao personalizar a experi√™ncia do usu√°rio, os sistemas de recomenda√ß√£o n√£o apenas facilitam a descoberta de novos produtos e conte√∫dos, como tamb√©m fortalecem a lealdade do cliente, criando um ciclo de reten√ß√£o e engajamento cont√≠nuo.

##### 4.3.6.1 Formula√ß√£o dos Sistemas de Recomenda√ß√£o

&nbsp;&nbsp;&nbsp;&nbsp;Durante o processo de formula√ß√£o de um sistema de recomenda√ß√£o com base nos dados de consumo de g√°s fornecidos √† equipe, foi considerado realizar a an√°lise com o objetivo de identificar padr√µes de uso e sugerir estrat√©gias para otimizar o consumo, evitando congestionamentos e sobrecargas no sistema, al√©m de melhorar a efici√™ncia do fornecimento. A partir desse estudo, propomos solu√ß√µes para diferentes perfis de usu√°rios, incluindo consumidores residenciais, empresariais e gerais.

&nbsp;&nbsp;&nbsp;&nbsp;O processo de tratamento dos dados dos usu√°rios foi estruturado em tr√™s etapas, conforme descrito a seguir:

- Segmenta√ß√£o do Consumo em Blocos de 4 Horas:
&nbsp;&nbsp;&nbsp;&nbsp;Para identificar picos de consumo, os dados foram separados em intervalos de 4 horas. Essa segmenta√ß√£o permite uma an√°lise granular, destacando os per√≠odos de maior e menor demanda ao longo do dia.

- Gera√ß√£o de Gr√°ficos para Visualiza√ß√£o:
&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a segmenta√ß√£o dos dados, gr√°ficos foram gerados para visualizar os picos e os per√≠odos de baixa demanda.

- An√°lise dos Blocos de Menor Congestionamento:
&nbsp;&nbsp;&nbsp;&nbsp;Com base nas m√©dias de consumo dos blocos de 4 horas, foram identificados os hor√°rios de menor congestionamento. A ideia √© que esses hor√°rios possam ser utilizados pela Compass para criar incentivos e estimular os clientes a consumir g√°s nesses per√≠odos, promovendo um uso mais equilibrado do sistema.

&nbsp;&nbsp;&nbsp;&nbsp;Para realizar esta an√°lise, foram tra√ßados tr√™s cen√°rios distintos de uso, que ajudaram a segmentar melhor o comportamento dos consumidores e propor solu√ß√µes espec√≠ficas:

1. Uso Geral: Refere-se a todos os clientes, independentemente do perfil. Esse cen√°rio abrange o comportamento padr√£o de consumo de g√°s de forma geral, sem segmenta√ß√µes espec√≠ficas.

2. Uso Empresarial: Focado em clientes corporativos, como empresas e condom√≠nios, que tendem a ter padr√µes de consumo mais intensos e concentrados em hor√°rios espec√≠ficos. A an√°lise aqui busca otimizar o consumo em hor√°rios de menor demanda para evitar picos que possam sobrecarregar o sistema.

3. Uso Residencial: Destinado a clientes que utilizam g√°s em suas resid√™ncias. O foco √© entender o comportamento de consumo residencial e sugerir hor√°rios de menor utiliza√ß√£o, de modo a equilibrar o fornecimento sem comprometer a conveni√™ncia dos usu√°rios.

##### 4.3.6.2 CEN√ÅRIO 1 - USO GERAL

<div align="center">
   
   <sub>Figura 27 - Gr√°fico do cen√°rio 1 do Sistema de Recomenda√ß√£o - Uso Geral </sub>

   <img src="..\assets\sistema_de_recomendacao_cenario_1_geral.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Na an√°lise do consumo geral, foi gerado um gr√°fico com base no uso de g√°s de todos os usu√°rios. A partir da visualiza√ß√£o do mapa de calor, √© poss√≠vel identificar padr√µes claros de consumo ao longo do dia. Observa-se que os blocos de hor√°rio entre 00h e 03h, assim como entre 20h e 23h, apresentam menores volumes de consumo.

&nbsp;&nbsp;&nbsp;&nbsp;Ao aplicar um sistema de recomenda√ß√£o para sugerir o melhor hor√°rio de menor consumo, identificamos que o bloco mais adequado √© o que se inicia √†s 20h, com um consumo m√©dio de 0,30 m¬≥. Vale destacar que, para fins de visualiza√ß√£o, o gr√°fico foi gerado com dados de 100 usu√°rios. No entanto, o c√°lculo final para a recomenda√ß√£o considerou o total de 1.716 usu√°rios.

##### 4.3.6.3. CEN√ÅRIO 2 - USO EMPRESARIAL

<div align="center">
   
   <sub>Figura 28 - Gr√°fico do cen√°rio 2 do Sistema de Recomenda√ß√£o - Uso Empresarial </sub>

   <img src="..\assets\sistema_de_recomendacao_cenario_2_empresarial.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Para a an√°lise de consumo empresarial, que inclui perfis de empresas e condom√≠nios, tamb√©m foi gerado um gr√°fico a partir do uso geral de g√°s desses perfis. A an√°lise do mapa de calor revela que os blocos de hor√°rio entre 12h e 15h, e entre 16h e 19h, concentram os maiores picos de consumo. Esses per√≠odos refletem o funcionamento intenso das atividades comerciais.

&nbsp;&nbsp;&nbsp;&nbsp;Ao utilizar o sistema de recomenda√ß√£o para identificar o hor√°rio de menor demanda, foi indicado que o bloco mais apropriado para otimizar o uso de g√°s empresarial √© o das 20h, com um consumo m√©dio de 0,89 m¬≥. Esse intervalo noturno pode ser explorado por empresas que t√™m flexibilidade operacional para transferir atividades que consomem g√°s para hor√°rios fora do pico.

&nbsp;&nbsp;&nbsp;&nbsp;Tanto o gr√°fico quanto os c√°lculos foram gerados considerando todos os perfis empresariais dispon√≠veis na base de dados.

##### 4.3.6.4. CEN√ÅRIO 3 - USO RESIDENCIAL

<div align="center">
   
   <sub>Figura 29 - Gr√°fico do cen√°rio 3 do Sistema de Recomenda√ß√£o - Uso Residencial </sub>

   <img src="..\assets\sistema_de_recomendacao_cenario_3_residencial.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Na an√°lise do uso residencial, foi gerado um gr√°fico com base no consumo geral dos usu√°rios dom√©sticos. A partir do mapa de calor, √© percept√≠vel que os blocos de hor√°rios entre 00h e 03h e entre 20h e 23h s√£o aqueles em que o consumo tende a ser maior.

&nbsp;&nbsp;&nbsp;&nbsp;Com a aplica√ß√£o do sistema de sugest√£o de hor√°rios, constatou-se que o melhor momento para reduzir o consumo residencial √© o bloco que come√ßa √†s 4h, com um consumo m√©dio de 0,28 m¬≥. 

&nbsp;&nbsp;&nbsp;&nbsp;√â importante frisar que o gr√°fico foi gerado utilizando dados de 80 usu√°rios para fins de visualiza√ß√£o, mas o c√°lculo final de recomenda√ß√£o foi baseado no total de 1.676 usu√°rios.

##### 4.3.6.5. Conclus√£o

&nbsp;&nbsp;&nbsp;&nbsp;Com base nos padr√µes de consumo observados nos tr√™s cen√°rios, sugerimos √† _Compass_ a implementa√ß√£o de um esquema otimizado de hor√°rios, dividido em quatro intervalos distintos, para promover o uso mais eficiente do g√°s. Esses hor√°rios podem ser aproveitados para direcionar iniciativas como descontos e promo√ß√µes durante os per√≠odos de menor demanda, ajudando a evitar sobrecargas no sistema e a incentivar o uso racional.

&nbsp;&nbsp;&nbsp;&nbsp;Essas a√ß√µes poderiam ser respons√°veis por otimizar o uso de g√°s e proporcionar benef√≠cios aos usu√°rios finais, criando um sistema mais sustent√°vel e equilibrado.

### 4.4. Compara√ß√£o de Modelos
&nbsp;&nbsp;&nbsp;&nbsp; Esta se√ß√£o tem como objetivo documentar as compara√ß√µes entre os diferentes modelos que foram utilizados para cumprir o objetivo do projeto durante a Sprint 4. Anteriormente, foi escolhido e explorado apenas um modelo candidato. Para a se√ß√£o atual, ser√£o testados diversos modelos, a fim de realmente encontrar um que melhor se encaixe no projeto atual. 

#### 4.4.1 Isolation Forest
&nbsp;&nbsp;&nbsp;&nbsp;Para o nosso pr√≥ximo modelo, escolhemos o *Isolation Forest*, um modelo preditivo n√£o supervisionado utilizado para detec√ß√£o de anomalias, que funciona isolando pontos de dados por meio da constru√ß√£o de √°rvores de decis√£o. Desse modo, a ideia principal desse modelo √© que as anomalias, por serem menos frequentes e diferentes do restante dos dados, sejam mais f√°ceis de isolar, ou seja, precisem de menos divis√µes na √°rvore para serem separadas (LIU, 2008). Assim, o algoritmo seleciona aleatoriamente uma caracter√≠stica e um valor de corte, e ao repetir esse processo v√°rias vezes, ele consegue identificar as observa√ß√µes que se desviam do padr√£o, sendo, consequentemente, o *Isolation Forest* um modelo eficiente, escal√°vel e amplamente utilizado em √°reas como detec√ß√£o de fraudes, an√°lise de seguran√ßa e monitoramento de redes.

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, al√©m da constru√ß√£o do modelo, a etapa de an√°lise dos resultados de suas m√©tricas √© essencial para avaliar a sua efic√°cia em identificar padr√µes an√¥malos sem o uso de r√≥tulos previamente definidos. M√©tricas como o √çndice de Silhueta, o √çndice de Davies-Bouldin (DBI) e o √çndice de Calinski-Harabasz (CH) fornecem insights sobre a coes√£o e separa√ß√£o dos grupos detectados, ajudando a entender a qualidade da segmenta√ß√£o entre os dados normais e an√¥malos. Uma an√°lise cuidadosa dessas m√©tricas permite refinar o modelo, identificar limita√ß√µes e ajustar par√¢metros para obter maior precis√£o na detec√ß√£o de anomalias, garantindo que o modelo atenda √†s expectativas do projeto (STRAUSS, 2022).

&nbsp;&nbsp;&nbsp;&nbsp; √Å vista disso, utilizando o modelo desenvolvido, e tendo como m√©tricas *Davies-Bouldin Index (DBI)*, *Calinski-Harabasz Index (CH Index)* e *Silhouette Score*, fomos capazes de identificar 8850 anomalias de 177075 amostras coletadas, como pode ser visto no gr√°fico abaixo:

<div align="center">
   
   <sub>Figura 30 - Gr√°fico de Contagem de Amostras: Normais vs Anomalias </sub>

   <img src="..\assets\isolationforest_normal_anomalia.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, no estudo de seus resultados, tamb√©m fomos capazes de identificar o seu impacto nos perfis de consumo, tendo em vista que as anomalias se encontram bem mais concentrados no perfil do cliente que usa coc√ß√£o e no do cliente que usa coc√ß√£o e aquecedor, sendo o primeiro bem mais evidente, como podemos ver no gr√°fico a seguir:

<div align="center">
   
   <sub>Figura 31 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="..\assets\perfil_consumo_normal_anomalia.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Logo, com base na an√°lise dos resultados do modelo *Isolation Forest* e nas m√©tricas utilizadas, podemos concluir que o desempenho na detec√ß√£o de anomalias foi eficaz, pois elas ajudaram a validar a separa√ß√£o clara entre as anomalias e os dados normais, indicando que o modelo conseguiu isolar de forma eficaz os outliers sem comprometer a integridade do restante dos dados. Portanto, a combina√ß√£o do isolamento das anomalias com a an√°lise de qualidade da separa√ß√£o entre clusters permitiu uma vis√£o mais completa da robustez do modelo, garantindo maior confiabilidade na detec√ß√£o de comportamentos at√≠picos, considerando as *features* utilizadas.

##### 4.4.1.1 Fine tuning dos hiperpar√¢metros

&nbsp;&nbsp;&nbsp;&nbsp;O processo de fine tuning de hiperpar√¢metros √© um procedimento realizado para encontrar a melhor combina√ß√£o de hiperpar√¢metros que fa√ßa com que o modelo tenha a melhor performance. Nesse sentido, os hiperpar√¢metros s√£o configura√ß√µes externas ao tratamento dos dados e ao pr√≥prio modelo e que devem ser escolhidas manualmente, o que faz com que, a cada nova combina√ß√£o de configura√ß√µes, o modelo tenha um resultado diferente.

&nbsp;&nbsp;&nbsp;&nbsp;Para isso, existem algoritmos especializados que automatizam o processo de ajuste fino dos hiperpar√¢metros e testam todas as combina√ß√µes. Com efeito, a ferramenta escolhida para realizar o fine tuning no Isolation Forest foi o RandomizedSearchCV, um algoritmo que realiza a valida√ß√£o cruzada do modelo com diferentes hiperpar√¢metros aleat√≥ria e repetidamente conforme o n√∫mero de itera√ß√µes escolhido.

&nbsp;&nbsp;&nbsp;&nbsp;Para que o RandomizedSearchCV possa realizar as itera√ß√µes, foi necess√°rio selecionar quais os poss√≠veis valores que gostar√≠amos de testar para os hiperpar√¢metros:

```
param_dist = {
    'n_estimators': [50, 100, 150, 200, 300],
    'max_samples': ['auto', 0.5, 0.75, 1.0],
    'contamination': [0.01, 0.05, 0.1, 'auto'],
    'max_features': [0.5, 0.75, 1.0],
    'bootstrap': [True, False],
    'n_jobs': [-1, 1],
    'warm_start': [True, False],
    'verbose': [0, 1]
}
```
&nbsp;&nbsp;&nbsp;&nbsp; Depois, foi necess√°rio configurar o RandomizedSearchCV, escolhendo qual modelo deveria ter seus hiperpar√¢metros ajustados, o n√∫mero de valida√ß√µes cruzadas e quantas itera√ß√µes o modelo deve ter. Al√©m disso, √© necess√°rio escolher uma m√©trica para que o algoritmo consiga identificar qual combina√ß√£o de hiperpar√¢metros faz o modelo performar melhor. Entretanto, como escolhemos um modelo n√£o supervisionado e escolhemos 3 m√©tricas, foi necess√°rio criar uma fun√ß√£o que unisse as 3 m√©tricas para que fossem utilizadas de uma √∫nica vez pelo algoritmo de fine tuning.

```
```
&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s isso, o modelo retornou as m√©tricas do Isolation Forest com a utiliza√ß√£o da nova escolha de hiperpar√¢metros. Nessa situa√ß√£o o modelo, ap√≥s o ajuste fino de hiperpar√¢metros, performou e conseguiu atingir um score de xxx no coeficiente da silhueta, xxx no DBI e xxx no CH Index, o que mostra uma melhoria vis√≠vel na separa√ß√£o dos dados.

##### 4.4.1.2 An√°lise de resultados ap√≥s o ajuste fino de hiperpar√¢metros

&nbsp;&nbsp;&nbsp;&nbsp;Para entender se a nova configura√ß√£o de hiperpar√¢metros realmente ajudou o modelo a ter uma melhor performance, foi necess√°rio realizar uma nova an√°lise de resultados ao mesmo molde da an√°lise anterior √† aplica√ß√£o da nova configura√ß√£o do modelo. Os seguintes testes foram realizados para entender os novos resultados:

1. Contagem de anomalias e dados normais: foi realizada a contagem de quantos dados foram classificados como anomalias ou n√£o. Nesse sentido, pouco mais de 8.500 dados da amostra foram classificados como an√¥malos, enquanto quase 170.000 foram classificados como dados normais, evidenciando que 5.07% dos dados da amostra s√£o anomalias.

<div align="center">
   
   <sub>Figura 32 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="../assets/isoForestAnomalias.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

2. Contagem de anomalias entre o consumo horarizado e o consumo total em metros c√∫bicos: foi criado o gr√°fico que possibilita ver os dados an√¥malos relacionados ao consumo em metros c√∫bicos (colunas pulseCount e gain) e ao consumo horarizado (diferen√ßa de consumo entre duas medi√ß√µes de um mesmo cliente dividida pela diferen√ßa de tempo entre as medi√ß√µes). √â poss√≠vel entender que o Isolation Forest entende os dados de consumo que saem da m√©dia total como an√¥malos (dados em laranja) e que s√£o os exemplos mais dispersos no gr√°fico e fora da maior concentra√ß√£o (dados em azul). Entretanto, ainda assim h√° dados dentro da concentra√ß√£o que s√£o classificados como anomalias e que devem ser investigados na pr√≥xima sprint.

<div align="center">
   
   <sub>Figura 33 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="../assets/isoForestConsumoTotalHorarizado.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

3. Quantidade de anomalias e dados normais entre os perfis de consumo: para que se pudesse entender como as anomalias est√£o distribu√≠das em cada perfil de consumo, foi criado um gr√°fico de barras que permite ver a distribui√ß√£o de consumo em cada perfil e a classifica√ß√£o dada pelo modelo. Os perfis de consumo Coc√ß√£o e Coc√ß√£o + Aquecedor s√£o os que mais possuem clientes cadastrados, mas, ainda assim, o perfil Coc√ß√£o recebeu mais dados classificados como anomalias diferentemente do perfil Coc√ß√£o + Aquecedor. Essa configura√ß√£o aponta para a necessidade de identificar quais clientes possuem o perfil que mais predominam os dados an√¥malos e entender se a classifica√ß√£o √© coerente com a realidade.

<div align="center">
   
   <sub>Figura 34 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="../assets/isoForestPerfil.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

4. Quantidade de anomalias de acordo com o tempo: depois, foi necess√°rio entender como as anomalias se comportavam ao longo do tempo registrado nas medi√ß√µes. No gr√°fico a seguir, √© poss√≠vel compreender que houve um salto no n√∫mero de anomalias a partir de maio, o que pode ser explicado por clientes que n√£o tinham consumo nos primeiros 3 meses de medi√ß√µes, sazonalidade ou outras quest√µes econ√¥micas e/ou culturais.

<div align="center">
   
   <sub>Figura 35 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="../assets/isoForestAnomaliasTempo.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

5. Anomalias pela diferen√ßa de tempo em horas entre medi√ß√µes: a √∫ltima an√°lise foi feita relacionando a varia√ß√£o de consumo a cada hora e a diferen√ßa em horas entre duas medi√ß√µes consecutivas de um mesmo cliente. Nesse quesito, o modelo Isolation Forest encontrou anomalias dentro do conjunto de dados mais concentrados, o que leva a entender que a diferen√ßa de tempo entre as medi√ß√µes, quando n√£o uma diferen√ßa abrupta, pode n√£o ser a melhor caracter√≠stica para definir um dado an√¥malo ou n√£o.

<div align="center">
   
   <sub>Figura 36 - Gr√°fico de Contagem Perfis de Consumo: Normais vs Anomalias </sub>

   <img src="../assets/isoForestDeltaTime.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

##### 4.4.1.3 Conclus√£o

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, √© poss√≠vel entender que o modelo Isolation Forest, ap√≥s o ajuste fino de hiperpar√¢metros, conseguiu identificar anomalias e trazer insights valiosos sobre em quais perfis ou din√¢micas de consumo os dados an√¥malos se concentram, permitindo uma investiga√ß√£o mais aplicada e profunda na √∫ltima sprint para entender como relacionar cada anomalias com as instala√ß√µes que as geraram.

#### 4.4.2 One Class SVM  

&nbsp;&nbsp;&nbsp;&nbsp;De acordo com Cortes (2021), o Support Vector Machine (SVM) √© um modelo de aprendizado supervisionado amplamente utilizado para tarefas de classifica√ß√£o e regress√£o. Seu objetivo principal √© encontrar um hiperplano que separe os dados em diferentes classes, maximizando a margem, ou seja, a dist√¢ncia entre os pontos mais pr√≥ximos de classes distintas, chamados de vetores de suporte. O SVM √© especialmente eficaz em espa√ßos de alta dimensionalidade e pode lidar com dados n√£o linearmente separ√°veis por meio de fun√ß√µes de kernel. Essas fun√ß√µes transformam os dados em um espa√ßo de maior dimens√£o, onde a separa√ß√£o entre as classes se torna vi√°vel.

&nbsp;&nbsp;&nbsp;&nbsp;Como mencionado, as fun√ß√µes de kernel s√£o t√©cnicas essenciais no SVM, pois permitem a transforma√ß√£o dos dados para um espa√ßo de maior dimens√£o, possibilitando que padr√µes que n√£o s√£o linearmente separ√°veis no espa√ßo original se tornem separ√°veis nesse novo espa√ßo. Elas s√£o fundamentais para o poder do SVM na resolu√ß√£o de problemas complexos, especialmente quando os dados apresentam alta n√£o linearidade.

&nbsp;&nbsp;&nbsp;&nbsp;Com base nesse conceito, espera-se que as fun√ß√µes de kernel, em vez de encontrar diretamente uma linha de separa√ß√£o nos dados originais ‚Äî o que pode n√£o ser vi√°vel ‚Äî, projetem os dados em um novo espa√ßo, onde a separa√ß√£o linear √© poss√≠vel. Vale destacar que essa proje√ß√£o ocorre sem a necessidade de calcular explicitamente a transforma√ß√£o de cada ponto.

&nbsp;&nbsp;&nbsp;&nbsp;Ao longo do tempo, uma varia√ß√£o do SVM foi desenvolvida para lidar com problemas espec√≠ficos de detec√ß√£o de anomalias e outliers, dando origem ao One-Class SVM. Essa varia√ß√£o √© uma adapta√ß√£o do SVM tradicional, projetada para cen√°rios em que h√° exemplos apenas de uma classe. O objetivo do One-Class SVM √© identificar se novos exemplos s√£o semelhantes √† classe normal ou se diferem, sendo considerados anomalias.

&nbsp;&nbsp;&nbsp;&nbsp;Desta forma, conclui-se que o One-Class SVM √© uma variante do SVM projetada para a detec√ß√£o de anomalias. Ele busca definir uma fronteira que envolva a maioria das amostras normais no espa√ßo de caracter√≠sticas, separando-as de poss√≠veis anomalias. Durante o treinamento, o algoritmo ajusta um hiperplano que encapsula as amostras normais, enquanto na fase de teste, amostras que caem fora dessa margem s√£o classificadas como an√¥malas. Esse m√©todo √© amplamente utilizado em tarefas como detec√ß√£o de outliers, fraudes e em sistemas com informa√ß√µes limitadas sobre a distribui√ß√£o de dados an√¥malos (Bando, 2024).

##### 4.4.2.1 Cria√ß√£o de modelo e utiliza√ß√£o das m√©tricas

&nbsp;&nbsp;&nbsp;&nbsp;Hiperpar√¢metrosPara o desenvolvimento do modelo preditivo One-Class SVM, utilizamos um conjunto de features selecionadas com base em sua relev√¢ncia para a detec√ß√£o de anomalias no consumo de g√°s natural. As features escolhidas foram:

- temp_scaled
- perfil_consumo_Aquecedor
- perfil_consumo_Caldeira
- perfil_consumo_Coc√ß√£o
- perfil_consumo_Coc√ß√£o + Aquecedor
- perfil_consumo_Coc√ß√£o + Aquecedor + Piscina
- perfil_consumo_Coc√ß√£o + Caldeira
- consumo em m¬≥
- consumo_horarizado
- delta_time
- varia√ß√£o_consumo

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a defini√ß√£o dessas features, utilizamos tr√™s m√©tricas principais para avaliar a qualidade do modelo e garantir sua efic√°cia na detec√ß√£o de anomalias: o Silhouette Score, o Calinski-Harabasz Index (CH Index) e o Davies-Bouldin Index (DBI).

- Silhouette Score: Essa m√©trica avalia o qu√£o bem os dados est√£o agrupados, medindo a coes√£o dentro dos clusters e a separa√ß√£o entre eles. Valores pr√≥ximos de 1 indicam que os dados est√£o bem separados e que os clusters s√£o compactos.
- Calinski-Harabasz Index (CH Index): O CH Index mede a dispers√£o entre os clusters em compara√ß√£o com a dispers√£o dentro dos clusters. Valores mais altos indicam uma melhor separa√ß√£o e defini√ß√£o entre os grupos.
- Davies-Bouldin Index (DBI): O DBI mede a similaridade entre os clusters. Valores mais baixos, pr√≥ximos de 0, indicam que os clusters est√£o bem separados, ou seja, com baixa sobreposi√ß√£o entre os grupos.

&nbsp;&nbsp;&nbsp;&nbsp;A aplica√ß√£o dessas m√©tricas no contexto do nosso projeto √© fundamental para avaliar e aprimorar a efic√°cia do modelo na detec√ß√£o de anomalias, garantindo que ele identifique de forma precisa os desvios no consumo de g√°s natural.


**Primeira execu√ß√£o do modelo One-CLass SVM:**

&nbsp;&nbsp;&nbsp;&nbsp;Durante o desenvolvimento do modelo preditivo One-Class SVM, projetado para a detec√ß√£o de anomalias, realizamos a primeira execu√ß√£o do modelo sem ajustes de hiperpar√¢metros. Nesta etapa inicial, ao analisarmos o gr√°fico gerado, observamos uma quantidade significativa de anomalias, embora o n√∫mero de dados normais (28.036) seja consideravelmente maior que o de anomalias (1.476). Esse resultado preliminar sugere que, apesar de identificar uma quantidade relevante de anomalias, o modelo ainda pode ser aprimorado com ajustes finos nos hiperpar√¢metros para alcan√ßar uma detec√ß√£o mais precisa.

<div align="center">
   
   <sub>Figura 37 - Primeira visualiza√ß√£o de clusters gerada pelo modelo One-Class SVM </sub>

   <img src="../assets/gr√°fico_one_class_svm.png">

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>


##### 4.4.2.2 Hiperpar√¢metros do modelo One Class SVM

&nbsp;&nbsp;&nbsp;&nbsp;Hiperpar√¢metros em um modelo preditivo s√£o valores que precisam ser definidos antes do processo de treinamento, controlando como o algoritmo ser√° ajustado aos dados de entrada. Eles s√£o respons√°veis por influenciar diretamente o desempenho do modelo. Diferente dos par√¢metros do modelo, que s√£o ajustados automaticamente durante o treinamento, os hiperpar√¢metros precisam ser configurados manualmente ou por meio de t√©cnicas de otimiza√ß√£o.

&nbsp;&nbsp;&nbsp;&nbsp;Nesse contexto, existem t√©cnicas que auxiliam na busca pelos melhores hiperpar√¢metros, como o GridSearchCV e o RandomizedSearchCV, que facilitam a tarefa de encontrar a combina√ß√£o ideal. O GridSearchCV realiza uma nomeada busca exaustiva, testando todas as combina√ß√µes poss√≠veis de hiperpar√¢metros dentro de uma grade definida, o que garante que todas as op√ß√µes ser√£o avaliadas. 

&nbsp;&nbsp;&nbsp;&nbsp;Por outro lado, o RandomizedSearchCV adota uma abordagem mais aleat√≥ria, selecionando um n√∫mero limitado de combina√ß√µes poss√≠veis a partir de uma distribui√ß√£o ou intervalo de valores, reduzindo o tempo necess√°rio para encontrar uma boa solu√ß√£o, mas sem garantir a avalia√ß√£o de todas as combina√ß√µes.



##### 4.4.2.2.1 GridSearchCV

&nbsp;&nbsp;&nbsp;&nbsp;Conforme j√° mencionado acima, o GridSearchCV √© uma t√©cnica usada para encontrar os melhores hiperpar√¢metros de um modelo de aprendizado de m√°quina. Esse m√©todo √© especialmente √∫til para otimizar modelos como o OneClassSVM, que √© comumente usado em tarefas de detec√ß√£o de anomalias. O GridSearchCV ajuda a ajustar hiperpar√¢metros importantes, como nu (fra√ß√£o esperada de outliers) e gamma (que define a forma do kernel do modelo), garantindo que a combina√ß√£o ideal seja selecionada. 

&nbsp;&nbsp;&nbsp;&nbsp;Nesse contexto, ao aplicarmos este m√©todo, chegamos aos seguintes resultados: 
- nu: 0.01 
- gamma: 0.001

##### 4.4.2.2.2 RandomizedSearchCV

&nbsp;&nbsp;&nbsp;&nbsp;O RandomizedSearchCV √© uma t√©cnica utilizada para encontrar hiperpar√¢metros de um modelo de aprendizado de m√°quina de uma maneira que pode ser tida como mais eficiente. No entanto, ao contr√°rio do GridSearchCV, que testa todas as combina√ß√µes poss√≠veis dentro de uma grade de hiperpar√¢metros, o RandomizedSearchCV seleciona aleatoriamente um n√∫mero limitado de combina√ß√µes para avalia√ß√£o.

&nbsp;&nbsp;&nbsp;&nbsp;Nesse contexto, ao aplicarmos este m√©todo, tamb√©m chegamos aos seguintes resultados: 
- nu: 0.01 
- gamma: 0.001

##### 4.4.2.2.3 Aplicando os hiperparametros encontrados 

&nbsp;&nbsp;&nbsp;&nbsp;Com os hiperpar√¢metros otimizados identificados atrav√©s dos m√©todos GridSearchCV e RandomizedSearchCV, podemos agora aplic√°-los ao modelo OneClassSVM. Ambas as t√©cnicas indicaram os mesmos valores ideais para nu e gamma, sendo nu = 0.01 e gamma = 0.001. Esses valores foram ajustados para maximizar o desempenho do modelo na tarefa de detec√ß√£o de anomalias, garantindo um equil√≠brio adequado entre a sensibilidade ao identificar outliers e a forma do kernel do modelo.

**Execu√ß√£o do modelo ap√≥s os hiperpar√¢metros ajustados:**

&nbsp;&nbsp;&nbsp;&nbsp; Ap√≥s o ajuste dos hiperpar√¢metros do modelo, o mesmo c√≥digo foi executado novamente, gerando o gr√°fico de visualiza√ß√£o de clusters. Com os ajustes aplicados, √© poss√≠vel notar algumas diferen√ßas:

1. A quantidade de dados an√¥malos diminuiu significamente de 1476 para 296, e a quantidade de dados normais aumentou de 28036 para 29216. Dessa forma a diferen√ßa da quantidade de dados de cada cluster (anomalia e normal) obteve um aumento.

2. A visualiza√ß√£o dos dados normais (pontos em azul) est√° mais frequente, o que demostra melhor agora um aumento da quantidade desses dados.

<div align="center">
   
   <sub>Figura 38 - Visualiza√ß√£o de clusters gerada pelo modelo One-Class SVM ap√≥s ajuste </sub>

   <img src="../assets/grafico_one_class_svm_2.png">

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Portanto, √© evidente a import√¢ncia das m√©tricas e dos ajustes de hiperpar√¢metros na melhoria do desempenho do modelo preditivo. As mudan√ßas observadas entre a primeira e a segunda execu√ß√£o do modelo mostram como esses ajustes podem aumentar a precis√£o na detec√ß√£o de anomalias. Dessa forma, √© essencial realizar testes cont√≠nuos e refinamentos no modelo para garantir que ele seja o mais eficiente e fiel poss√≠vel na identifica√ß√£o de padr√µes an√¥malos.

#### 4.4.3 DBSCAN

&nbsp;&nbsp;&nbsp;&nbsp;Segundo Gabriel Monteiro (2020), o modelo DBSCAN utiliza uma abordagem que agrupa pontos com base na densidade. A forma√ß√£o dos clusters ocorre a partir da densidade de pontos em uma determinada √°rea. Se um ponto n√£o atender aos crit√©rios de densidade ou aos limites de dist√¢ncia estabelecidos, ele n√£o ser√° classificado em um cluster.

&nbsp;&nbsp;&nbsp;&nbsp;Desse modo, a defini√ß√£o dos par√¢metros √© fundamental para o funcionamento adequado do algoritmo. O par√¢metro Eps define a dist√¢ncia m√°xima em que dois pontos podem estar para serem considerados parte do mesmo cluster. J√° o par√¢metro Min Samples estabelece a quantidade m√≠nima de pontos em uma regi√£o necess√°ria para garantir que haja densidade suficiente para formar um cluster. O DBSCAN n√£o requer a defini√ß√£o pr√©via do n√∫mero de clusters e √© mais r√°pido que o k-means. Ele identifica padr√µes n√£o lineares e lida com outliers, mas pode enfrentar dificuldades em datasets com alta variabilidade de densidade e em dados de alta dimensionalidade, a escolha dos par√¢metros √© cr√≠tica, e a ordem dos dados pode afetar os resultados.

&nbsp;&nbsp;&nbsp;&nbsp;Como primeiro passo da implementa√ß√£o do DBScan, para fins de efici√™ncia na testagem dos c√≥digos, fez-se necess√°ria uma nova redu√ß√£o do DataFrame. Considerando os √∫ltimos ajustes de pr√©-processamento, configurando a varia√ß√£o de consumo de forma horarizada, ou seja, calculada de hor√°rio em hor√°rio, a lista de features selecionadas para a constru√ß√£o do modelo DBScan tamb√©m foi refeita, fazendo a inclus√£o das novas colunas *consumo_horarizado* e *delta_time*.

```python
colunas_selecionadas2 = [
    'temp_scaled',
    'perfil_consumo_Aquecedor',
    'perfil_consumo_Caldeira',
    'perfil_consumo_Coc√ß√£o',
    'perfil_consumo_Coc√ß√£o + Aquecedor',
    'perfil_consumo_Coc√ß√£o + Aquecedor + Piscina',
    'perfil_consumo_Coc√ß√£o + Caldeira',
    'consumo em m^3',
    'consumo_horarizado',
    'delta_time'
]

# Redu√ß√£o do modelo na vari√°vel df_sample2 para testagem do DBScan
df_sample2 = df_merged[colunas_selecionadas2].sample(frac=0.008, random_state=42).astype(int).copy() # 8% dos dados

```
&nbsp;&nbsp;&nbsp;&nbsp;Com isso, fez-se a programa√ß√£o do DBScan, a qual come√ßa com a escalonagem das features selecionadas para implementar uma normaliza√ß√£o, auxiliando nas opera√ß√µes l√≥gicas. Ent√£o, o DBScan √© implementado, configurando-se um valor determinado de *eps* (para cada ponto, o raio m√°ximo para a detec√ß√£o de pontos vizinhos) e do *min_samples* (quantidade m√≠nimas de pontos de um agrupamento para ser considerado um cluster). Ent√£o, para auxiliar na plotagem do gr√°fico, fez-se a redu√ß√£o de dimensionalidade com PCA para apenas 2 componentes, e plotou-se o gr√°fico (Figura 39). Com base no algoritmo, com um eps de 0.6 e um min_samples de 100, foram gerados cerca de 8 clusters, sendo os clusters com valor -1 considerados como "ru√≠dos", ou anomalias.

```python
## Importa√ß√£o das bibliotecas
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN

# Normaliza√ß√£o das colunas selecionadas com o Standart Scaler
scaler = StandardScaler()
features_scaled = scaler.fit_transform(df_sample2)

# Modelo DBSCAN. Eps = dist√¢ncia m√≠nima para 2 pontos serem considerados vizinhos. Min_samples = n√∫mero m√≠nimo de pontos em uma vizinhan√ßa para ser considerado um cluster
dbscan = DBSCAN(eps=0.6, min_samples=100) # Par√¢metros iniciais, os quais ser√£o aprimorados com o futuro fine tuning
clusters = dbscan.fit_predict(features_scaled)

# Redu√ß√£o de dimensionalidade para visualiza√ß√£o do gr√°fico
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Fit dos clusters no DataFrame df_sample2
df_sample2['cluster'] = clusters

# Gr√°fico de visualiza√ß√£o dos clusters em 2D com PCA
plt.figure(figsize=(10, 7))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=clusters, palette='tab10', s=20, edgecolor='k')
plt.title('Clusters DBSCAN - Sample DataFrame')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# Contagem os pontos em cada cluster
print(df_sample2['cluster'].value_counts())

```

<div align="center">
   
   <sup>Figura 39 - Modelo DBScan</sup>

   <img src="..\assets\dbscan_antes_hip.jpg"> 
   
   <sub>Fonte: Material produzido pelos autores (2024)</sub>
   
</div>

**Fine Tuning de Hiperpar√¢metros**

&nbsp;&nbsp;&nbsp;&nbsp;Considerando-se que a altera√ß√£o dos hiperpar√¢metros eps e min_samples interfere no sucesso do modelo, implementou-se um Fine Tuning para realizar v√°rias combina√ß√µes de valores, avaliando os melhores com o c√°lculo da Silhueta.

```python
# Defini√ß√£o de intervalos de eps e min_samples para testagem
eps_values = np.arange(0.2, 1.0, 0.1)  # Testando valores entre 0.4 e 1.0, pulando de 0.1 em 0.1
min_samples_values = range(50, 150, 25)  # Testando valores de 50 a 150, pulando de 25 em 25

# Vari√°veis para guardar os melhores par√¢metros
best_eps = None
best_min_samples = None
best_silhouette_score = -1 # menor valor poss√≠vel para o best_silhouette_score
best_clusters = None

# Loop para testar todas as combina√ß√µes de eps e min_samples
for eps in eps_values:
    for min_samples in min_samples_values:
        # Modelo DBSCAN com os par√¢metros de cada inst√¢ncia
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        clusters = dbscan.fit_predict(features_scaled)
        
        if len(set(clusters)) > 1:  # Garante que h√° mais de um cluster
            score = silhouette_score(features_scaled, clusters)
            
            # Verifica√ß√£o se o score atual √© o melhor
            if score > best_silhouette_score:
                best_silhouette_score = score
                best_eps = eps
                best_min_samples = min_samples
                best_clusters = clusters

# Aplicar os melhores par√¢metros encontrados ao DataFrame original
df_sample2['cluster'] = best_clusters

# Redu√ß√£o de dimensionalidade para visualiza√ß√£o do gr√°fico
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Gr√°fico de visualiza√ß√£o dos clusters em 2D com PCA
plt.figure(figsize=(10, 7))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=df_sample2['cluster'], palette='tab10', s=20, edgecolor='k')
plt.title(f'Clusters DBSCAN - eps: {best_eps}, min_samples: {best_min_samples}')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# Contagem dos pontos em cada cluster
print(df_sample2['cluster'].value_counts())
print(f'Melhor eps: {best_eps}, Melhor min_samples: {best_min_samples}, Melhor Silhouette Score: {best_silhouette_score}')

```
&nbsp;&nbsp;&nbsp;&nbsp;Testando de valores de eps entre 0.2 e 1.0, e de min_samples entre 50 e 150, obteve-se o seguinte output no modelo: *Melhor eps: 0.5000000000000001, Melhor min_samples: 50, Melhor Silhouette Score: 0.7629696140135187*. Este resultado pode ser observado no gr√°fico da Figura 40:

<div align="center">
   
   <sup>Figura 40 - Modelo DBScan / Eps: 0.5 e min_samples 50 </sup>

   <img src="..\assets\dbscan_hip.jpg"> 
   
   <sub>Fonte: Material produzido pelos autores (2024)</sub>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Ent√£o, como decis√£o interna da equipe, al√©m do coeficiente de silhueta foram adicionadas mais duas m√©tricas de avalia√ß√£o do DBScan desenvolvido, sendo elas o Davies-Bouldin Index (DBI) e o Calinski-Harabasz Index (CH Index). Abaixo, segue o algoritmo desenvolvido para essa an√°lise dos hiperpar√¢metros e o output correspondente.

```python
from sklearn.metrics import davies_bouldin_score, calinski_harabasz_score

# Defini√ß√£o de intervalos de eps e min_samples para testagem
eps_values = np.arange(0.2, 1.0, 0.1)  # Testar valores entre 0.2 e 1.0, variando de 0.1 a 0.1
min_samples_values = range(50, 150, 25)  # Testar valores de 50 a 150, variando de 25 a 25

# Vari√°veis para guardar os melhores par√¢metros
best_eps = None
best_min_samples = None
best_silhouette_score = -1
best_dbi = float('inf')  # Para DBI, menor √© melhor, ent√£o inicializamos com infinito
best_ch = -1  # Para CH, maior √© melhor, ent√£o inicializamos com -1
best_clusters = None

# Loop para testar todas as combina√ß√µes de eps e min_samples
for eps in eps_values:
    for min_samples in min_samples_values:
        # Modelo DBSCAN com os par√¢metros atuais
        dbscan = DBSCAN(eps=eps, min_samples=min_samples)
        clusters = dbscan.fit_predict(features_scaled)
        
        if len(set(clusters)) > 1:  # Garantir que h√° mais de um cluster
            
            # Silhouette Score
            sil_score = silhouette_score(features_scaled, clusters)
            
            # Davies-Bouldin Index (DBI)
            dbi = davies_bouldin_score(features_scaled, clusters)
            
            # Calinski-Harabasz Index (CH Index)
            ch = calinski_harabasz_score(features_scaled, clusters)
            
            # Verifica se este √© o melhor conjunto de m√©tricas at√© agora
            if (sil_score > best_silhouette_score and dbi < best_dbi and ch > best_ch):
                best_silhouette_score = sil_score
                best_dbi = dbi
                best_ch = ch
                best_eps = eps
                best_min_samples = min_samples
                best_clusters = clusters

# Aplica os melhores par√¢metros encontrados ao DataFrame df_sample2
df_sample2['cluster'] = best_clusters

# Redu√ß√£o de dimensionalidade para visualiza√ß√£o do gr√°fico
pca = PCA(n_components=2)
features_pca = pca.fit_transform(features_scaled)

# Gr√°fico de visualiza√ß√£o dos clusters em 2D com PCA
plt.figure(figsize=(10, 7))
sns.scatterplot(x=features_pca[:, 0], y=features_pca[:, 1], hue=df_sample2['cluster'], palette='tab10', s=20, edgecolor='k')
plt.title(f'Clusters DBSCAN - eps: {best_eps}, min_samples: {best_min_samples}')
plt.xlabel('Componente Principal 1')
plt.ylabel('Componente Principal 2')
plt.show()

# Contagem dos pontos em cada cluster
print(df_sample2['cluster'].value_counts())
print(f'Melhor eps: {best_eps}, Melhor min_samples: {best_min_samples}, Melhor Silhouette Score: {best_silhouette_score}')
print(f'Melhor Davies-Bouldin Index (DBI): {best_dbi}')
print(f'Melhor Calinski-Harabasz Index (CH Index): {best_ch}')

```

&nbsp;&nbsp;&nbsp;&nbsp;Calculando-se as 3 m√©tricas, obteve-se 10 clusters (contando com um de anomalias) e o seguinte output:<br>

**Melhor eps**: 0.2, Melhor min_samples: 50, Melhor Silhouette Score: 0.7482794489545446<br><br>
**Melhor Davies-Bouldin Index (DBI)**: 1.7877124152854893<br><br>
**Melhor Calinski-Harabasz Index (CH Index)**: 1383.0505542992075.

### An√°lise dos resultados

&nbsp;&nbsp;&nbsp;&nbsp;Para a avalia√ß√£o dos resultados do DBScan, foram executados alguns algoritmos de an√°lise. O primeiro deles foi a an√°lise de consist√™ncia de clusters, implementada com o seguinte c√≥digo:

```python
# Estat√≠sticas descritivas dos clusters
cluster_analysis = df_sample2.groupby('cluster')['consumo em m^3'].describe()
print(cluster_analysis)

```
&nbsp;&nbsp;&nbsp;&nbsp;Com a execu√ß√£o do c√≥digo, observou-se que os clusters definidos com -1 representam os outliers, os quais s√£o poss√≠veis de serem identificados por sua dispers√£o em rela√ß√£o aos demais clusters. o consumo m√©dio √© muito alto, com 544,58 m¬≥, e o desvio padr√£o tamb√©m √© elevado, em 793,06 m¬≥, o que nos mostra que os outliers apresentam uma varia√ß√£o ampla. J√° os clusters definidos com 0, tem o maior n√∫mero de pontos e um consumo m√©dio parecidos, possuem consumo moderadamente baixo e uma dispers√£o moderada. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Os clusters menores (1, 2, 3, 4, etc.) apresentam padr√µes de consumo bem variados, com m√©dias que v√£o de 7,24 m¬≥ at√© 53,81 m¬≥. Isso indica que perfis de consumo diferentes foram agrupados em categorias distintas, ou seja, o DBSCAN conseguiu identificar diferentes grupos de perfis de consumo e separar aqueles que est√£o fora do padr√£o, considerados an√¥malos.<br>

**Histograma de Outliers**

&nbsp;&nbsp;&nbsp;&nbsp;Para a uma interpreta√ß√£o mais direcionada aos outliers, executou-se um histograma com o seguinte c√≥digo, resultando-se na Figura 41:

```python
# Visualizar as primeiras entradas do cluster de ru√≠do (-1)
outliers = df_sample2[df_sample2['cluster'] == -1]
print(outliers.head())

# Visualizar distribui√ß√£o dos outliers
plt.figure(figsize=(10, 6))
sns.histplot(outliers['consumo em m^3'], bins=30, kde=True)
plt.title('Distribui√ß√£o de Consumo - Outliers (ru√≠do)')
plt.show()

```
<div align="center">
   
   <sup>Figura 41 - Histograma de outliers - DBScan</sup>

   <img src="..\assets\outliers_analise.jpg"> 
   
   <sub>Fonte: Material produzido pelos autores (2024)</sub>
   
</div>

<div align="center">
   
   <sup>Figura 42 - Histograma de outliers - DBScan</sup>

   <img src="..\assets\tabela_outliers.jpg"> 
   
   <sub>Fonte: Material produzido pelos autores (2024)</sub>
   
</div>


&nbsp;&nbsp;&nbsp;&nbsp;A visualiza√ß√£o dos outliers com histogramas demonstra que a maioria dos pontos classificados como "ru√≠do" est√° concentrada em consumos baixos. Mais da metade dos outliers tem n√≠veis de consumo abaixo de 500 m¬≥, enquanto alguns apresentam valores muito elevados, ultrapassando 5000 m¬≥, conforme mostrado nos dados descritivos.<br>
&nbsp;&nbsp;&nbsp;&nbsp;A tabela gerada pelo c√≥digo (Figura 42) mostra uma amostra dos pontos classificados como outliers (ou seja, pontos com r√≥tulo -1), que n√£o foram agrupados em nenhum dos clusters do modelo DBSCAN. A tabela mostra que cada outlier possui diferentes combina√ß√µes de consumo nos diversos perfis, o que provavelmente levou √† sua classifica√ß√£o como ponto an√¥malo (ou "ru√≠do"), pois n√£o se encaixaram nos padr√µes identificados pelos clusters principais. Esses pontos podem representar comportamentos fora do comum, que devem ser investigados mais a fundo.

**Propor√ß√£o de Pontos Classificados como Ru√≠do**

&nbsp;&nbsp;&nbsp;&nbsp;Para se entender a porcentagem de anomalias em rela√ß√£o ao DataFrame reduzido, executou-se o seguinte c√≥digo:

```python
# Porcentagem de pontos classificados como ru√≠do
noise_points = (df_sample2['cluster'] == -1).sum()
total_points = len(df_sample2)
noise_percentage = (noise_points / total_points) * 100
print(f'Porcentagem de pontos classificados como ru√≠do: {noise_percentage:.2f}%')
```

&nbsp;&nbsp;&nbsp;&nbsp;Com o resultado acima, √© poss√≠vel identificar que, em m√©dia, aproximadamente 4,32% dos pontos n√£o estavam em nenhum cluster. Esse valor √© relativamente moderado, o que significa que a maior parte dos dados foi agrupada nos clusters, mas uma boa parte foi classificada como "ru√≠do" ou anomalias. Posteriormente, implementou-se outro algoritmo para se entender a totalidade de cada cluster:

```python
# Contar quantos pontos est√£o em cada cluster
cluster_counts = df_sample2['cluster'].value_counts()
print("Contagem dos pontos em cada cluster:")
print(cluster_counts)
```

&nbsp;&nbsp;&nbsp;&nbsp;A contagem de pontos por cluster mostra que o cluster 0 √© o maior, concentrando a maior parte dos dados, com um total de 12.883 pontos. J√° os clusters menores variam entre 54 e 3.547 pontos. Isso √© algo comum em problemas onde o DBSCAN agrupa a maioria dos dados em um cluster principal e distribui os demais em clusters menores, dependendo da densidade dos dados.

&nbsp;&nbsp;&nbsp;&nbsp;Em suma, entendeu-se que o modelo DBSCAN foi capaz de identificar clusters importantes de consumo, distinguindo com sucesso os principais grupos de usu√°rios e padr√µes de consumo, al√©m de detectar comportamentos mais incomuns. A an√°lise revelou uma estrutura clara, onde a maior parte dos dados foi agrupada em clusters significativos, permitindo a identifica√ß√£o de diferentes perfis de consumo. Al√©m disso, foi poss√≠vel identificar a exist√™ncia de 1021 outliers, que representam consumos fora do padr√£o esperado. Esses outliers podem indicar anomalias ou situa√ß√µes que merecem uma investiga√ß√£o mais detalhada, como erros de medi√ß√£o ou casos at√≠picos que fogem da norma. De maneira geral, o DBSCAN se mostrou eficiente para segmentar os dados e destacar √°reas cr√≠ticas, oferecendo uma base s√≥lida para a√ß√µes corretivas e otimiza√ß√µes futuras.

#### 4.4.4 OPTICS

&nbsp;&nbsp;&nbsp;&nbsp;O modelo OPTICS (Ordering Points To Identify the Clustering Structure) √© uma t√©cnica de clustering muito utilizada para identificar outliers e anomalias (Ankerst, 1999). √â similiar ao DBScan, por√©m fornece uma fexibilidade maior na detec√ß√£o de clusters com densidades variadas, algo que pode ser √∫til ao analisar dados de consumo de g√°s, uma vez que podem existir padr√µes de consumo muito variados. <br>

**Primeira execu√ß√£o do modelo OPTICS:**

&nbsp;&nbsp;&nbsp;&nbsp;A partir desta defini√ß√£o, o modelo OPTICS foi o √∫ltimo modelo n√£o-supervisionado de clusteriza√ß√£o que foi testado durante a execu√ß√£o do projeto. A princ√≠pio, o modelo foi executado sem ajuste de hiperpar√¢metros, e retornou uma visualiza√ß√£o de clusters considerada pela equipe como incorreta, uma vez que classificou mais pontos como anomalias do que com normais: 

<div align="center">
   
   <sub>Figura 43 - Primeira visualiza√ß√£o de clusters gerada pelo modelo OPTICS </sub>

   <img src="../assets/grafico_optics_1.png">

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>
  
**Ajuste fino dos hiperpar√¢metros:**

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s a primeira tentativa, foi realizado o ajuste fino dos hiperpar√¢metros, a fim de conseguir uma melhor clusteriza√ß√£o de dados. <br>
Para o modelo OPTICS, os principais hiperpar√¢metros a serem ajustados s√£o:
- min_samples: O n√∫mero m√≠nimo de pontos em uma regi√£o para que essa regi√£o seja considerada um cluster;
- xi: Par√¢metro de densidade que controla a sensibilidade do modelo em rela√ß√£o a mudan√ßas de densidade nos dados;
- min_cluster_size: Tamanho m√≠nimo de um cluster em termos da fra√ß√£o total dos pontos

&nbsp;&nbsp;&nbsp;&nbsp;Para esta abordagem, foi utilizado o m√©todo de GridSearch para encontrar a melhor combina√ß√£o de valores de hiperpar√¢metros. O GridSearch √© um algoritmo que testa todas as combina√ß√µes de hiperpar√¢metros e retorna aquela que possui melhores resultados. Ainda nesta an√°lise, foi utilizado como m√©trica de efici√™ncia do modelo e compara√ß√£o entre hiperpar√¢metros o Davies-Bouldin Score, que mede a qualidade dos clusters gerados pelo modelo baseando-se na similaridade m√©dia entre os clusters e na sua separa√ß√£o.
&nbsp;&nbsp;&nbsp;&nbsp;Ao rodar tal algoritmo de GridSearch, foi retornada uma melhor combina√ß√£o de hiperpar√¢metros com um Davies-Bouldin Score de aproximadamente 0,68. Vale ressaltar que, quanto menor for este valor, melhor. 

**Testes com hiperpar√¢metros ajustados:**

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s ajustar os hiperpar√¢metros para os melhores valores, foi poss√≠vel executar o modelo de clusteriza√ß√£o novamente e visualizar como ficou a distribui√ß√£o dos pontos nos clusters. A partir disso, foi poss√≠vel concluir que:
1. A quantidade de anomalias identificadas caiu muito, passando para apenas 0,12% do dataframe amostral (1% dos dados), o que corresponde a 36 linhas de dados. Isso reflete que o modelo considerou a maior parte dos dados como normais, e apenas uma pequena parcela como anomalia, como √© o esperado, uma vez que as anomalias s√£o exce√ß√µes. Al√©m disso, √© importante ressaltar que pontos com valor de consumo horarizado negativo foram manualmente considerados anomalias.
2. A distribui√ß√£o dos dois clusters (normal e anormal) ficou muito mais claro na segunda visualiza√ß√£o. Agora, √© poss√≠vel entender que o modelo considerou como anomalia apenas os dados que possu√≠am consumo horarizado extremamente baixo ou negativos.

<div align="center">
   
   <sub>Figura 44 - Visualiza√ß√£o de clusters gerada pelo modelo OPTICS ap√≥s ajuste</sub>

   <img src="../assets/grafico_optics_2.png"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>
  
**Conclus√£o dos resultados:**

&nbsp;&nbsp;&nbsp;&nbsp;Em conclus√£o, √© poss√≠vel perceber atrav√©s da an√°lise dos dados e clusteriza√ß√£o com o modelo OPTICS que dados que possuem consumo horarizado negativos s√£o considerados an√¥malos, algo que faz sentido no contexto do projeto atual, que analisa dados de consumo de g√°s. Al√©m disso, o modelo classificou os dados com varia√ß√£o de tempo entre medi√ß√µes muito alta como anomalia, algo que tamb√©m faz sentido de acordo com informa√ß√µes que foram passadas pela empresa parceira. Dessa forma, a utiliza√ß√£o de tal modelo aplicado a todos os dados de consumo que possu√≠mos se faz de extrema utilidade para identificar ao menos estes dois tipos de anomalia que, por mais que n√£o sejam as √∫nicas, s√£o significativas.

  
#### 4.4.5. Modelo Supervisionado Para Previs√£o de Consumo
&nbsp;&nbsp;&nbsp;&nbsp;Um modelo de aprendizagem supervisionada consegue aprender a partir de dados com resultados pr√©-definidos, ou seja, j√° rotulados. Esse tipo de modelo se faz muito √∫til quando j√° possu√≠mos dados que est√£o rotulados ou classificados e queremos classificar novos dados. No contexto do projeto, utilizaremos um modelo supervisionado em conjunto com uma base de dados sint√©tica disponibilizada pela Compass para construir um modelo preditivo que consiga dizer quantos metros c√∫bicos de g√°s um cliente ir√° consumir no pr√≥ximo m√™s (Candido, 2023). <br>
&nbsp;&nbsp;&nbsp;&nbsp;Para este modelo, utilizaremos dados sint√©ticos disponibilizados pela Compass. Estes dados s√£o mais simplificados em rela√ß√£o aos dados reais, possuindo apenas o c√≥digo do cliente, √≠ndice do cliente, data (em intervalos de 1 dia) e o consumo de tal instala√ß√£o naquele dia.


##### 4.4.5.1. Explora√ß√£o, limpeza e enriquecimento dos dados
&nbsp;&nbsp;&nbsp;&nbsp; Como os dados sint√©ticos eram muito mais simples do que os dados reais, todo o processo de pr√©-processamento foi feito de maneira mais r√°pida. Assim, foi realizada o tratamento de valores faltantes (NaN) e valores duplicados:
```python
## Calculo da m√©dia de consumo di√°rio para cada instala√ß√£o (clientCode + clientIndex)
df_sintetico['media_consumo_instalacao'] = df_sintetico.groupby(['clientCode', 'clientIndex'])['consumo_dia'].transform('mean')

## Substitui√ß√£o os valores NaN na coluna consumo_dia pela m√©dia da instala√ß√£o correspondente
df_sintetico['consumo_dia'] = df_sintetico['consumo_dia'].fillna(df_sintetico['media_consumo_instalacao'])

print(f"Restam {df_sintetico['consumo_dia'].isnull().sum()} valores NaN ap√≥s preencher com a m√©dia da instala√ß√£o.")
```

&nbsp;&nbsp;&nbsp;&nbsp;O enriquecimento do dataset √© uma medida tomada para acrescentar mais dados que podem ser √∫teis para um modelo preditivo. Um exemplo de enriquecimento foi feito durante a se√ß√£o de pr√©-processamento dos dados para encontro de anomalias, onde foi realizada uma consulta em uma API de temperatura a fim de saber a temperatura m√©dia de cada dia em cada medi√ß√£o. Para o dataset sint√©tico, o enriquecimento foi feito apenas por meio de c√°lculos com os dados j√° existentes (consumo di√°rio e dia): 
```python
def obter_estacao(data):
    """
    Fun√ß√£o para obter a esta√ß√£o do ano com base na data.
    No Brasil, as esta√ß√µes s√£o:
    - Ver√£o: 21 de dezembro a 20 de mar√ßo
    - Outono: 21 de mar√ßo a 20 de junho
    - Inverno: 21 de junho a 20 de setembro
    - Primavera: 21 de setembro a 20 de dezembro
    """
    mes = data.month
    dia = data.day
    
    if (mes == 12 and dia >= 21) or (mes == 1) or (mes == 2) or (mes == 3 and dia <= 20):
        return 'Ver√£o'
    elif (mes == 3 and dia >= 21) or (mes == 4) or (mes == 5) or (mes == 6 and dia <= 20):
        return 'Outono'
    elif (mes == 6 and dia >= 21) or (mes == 7) or (mes == 8) or (mes == 9 and dia <= 20):
        return 'Inverno'
    else: # (mes == 9 and dia >= 21) or (mes == 10) or (mes == 11) or (mes == 12 and dia <= 20)
        return 'Primavera'
```
##### 4.4.5.2. Modelo Holt-Winters
&nbsp;&nbsp;&nbsp;&nbsp;O m√©todo de Holt-Winters √© uma t√©cnica de previs√£o de s√©ries temporais que incorpora a tend√™ncia e a sazonalidade dos dados. √â um modelo que se mostra particularmente eficaz quando os dados apresentam padr√µes sazonais definidos ao longo do tempo (Tebaldi, 2019). Dessa forma, o modelo Holt-Winters se faz ideal para analisar, por exemplo, o consumo de g√°s de um cliente ao longo dos anos, uma vez que este consumo √© sazonal e aumenta ou diminui em determinadas √©pocas.

**Agrupamento de dados por m√™s e execu√ß√£o do modelo:** 

&nbsp;&nbsp;&nbsp;&nbsp;Para o projeto atual, decidimos agrupar os dados de consumo por m√™s, ou seja, ajustar o dataframe para obter o consumo mensal de cada instala√ß√£o, lembrando que uma instala√ß√£o √© uma combina√ß√£o √∫nica entre clientCode e clientIndex. Como o Holt-Winters apenas usa uma vari√°vel no tempo, as √∫nicas *features* que utilizamos foi o consumo mensal e o m√™s deste consumo. Dessa forma, rodamos um primeiro modelo:
```python
from statsmodels.tsa.holtwinters import ExponentialSmoothing

modelo_mensal = ExponentialSmoothing(
    instalacao_mensal_df['consumo_dia'], 
    trend='add',      
    seasonal='add',   
    seasonal_periods=12  ## Sazonalidade anual (12 meses)
)

## Fit do modelo
ajuste_mensal = modelo_mensal.fit()

previsao_mensal = ajuste_mensal.forecast(1)  ## Prever apenas o pr√≥ximo m√™s

print(previsao_mensal)
```

<div align="center">
   
   <sub>Figura 45 - Gr√°fico de Consumo Mensal Com Ponto de Previs√£o </sub>

   <img src="../assets/grafico_ponto_previsao.png"> 
   
   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

&nbsp;&nbsp;&nbsp;&nbsp;Para esta instala√ß√£o, inicialmente, o modelo previu um consumo de 1,45 metros c√∫bicos de g√°s no pr√≥ximo m√™s. 

**Crit√©rio para identificar previs√µes corretas:**

&nbsp;&nbsp;&nbsp;&nbsp;Como n√£o √© poss√≠vel, na maior parte dos casos, prever com 100% de certeza o valor que o cliente consumir√° no pr√≥ximo m√™s, decidimos considerar como "correta" toda previs√£o que estivesse num intervalo de 10% para baixo ou para cima do valor real. Dessa forma, foi necess√°rio separar os dados em treinamento (80%) e teste (20%). Assim, treinamos o modelo, geramos as previs√µes, e comparamos as previs√µes com os valores reais. Se o valor da previs√£o estiver neste intervalo de 10%, consideramos correta a previs√£o. 

<div align="center">
   
   <sub>Figura 46 - Gr√°fico de Previs√£o de Consumo em Compara√ß√£o com Dados Reais </sub>

   <img src="../assets/grafico_previsao_consumo.png"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>
  
&nbsp;&nbsp;&nbsp;&nbsp;Nesta tentativa, o modelo previu corretamente cerca de 77% dos valores corretamente, no intervalo de confian√ßa de 10%.

**C√°lculo das m√©tricas de erro:**

&nbsp;&nbsp;&nbsp;&nbsp;Como principais tr√™s m√©tricas para avaliar o modelo, escolhemos o Erro M√©dio Absoluto, o Erro M√©dio Quadr√°tico e o Erro M√©dio Percentual. Cada uma dessas m√©tricas tem como objetivo avaliar o quanto o modelo erra as previs√µes. <br>
&nbsp;&nbsp;&nbsp;&nbsp;O erro absoluto m√©dio calcula a m√©dia das diferen√ßas absolutas entre os valores reais e os valores que foram previstos pelo modelo. Um erro m√©dio absoluto de 1, por exemplo, quer dizer que, quando o modelo erra, ele erra por cerca de 1 unidade (metro c√∫bico, no nosso caso). J√° o erro m√©dio quadr√°tico calcula a mesma m√©dia que a m√©trica anterior, por√©m elevando os erros ao quadrado, o que resulta em uma penaliza√ß√£o maior para erros maiores. Ademais, o erro m√©dio percentual mede a m√©dia dos erros absolutos divididos pelos valores reais, multiplicados por 100%. Por exemplo, um erro percentual de 0.05 quer dizer que o modelo tem um erro de 5% em rela√ß√£o aos valores reais. <br>
&nbsp;&nbsp;&nbsp;&nbsp;Por fim, o c√°lculo dessas m√©tricas √© de extrema import√¢ncia para avaliar o modelo. Uma vez que o ideal √© que todas essas m√©tricas sejam as menores poss√≠veis, podemos ajustar o modelo e verificar se ele melhorou ou piorou com base nos valores dessas m√©tricas. 

**Ajuste de hiperpar√¢metros do modelo:**

&nbsp;&nbsp;&nbsp;&nbsp;Os hiperpar√¢metros de um modelo s√£o os valores ou configura√ß√µes que passamos para um modelo de maneira manual. Tais hiperpar√¢metros causam um impacto significante no desempenho do modelo, e s√£o respons√°veis por controlar o processo de treinamento e definir a estrutura do modelo. Por conta disso, √© muito importante encontrar uma combina√ß√£o ideal de hiperpar√¢metros para que o modelo performe da melhor maneira poss√≠vel.

Para o modelo Holt-Winters, foram identificados 4 hiperpar√¢metros a serem ajustados:

* trend: Define o tipo de componente de tend√™ncia que ser√° utilizado nos c√°lculos do modelo. Sendo do tipo 'add', o modelo utilizar√° uma tend√™ncia aditiva, onde a tend√™ncia √© somada √† s√©rie temporal. Sendo do tipo 'mul', o modelo utilizar√° uma tend√™ncia multiplicativa, sendo mais adequado quando a taxa de crescimento √© proporcional ao n√≠vel da s√©rie temporal. Tendo este par√¢metro como 'None', temos que o modelo n√£o inclui componente de tend√™ncia.
* seasonal: Define o tipo de componente sazonal que o modelo utilizar√° em seus c√°lculos, capturando padr√µes repetitivos intervalos de tempo, como, por exemplo, um consumo de g√°s que aumenta em meses de f√©rias escolares. Os valores para esse par√¢metro s√£o os mesmos do par√¢metro 'trend'.
* seasonal_periods: Define o n√∫mero de per√≠odos que um ciclo sazonal completo deve ter. Para um ciclo de um ano, por exemplo, o valor deveria ser de 12.
* damped_trend: Define se a tend√™ncia deve ser amortecida ao longo do tempo. Se verdadeiro, aplica o amortecimento, fazendo com que a tend√™ncia seja menos pronunciada com o passar do tempo. Se falso, n√£o aplica amortecimento e a tend√™ncia pode continuar indefinidamente.

```python
## Par√¢metros a serem testados
trend_options = ['add', 'mul', None]
seasonal_options = ['add', 'mul', None]
seasonal_periods = [i for i in range(2, 31)]  
damped_trend_options = [True, False]
```

**Grid Search Manual para encontrar melhores par√¢metros:**

&nbsp;&nbsp;&nbsp;&nbsp;O algoritmo de Grid Search √© utilizado na an√°lise de dados para encontrar a melhor combina√ß√£o poss√≠vel de hiperpar√¢metros para um modelo. Acima, definimos a nossa "grade" de par√¢metros, e, abaixo, iremos testar todas essas combina√ß√µes e calcular as m√©tricas de erro para cada uma. Vale ressaltar que o algoritmo de Grid Search possui um custo computacional alto, e s√≥ √© vi√°vel utiliz√°-lo aqui porque a quantidade de par√¢metros √© pequena (Constantin, 2024). Dessa forma, rodamos o seguinte algoritmo, que nos retornou a melhor combina√ß√£o de hiperpar√¢metros para o modelo, ou seja, aquela que minimizou os erros:

```python
for trend in trend_options:
    for seasonal in seasonal_options:
        for period in seasonal_periods:
            for damped in damped_trend_options:
                try:
                    ## Apenas testar damped_trend se houver uma tend√™ncia
                    if trend is None and damped:
                        continue
                    
                    ## Definir e ajustar o modelo Holt-Winters com os dados de treino
                    modelo_treino = ExponentialSmoothing(
                        train['consumo_dia'], 
                        trend=trend, 
                        seasonal=seasonal,
                        seasonal_periods=period,
                        damped_trend=damped
                    )

                    ajuste_treino = modelo_treino.fit()

                    ## Fazer a previs√£o para o n√∫mero de meses no conjunto de teste
                    previsao_teste = ajuste_treino.forecast(len(test))

                    ## Calcular as m√©tricas
                    mae = mean_absolute_error(test['consumo_dia'], previsao_teste)
                    mse = mean_squared_error(test['consumo_dia'], previsao_teste)
                    mape = mean_absolute_percentage_error(test['consumo_dia'], previsao_teste)

                    ## Comparar o erro e armazenar o melhor modelo (baseado no MAE)
                    if mae < best_mae:
                        best_mae = mae
                        best_mse = mse
                        best_mape = mape
                        best_params = {
                            'trend': trend,
                            'seasonal': seasonal,
                            'seasonal_periods': period,
                            'damped_trend': damped
                        }

                except Exception as e:
                    print(f"Erro com a configura√ß√£o {trend}, {seasonal}, {period}, damped={damped}: {e}")
```

**Modelagem com hiperpar√¢metros ajustados:** 

&nbsp;&nbsp;&nbsp;&nbsp;A partir dos hiperpar√¢metros ajustados, o modelo foi executado novamente:

```python
modelo_treino = ExponentialSmoothing(
    train['consumo_dia'], 
    trend=best_params['trend'],      
    seasonal=best_params['seasonal'],   
    seasonal_periods=best_params['seasonal_periods'],
    damped_trend=best_params['damped_trend']  
)

ajuste_treino = modelo_treino.fit()

# Fazer a previs√£o para o n√∫mero de meses no conjunto de teste
previsao_teste = ajuste_treino.forecast(len(test))

# Verificar as previs√µes
print(previsao_teste)
```

&nbsp;&nbsp;&nbsp;&nbsp;Este novo modelo contou com m√©tricas de erro consideravelmente reduzidas e com precis√£o, no n√≠vel de 10% de confian√ßa, de cerca de 89%. 

<div align="center">
   
   <sub>Figura 47 - Gr√°fico de Previs√£o de Consumo em Compara√ß√£o com Dados Reais Ap√≥s Ajustes </sub>

   <img src="../assets/grafico_previsao_consumo_ajustado.png"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

**Finaliza√ß√£o do modelo para previs√£o de consumo:**

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, esta se√ß√£o implementou completamente um modelo preditivo √† parte de todo o escopo inicial (de encontrar anomalias em dados reais). Foi feito o pr√©-processamento dos dados sint√©ticos entregues pela empresa parceira, ajuste em dados de treino e teste, treinamento de modelo, ajuste de hiperpar√¢metros e valida√ß√£o de resultados a partir de m√©tricas que calculam a taxa de erro do modelo. De acordo com a pr√≥pria empresa parceira, uma previs√£o de consumo de g√°s para um cliente agrega muito valor √†s opera√ß√µes da Compass, uma vez que esta previs√£o pode estar presente, por exemplo, na conta de g√°s de determinado cliente, que j√° poder√° estar ciente de quanto ele ter√° que pagar, em m√©dia, pelo g√°s no m√™s que vem.

#### 4.4.6. Compara√ß√£o final entre os modelos

&nbsp;&nbsp;&nbsp;&nbsp;Esta se√ß√£o tem como finalidade comparar todos os resultados dos modelos n√£o-supervisionados desenvolvidos durante a se√ß√£o 4.4. Tal compara√ß√£o tem como objetivo mostrar quais modelos performaram melhor a atividade de encontrar anomalias, para que, no futuro, a equipe possa escolher apenas um modelo final para performar o trabalho de encontrar as anomalias nos dados de consumo de g√°s da Compass.

&nbsp;&nbsp;&nbsp;&nbsp;Durante toda a sprint 4 do projeto, foram testados 4 modelos n√£o-supervisionados diferentes, todos com o mesmo objetivo de identificar anomalias no consumo de g√°s. Para realizar a compara√ß√£o entre todos esses modelos, foram escolhidas 3 principais m√©tricas, que, no geral, s√£o respons√°veis por metrificar o qu√£o bem o modelo definiu os clusters:

* ```Davies-Bouldin Index```: mede a rela√ß√£o intra-cluster e a dist√¢ncia inter-cluster. Quanto menor, melhor;
* ```Calinski-Harabasz Index```: mede a separa√ß√£o dos clusters com base na dispers√£o intra e inter-cluster. Quanto maior, melhor;
* ```Coeficiente de Silhueta```: mede o qu√£o bem um ponto de dado se encaixa em seu pr√≥prio cluster em compara√ß√£o com outros clusters. Quanto maior, melhor;

**M√©tricas do DBScan:**

&nbsp;&nbsp;&nbsp;&nbsp;O modelo DBScan conseguiu as seguintes m√©tricas:

- ```Melhor Silhouette Score```: 0.7482794489545446
- ```Melhor Davies-Bouldin Index (DBI)```: 1.7877124152854893
- ```Melhor Calinski-Harabasz Index (CH Index)```: 1383.0505542992075

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, a visualiza√ß√£o dos clusters gerados pelo modelo foi a seguinte:

<div align="center">
   
   <sub>Figura 48 - Visualiza√ß√£o da clusteriza√ß√£o produzida pelo DBScan </sub>

   <img src="../assets/grafico_dbscan.jpeg"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

**M√©tricas do Isolation Forest:**

&nbsp;&nbsp;&nbsp;&nbsp;O modelo Isolation Forest conseguiu as seguintes m√©tricas:

* ```Melhor Silhouette Score (Com pseudoclusteriza√ß√£o)```: 0.8526
* ```Melhor Davies-Bouldin Index (DBI)```: 2.5454
* ```Melhor Calinski-Harabasz Index (CH Index)```: 10685.4068
    
&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, a visualiza√ß√£o dos resultados gerados pelo modelo foi a seguinte:

<div align="center">
   
   <sub>Figura 49 - Visualiza√ß√£o dos resultados do modelo Isolation Forest</sub>

   <img src="../assets/grafico_isolation.jpeg"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

**M√©tricas do One Class SVM:**

&nbsp;&nbsp;&nbsp;&nbsp;O modelo One Class SVM conseguiu as seguintes m√©tricas:

* ```Melhor Silhouette Score```: 0.8923534243010977
* ```Melhor Davies-Bouldin Index (DBI)```: 2.7424308335681506
* ```Melhor Calinski-Harabasz Index (CH Index)```: 1890.2268179584257

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, a visualiza√ß√£o dos resultados gerados pelo modelo foi a seguinte:

<div align="center">
   
   <sub>Figura 50 - Visualiza√ß√£o dos resultados do modelo One-Class SVM</sub>

   <img src="../assets/grafico_svm_oneclass.jpeg"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

**M√©tricas do OPTICS:**

&nbsp;&nbsp;&nbsp;&nbsp;O modelo OPTICS conseguiu as seguintes m√©tricas:

* ```Melhor Silhouette Score```: 0.9736186895434548
* ```Melhor Davies-Bouldin Index (DBI)```: 0.6797220500482788
* ```Melhor Calinski-Harabasz Index (CH Index)```: 12534.881388176493

&nbsp;&nbsp;&nbsp;&nbsp;Al√©m disso, a visualiza√ß√£o dos clusters gerados pelo modelo foi a seguinte:

<div align="center">
   
   <sub>Figura 51 - Visualiza√ß√£o dos resultados do modelo One-Class SVM</sub>

   <img src="../assets/grafico_optics_notebook.png"> 

   <sup>Fonte: Material produzido pelos autores (2024)</sup>
   
</div>

**Tabela de m√©tricas:**

<div align="center">

<sub>Tabela 4 - M√©tricas dos modelos </sub>

</div>

<div align="center">


| Modelo              | Silhouette Score | Davies-Bouldin Index (DBI) | Calinski-Harabasz Index (CH Index) |
|---------------------|------------------|----------------------------|------------------------------------|
| DBScan              | 0.7483           | 1.7877                     | 1383.0506                          |
| Isolation Forest 1  | 0.8526           | 2.5454                     | 10685.4068                         |
| SVM One-Class       | 0.8924           | 2.7424                     | 1890.2268                          |
| OPTICS              | 0.9736           | 0.6797                     | 12534.8814                         |

</div>

<div align="center">

<sup>Fonte: Material produzido pelos autores (2024)</sup>
</div>


**Discuss√£o dos resultados:**

&nbsp;&nbsp;&nbsp;&nbsp;A partir dos √≠ndices calculados e das visualiza√ß√µes dos resultados de cada modelo, foi poss√≠vel chegar a algumas conclus√µes:

* O modelo OPTICS foi o que apresentou as melhores m√©tricas, possuindo o maior coeficiente de silhueta, menor √≠ndice Davies-Bouldin e maior √≠ndice Calinski-Harabasz. Al√©m disso, ficou claro durante a interpreta√ß√£o gr√°fica dos resultados que o modelo considerou an√¥malos aqueles dados que possu√≠am uma varia√ß√£o de consumo negativa ou que tinham uma varia√ß√£o de tempo entre medi√ß√µes muito elevada.
* O modelo Isolation Forest, em quest√µes de m√©tricas, teve o segundo melhor desempenho, possuindo um alto valor de coeficiente de silhueta e de √≠ndice Calinski-Harabasz, o modelo de Isolation Forest tamb√©m foi capaz de realizar uma distin√ß√£o entre dados normais e an√¥malos semelhante ao modelo OPTICS, identificando como anomalias dados com grande varia√ß√£o de consumo e com grande varia√ß√£o de tempo entre medi√ß√µes.
* O modelo SVM One-Class tamb√©m contou com m√©tricas relativamente boas, contando com um coeficiente de silhueta de cerca de 89%. Al√©m disso, em rela√ß√£o a visualiza√ß√£o dos resultados, o modelo foi capaz de classificar os dados que possuem um consumo total em metros c√∫bicos muito elevado por√©m com baixa varia√ß√£o de consumo como anormais. Al√©m disso, ele tamb√©m classificou como anomalia os dados com varia√ß√£o de consumo extremamente alta.
* O modelo DBScan contou com a segunda melhor m√©trica para o √≠ndice Davies-Bouldin e valores decentes para o coeficiente de de silhueta e √≠ndice Calinski-Harabasz. Al√©m disso, olhando para a visualiza√ß√£o gr√°fica dos resultados, tem se que o modelo gerou uma separa√ß√£o dos dados em 11 clusters diferentes, sendo um deles anormal.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, a partir dos resultados obtidos, foi decidido que os modelos com maior potencial para identificar as anomalias nos dados de consumo de g√°s da Compass s√£o o OPTICS e o Isolation Forest, sendo que o segundo √© mais espec√≠fico e mais utilizado no mundo de an√°lise de dados para identificar anomalias e outliers. Dessa forma, a sequ√™ncia do trabalho no projeto deve estar focalizada no desenvolvimento de um desses dois modelos.

### 4.5. Avalia√ß√£o

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s os procedimentos realizados desde o in√≠cio do projeto - o entendimento do neg√≥cio na Sprint 1, o pr√©-processamento de dados na Sprint 2, a modelagem de dados na Sprint 3 e a compara√ß√£o de modelos na Sprint 4 -, o modelo Isolation Forest foi escolhido como o modelo final para entrega do projeto √†s empresas *Compass* e *Iolit*.

#### 4.5.1 Descri√ß√£o da solu√ß√£o final

&nbsp;&nbsp;&nbsp;&nbsp;A solu√ß√£o final desenvolvida pelo grupo Galv√£o & Associados √© o M√©todo Galv√£o, uma ferramenta de an√°lise e visualiza√ß√£o de dados com intelig√™ncia artificial que conta com 2 features: um modelo preditivo para detec√ß√£o de anomalias nos dados de consumo de g√°s natural; e um modelo preditivo para prever o consumo futuro de g√°s natural. Ambos os modelos j√° podem ser utilizados para verificar quais clientes possuem dados an√¥malos ou para verificar quanto um cliente vai consumir nos pr√≥ximos meses, funcionalidades acessadas de forma facilitada e visualmente simplificada por meio de uma dashboard interativa e ajustada automaticamente ao conjunto de dados.

#### 4.5.2 Justificativa da escolha do modelo final

&nbsp;&nbsp;&nbsp;&nbsp;Ao in√≠cio do projeto, foram levantadas as maiores dores e problemas que as empresas parceiras enfrentam, sendo eles:

- A dificuldade de encontrar anomalias nos dados de forma generalizada, r√°pida e autom√°tica;
- A interpreta√ß√£o dos dados coletados;

&nbsp;&nbsp;&nbsp;&nbsp;Para isso, foram desenvolvidas 2 personas que identificam os dois principais p√∫blicos que podem melhor se beneficiar com o uso do modelo proposto: Thiago, um cientista de dados; e Thalyta, uma *product manager*. Para que o modelo trouxesse valor para ambos os p√∫blicos, ele precisaria facilitar a rotina de tratamento e gera√ß√£o de predi√ß√µes para pessoas que trabalham em equipes de tecnologia e dados da *Compass* ao mesmo tempo em que gera insights de forma clara e de f√°cil entendimento √†queles que trabalham em √°reas de neg√≥cios e n√£o necessariamente possuem o conhecimento t√©cnico e tecnol√≥gico para utilizarem diretamente o modelo preditivo.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, o modelo final Isolation Forest - ap√≥s passar pelas etapas de pr√©-processamento dos dados, desenvolvimento do modelo, ajuste fino de hiperpar√¢metros e avalia√ß√£o de desempenho - evidenciou a boa e melhor performance de todas na detec√ß√£o de anomalias no consumo de g√°s nos dados compartilhados pelas empresas parceiras. Al√©m disso, o modelo, ap√≥s ser exportado, p√¥de ser reutilizado facilmente com apenas 2 linhas de c√≥digo para a importa√ß√£o, o que mostrou o benef√≠cio de facilitar a rotina de equipes de tecnologia sem que muito tempo seja perdido no desenvolvimento e testes de diversos modelos. Tamb√©m, ap√≥s o desenvolvimento da dashboard interativa, foi poss√≠vel utilizar o modelo para gerar predi√ß√µes e gr√°ficos de forma r√°pida e did√°tica, sem que fosse necess√°rio utilizar c√≥digos e programa√ß√£o - uma maneira melhor de enxergar os dados para as pessoas que n√£o possuem conhecimento t√©cnico aprofundado.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, o modelo final desenvolvido pela equipe tem o potencial para agregar valor e comodidade √† experi√™ncia de equipes de neg√≥cios e tecnologia, sendo uma ferramenta que permite a utiliza√ß√£o livre e das maneiras que mais sejam necessitadas pelo usu√°rio, uma flexibilidade necess√°ria no contexto das empresas *Compass* e *Iolit*.

#### 4.5.3 Plano de conting√™ncia

&nbsp;&nbsp;&nbsp;&nbsp;Um plano de conting√™ncia √© uma a√ß√£o e ferramenta da gest√£o de riscos que auxilia organiza√ß√µes e equipes a terem a√ß√µes pr√°ticas e concretas em face de poss√≠veis riscos ou falhas que possam ocorrer e impactar o andamento normal de suas atividades. No caso desse projeto, o plano de conting√™ncia abordar√° poss√≠veis falhas ou cen√°rios indesej√°veis do modelo preditivo e a√ß√µes que podem ser feitas para reverter ou tratar os problemas.

&nbsp;&nbsp;&nbsp;&nbsp;Para a delimita√ß√£o do plano de conting√™ncia, √© necess√°rio elencar os poss√≠veis riscos. Desse modo, foram escolhidos 3 riscos que podem impedir a utiliza√ß√£o do modelo preditivo:

1. Erros de importa√ß√£o de bibliotecas;
2. Conjuntos de dados sem pr√©-processamento;
3. Dashboard sem funcionamento;
4. Falta de detec√ß√£o de anomalias;
5. Dados de consumo negativo classificados como consumo normal.

&nbsp;&nbsp;&nbsp;&nbsp;Ap√≥s elencar os riscos ou erros, faz-se necess√°rio desenvolver planos de a√ß√£o para contornar os mesmos e permitir a continua√ß√£o do uso da solu√ß√£o.

**Plano de a√ß√£o 1: Importa√ß√£o de bibliotecas**

&nbsp;&nbsp;&nbsp;&nbsp;O erro por importa√ß√£o de bibliotecas ocorre quando todas as bibliotecas necess√°rias para que o modelo preditivo seja executado n√£o est√£o presentes no notebook. A partir disso, √© poss√≠vel tra√ßar duas a√ß√µes, tendo como base a utiliza√ß√£o do modelo em arquivo e do modelo em c√≥digo:

- Quando o modelo est√° em um arquivo do tipo .joblib ou .pkl, √© necess√°rio importar a biblioteca utilizada na identifica√ß√£o da extens√£o do arquivo - ou seja, *joblib ou pickle*.
- Quando o modelo est√° sendo executado em c√©lulas de c√≥digo, √© necess√°rio certificar que todas as bibliotecas utilizadas para o desenvolvimento do modelo j√° foram importadas. No notebook do projeto, todas as bibliotecas foram importadas na primeira c√©lula, ent√£o √© necess√°rio executar ela antes de executar todo o c√≥digo em seguida.

**Plano de a√ß√£o 2: Pr√©-processamento dos dados**

&nbsp;&nbsp;&nbsp;&nbsp;O modelo preditivo atual possui 11 vari√°veis que utiliza para o treino e o teste com o conjunto de dados. Logo, conjuntos de dados que possuem vari√°veis diferentes ou nomeadas de forma diferente n√£o poder√£o ser utilizadas para gerar predi√ß√µes. Para isso:

- Conjuntos de dados que n√£o possuem as mesmas vari√°veis que s√£o utilizadas pelo modelo preditivo precisar√£o passar pelo pr√©-processamento de dados de acordo com o processo realizado no notebook deste projeto;
- Conjuntos de dados que possuem as mesmas vari√°veis, mas com nomes diferentes, precisar√£o ter suas vari√°veis renomeadas para estar em conformidade com o esperado pelo modelo.

**Plano de a√ß√£o 3: Deploy do dashboard**

&nbsp;&nbsp;&nbsp;&nbsp;A utiliza√ß√£o do dashboard, atualmente, funciona por meio de uma hospedagem realizada para demonstra√ß√£o. Isso significa que, em tempo indeterminado ap√≥s o t√©rmino do projeto, o dashboard poder√° n√£o funcionar mais. Para isso:

- √â necess√°rio que seja feita uma nova hospedagem do dashboard desenvolvido com *Streamlit* para que a utiliza√ß√£o do mesmo continue vi√°vel e dispon√≠vel para todos.

**Plano de a√ß√£o 4: Detec√ß√£o de anomalias**

&nbsp;&nbsp;&nbsp;&nbsp;Tempos ap√≥s o fim do treinamento e teste do modelo, apesar de boa performance, o mesmo pode come√ßar a apresentar um desempenho mediano ou ruim. Esse comportamento pode acontecer pela diferen√ßa de densidade entre classes que pode ocorrer com diferentes conjuntos de dados ou mudan√ßas espec√≠ficas de cada dataframe. Para isso:

- √â poss√≠vel realizar o retreino do modelo periodicamente, em busca de manter um bom score nas m√©tricas de avalia√ß√£o de desempenho.

**Plano de a√ß√£o 5: Detec√ß√£o de consumo negativo como anomalia**

&nbsp;&nbsp;&nbsp;&nbsp;Os dados de consumo de g√°s com valores negativos s√£o erros no registro efetuado pelo medidor e devem ser considerados como anomalias. Entretanto, o modelo pode detectar consumos negativos como dados normais, gerando previs√µes erradas. Para isso:

- √â poss√≠vel classificar manualmente os dados de consumo negativo como anomalias depois da previs√£o para que haja a uniformidade dos dados.

&nbsp;&nbsp;&nbsp;&nbsp;Dessa forma, ap√≥s estruturar um plano de conting√™ncia que cobre poss√≠veis erros e que cont√©m planos de a√ß√£o para cada um dos riscos, √© esperado que a gest√£o de riscos a partir do uso do modelo preditivo seja facilitada e que as atividades, caso um dos riscos ocorra, voltem √† normalidade com uma velocidade ainda maior.

#### 4.5.4 Verifica√ß√£o de aceita√ß√£o ou refuta√ß√£o de hip√≥teses

&nbsp;&nbsp;&nbsp;&nbsp;Ao longo das 10 semanas do projeto, foram feitas diversas hip√≥teses, mas 3 foram escolhidas para serem exploradas - as quais podem ser encontradas na se√ß√£o 4.2.3:

- Varia√ß√£o de consumo de acordo com m√™s do ano;
- Diferen√ßa entre n√∫meros de instala√ß√µes residenciais familiares e outras;
- Diferen√ßa de consumo para perfis de consumo que possuam caldeiras.

&nbsp;&nbsp;&nbsp;&nbsp;A se√ß√£o 4.2.3, desenvolvida durante a Sprint 2, al√©m do desenvolvimento das hip√≥teses, tamb√©m foi realizada a verifica√ß√£o de aceita√ß√£o ou refuta√ß√£o das mesmas. Em suma, os resultados obtidos foram:

- A primeira hip√≥tese, relacionada √† varia√ß√£o de consumo de acordo com m√™s do ano, n√£o √© verdadeira e n√£o pode ser aceita.
- A segunda hip√≥tese, relacionada √† diferen√ßa entre n√∫meros de instala√ß√µes residenciais familiares e outras, √© verdadeira e pode ser aceita.
- A terceira hip√≥tese, relacionada √† diferen√ßa de consumo para perfis de consumo que possuam caldeiras, √© verdadeira e pode ser aceita.

&nbsp;&nbsp;&nbsp;&nbsp;Portanto, √© poss√≠vel perceber que, com a continua√ß√£o do desenvolvimento dos modelos preditivos ao longo de mais 6 semanas ap√≥s a formula√ß√£o ads hip√≥teses iniciais, os resultados se mantiveram os mesmos, expressando a consist√™ncia dos dados ap√≥s os tratamentos e mudan√ßas.


## <a name="c5"></a>5. Conclus√µes e Recomenda√ß√µes

&nbsp;&nbsp;&nbsp;&nbsp;Em conclus√£o a todo o projeto, foi poss√≠vel obter como resultado um modelo Isolation Forest treinado especificamente para os dados fornecidos pela Compass. Este modelo √© capaz de, em tempo h√°bil, encontrar instala√ß√µes de g√°s que possuem consumo considerado anormal, levando em considera√ß√£o principalmente o tempo que se passou entre uma medi√ß√£o e outra e o quanto o cliente consumiu nesse tempo. A partir disso, o modelo identifica exatamente aquelas instala√ß√µes com consumo anormal e tamb√©m j√° as lista com ordem de prioridade a serem resolvidas. Al√©m disso, tamb√©m foi desenvolvido um modelo para prever o consumo de g√°s de um determinado cliente em meses futuros e uma dashboard interativa  para visualiza√ß√£o simplificada de tudo isso. 

&nbsp;&nbsp;&nbsp;&nbsp;Em rela√ß√£o ao modelo de previs√£o de consumo, utilizamos o modelo estat√≠stico Holt-Winters, que consegue fazer previs√µes com base em dados de s√©ries temporais, levando em considera√ß√£o tend√™ncias e sazonalidade. Identificamos dois casos principais onde este modelo pode ser utilizado pela Compass: prever o consumo de um cliente e adicionar esta informa√ß√£o, por exemplo, em uma conta de g√°s, ou conseguir prever o quanto de g√°s que toda a base de clientes residenciais, por exemplo, ir√° consumir, e utilizar esse valor como base para comprar o g√°s natural da Petrobr√°s. Assim, utilizamos os dados sint√©ticos fornecidos para a empresa para treinar o modelo com as varia√ß√µes individuais de cada cliente e conseguimos entregar, atrav√©s da dashboard, a possibilidade de simplesmente escolher um cliente e quantos meses se quer prever no futuro. De acordo com m√©tricas de erro como MSE, MAE e MSPE, o modelo atingiu resultados significativamente interessantes.

&nbsp;&nbsp;&nbsp;&nbsp;Sobre a dashboard, esta foi desenvolvida utilizando o aux√≠lio da biblioteca Streamlit, em Python, que permite a cria√ß√£o de *data apps* de maneira simplificada. Na dashboard, √© poss√≠vel, em apenas 3 cliques, subir os arquivos de dados em formato csv e, ent√£o, obter as instala√ß√µes que possuem consumo anormal. Al√©m disso, a dashboard conta com dados embutidos para realizar a previs√£o de consumo dos clientes. Entretanto, mesmo com os dados embutidos, √© poss√≠vel subir dados novos. Ainda em rela√ß√£o √† dashboard, √© necess√°rio explicitar que o pr√©-processamento dos dados n√£o √© feito l√°. Dessa forma, os dados colocados l√° j√° precisam estar limpos e tratados.  

&nbsp;&nbsp;&nbsp;&nbsp;Como recomenda√ß√µes do grupo para a Compass, sugerimos que o modelo seja testado e utilizado com a base completa de clientes, comparando os resultados com os modelos que a empresa j√° utiliza. Al√©m disso, recomendamos que a previs√£o de consumo seja testada com dados reais e em grande volume, uma vez que n√£o conseguimos fazer isso por conta da quantidade de dados. Por fim, a principal recomenda√ß√£o relacionada √† utiliza√ß√£o do modelo para encontrar anomalias √© de que, ao obter, na dashboard, uma tabela com as instala√ß√µes com anomalias, a prioridade em verificar as instala√ß√µes deve estar nas instala√ß√µes que aparecem primeiro. Com todo este trabalho, esperamos que os principais afetados pelo produto sejam tanto as pessoas dos times de dados e neg√≥cios que trabalham na Compass, mas tamb√©m os clientes da empresa. Os primeiros ser√£o impactados por meio da detec√ß√£o de anomalias e visualiza√ß√£o dos gr√°ficos, que podem trazer ideias valiosas para a equipe. J√° os clientes podem ser impactados por meio da previs√£o de consumo, uma vez que a Compass pode oferecer servi√ßos personalizados para clientes com previs√£o de consumo diferentes. 

&nbsp;&nbsp;&nbsp;&nbsp;Por fim, deixamos aqui um anexo final que fala um pouco mais sobre a dashboard desenvolvida no projeto:
[dashboard](extras/dashboard)

## <a name="c6"></a>6. Refer√™ncias

COMPASS. Central de resultados. Dispon√≠vel em: https://www.compassbr.com/divulgacao-e-resultados/central-de-resultados/. Acesso em: 12 ago. 2024.

COMPASS. Estrat√©gia de sustentabilidade. Dispon√≠vel em: https://www.compassbr.com/sustentabilidade/premios-e-reconhecimentos/. Acesso em: 12 ago. 2024.

COMPASS. Quem somos. Dispon√≠vel em: https://www.compassbr.com/sobre-a-compass/quem-somos/. Acesso em: 12 ago. 2024.

DE OLIVEIRA, F. R. et al. Clusteriza√ß√£o de clientes: um modelo utilizando vari√°veis categ√≥ricas e num√©ricas. 2020. Acesso em: 25 ago. 2024.

Dicion√°rio Escolar da L√≠ngua Portuguesa. 1 ed. Barueri, SP: Ciranda Cultural, 2015. Acesso em: 26 ago. 2024.

Enciclop√©dia. Significado de Hip√≥tese. Dispon√≠vel em: https://www.significados.com.br/hipotese/. Acesso em: 26 ago. 2024.

GABRIEL. Panorama sobre a Utiliza√ß√£o das T√©cnicas e Ferramentas de Gerenciamento de Riscos em Projetos. Boletim do Gerenciamento, v. 21, n. 21, p. 23‚Äì31, 30 dez. 2020. Acesso em: 13 ago. 2024.

GUSHIKEN, A. Value Proposition Canvas: o que √© e como funciona essa metodologia. G4 Educa√ß√£o, 2023. Dispon√≠vel em: https://g4educacao.com/portal/value-proposition-canvas. Acesso em: 13 ago. 2024.

LAUBHEIMER, P. 3 Persona Types: Lightweight, Qualitative, and Statistical. Dispon√≠vel em: https://www.nngroup.com/articles/persona-types/. Acesso em: 13 ago. 2024.

MIAN, S. H. et al. Adapting Universities for Sustainability Education in Industry 4.0: Channel of Challenges and Opportunities. Sustainability, v. 12, n. 15, p. 6100, 2020. Acesso em: 13 ago. 2024.

PINHEIRO, N. Pr√©-processamento de dados com Python. Dispon√≠vel em: https://medium.com/data-hackers/pr%C3%A9-processamento-de-dados-com-python-53b95bcf5ff4. Acesso em: 22 ago. 2024.

PORTER, M. Estrat√©gia competitiva: t√©cnicas e an√°lise de ind√∫stria e da concorr√™ncia. Rio de Janeiro: [s.n.], 1986. Acesso em: 13 ago. 2024.

REHKOPF, M. User stories with examples and a template. Atlassian. Dispon√≠vel em: https://www.atlassian.com/agile/project-management/user-stories. Acesso em: 13 ago. 2024.

REIS, H. L. S. et al. G√°s natural. Recursos minerais de Minas Gerais. Companhia de Desenvolvimento de Minas Gerais (CODEMGE), Belo Horizonte, p. 1-39, 2018. Acesso em: 13 ago. 2024.

TEAM, M. J. V. Jornada do usu√°rio: o que √©, para que serve e como criar. MJV Technology & Innovation, 4 ago. 2022. Dispon√≠vel em: https://www.mjvinnovation.com/pt-br/blog/jornada-do-usuario-o-que-e/. Acesso em: 26 ago. 2024.

ROBERTO, C. Crisp-DM: as 6 etapas da metodologia do futuro. Dispon√≠vel em: <https://blog.mbauspesalq.com/2022/04/12/crisp-dm-as-6-etapas-da-metodologia-do-futuro/>. Acesso em: 02. set. 2024

‚ÄåPORTO FILHO, Carlos Humberto. T√©cnicas de aprendizado n√£o supervisionado baseadas no algoritmo da caminhada do turista. 2017. Tese de Doutorado. Universidade de S√£o Paulo. Dispon√≠vel em: https://www.teses.usp.br/teses/disponiveis/82/82131/tde-20082018-122603/publico/Dissert_CarlosPortoFilho_corrigida.pdf. Acesso em: 06 set. 2024.

PARRA, T. M√©todo Silhouette para valida√ß√£o do algoritmo de K-Means. Dispon√≠vel em <https://www.linkedin.com/pulse/silhouette-score-para-avaliar-qualidade-dos-clusters-algoritmo-parra/>. Acesso em: 10 set. 2024.

G√©ron, Aur√©lien. "Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow." O'Reilly Media, 2019. Dispon√≠vel em: https://books.google.com.br/books?id=HHetDwAAQBAJ&printsec=frontcover#v=onepage&q&f=false. Acesso em: 11. set. 2024

SCALDELAI, Dirceu; DOS SANTOS, Solange R.; MATIOLI, Luiz C. √çndice de Densidade da Clusteriza√ß√£o: Uma Nova M√©trica para Valida√ß√£o Interna de Agrupamentos. Proceeding Series of the Brazilian Society of Computational and Applied Mathematics, v. 9, n. 1, 2022.

AHMED, Mohiuddin; SERAJ, Raihan; ISLAM, Syed Mohammed Shamsul. The k-means algorithm: A comprehensive survey and performance evaluation. Electronics, v. 9, n. 8, p. 1295, 2020.

BUENO, Luƒ±s Felipe et al. Uma combina√ß ao dos algoritmos Isolation Forest e K-Means aplicadaas Elei√ßoes Brasileiras. 2021.

B214a Bando, Isabela Hara. Algoritmos one-class para fluxos cont√≠nuos de dados e a detec√ß√£o de ataques em internet das coisas / Isabela Hara Bando. - Londrina, 2024. 60 f. : il. https://sites.uel.br/dc/wp-content/uploads/2024/06/TCC_ISABELA_BANDO.pdf

CORTES, Omar Andres Carmona; MELO, Wesley Eduardo de Oliveira. Utilizando an√°lise de sentimentos e SVM na classifica√ß√£o de tweets depressivos. Cadernos de Observa√ß√£o Tecnol√≥gica em Sistemas de Informa√ß√£o, v. 12, p. 102-110, 29 abr. 2021. DOI: 10.14210/cotb.v12.p102-110.

CANDIDO, G. Aprendizado supervisionado x n√£o supervisionado - Data Hackers - Medium. Dispon√≠vel em: <https://medium.com/data-hackers/aprendizado-supervisionado-x-n%C3%A3o-supervisionado-ca79b522659d>.

TEBALDI, P. C. Holt-Winters | O que √© e como funciona esse algoritmo? Dispon√≠vel em: <https://www.opservices.com.br/holt-winters/>.

CONSTANTIN, G. A otimiza√ß√£o de hiperpar√¢metros. Dispon√≠vel em: <https://www.linkedin.com/pulse/otimiza%C3%A7%C3%A3o-com-grid-search-gabriel-constantin/>. Acesso em: 23 set. 2024.

LIU, Fei Tony; TING, Kai Ming; ZHOU, Zhi-Hua. Isolation forest. In: 2008 eighth ieee international conference on data mining. IEEE, 2008. p. 413-422.

STRAUSS, Edilberto; J√öNIOR, Manoel Villas B√¥as; FERREIRA, Wagner Luiz Lobo. A IMPORT NCIA DE UTILIZAR M√âTRICAS ADEQUADAS DE AVALIA√á√ÉO DE PERFORMANCE EM MODELOS PREDITIVOS DE MACHINE LEARNING. Projectus, v. 7, n. 2, p. 52-62, 2022.

ANKERST, M. et al. OPTICS. ACM SIGMOD Record, v. 28, n. 2, p. 49‚Äì60, 1 jun. 1999


## <a name="attachments"></a>Anexos
[Dashboard](extras/dashboard)

[Flyer de apresenta√ß√£o do modelo](extras/FLYER%20GALVAO%20E%20ASSOCIADOS.pdf)
